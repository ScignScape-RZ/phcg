`section.Procedures and Integration`
`p.
So far I have criticized paradigms which try to account for linguistic 
meaning via concordance between linguistic and proppositional 
structur; the shape of predicate complexes.   This critique has two dimnsions: 
first, although a predicate structure, a predicative specificity, does indeed 
permeate  states of affairs insofar as we engage thm rationally, such 
logical order is not modeled by language itself so much as by cognitive 
pictures we develop via interprtive processes `i.triggred` by language dtails 
but, I believe, to some not insubstantial degree pre- or xtra-linguistic.  
Moreover, second, insofar as we `i.can` decelop formal models of 
language, these are not going to be modls of prdicate structure 
in any conventional sense.  Cognitive-interpretive processes may 
have formal structure %-- structure which may even show a lot of overlap 
with propositional forms %-- but these are not `i.linguistic` structures.   
Insofar as language triggers but does not constitute interpretive 
`q.scripts`/, the scripts themselves (i.e., conceptual prototypes and 
perceptual schema we keep intelectually at hand, to interpret and 
act constructively in all situations, linguistic and otherwise) are 
not linguiatic as such %-- and neither is any propositional order they 
may simulate.  Language `i.does`/, however, structure the `i.integration` 
of `i.multiple` interpretive scripts, so the structure of this 
integration `i.is` linguistic structure per se %-- and formally 
modeling such integration can be an interesting tactic for 
formally modeling linguistic phenomena.  However, 
we should not assume that such a formal model will reemble or 
be reducible to formal logic in any useful way %-- formalization 
does not automatically entail some kind of de facto isomorphism 
to a system of logic (if not first-order then second-order, modal, etc.).  
`p`

`p.
Instead, I want to focus in on branches of computer science and 
mathematics (such a process algebra, which I have already referenced) 
as part of our scientific background insofar as the `i.structural 
integration` of diverse `q.processes` (computational processes 
in a formal sense, but perhaps analogously 
cognitive processes in a linguistic sense) can be technically represented.  
`p`

`p.
In `q.truth-theoretic` semantics, artifacts of language 
are intuitively pictured in terms of propositional structure.  
The guiding intuition compares word meanings to logical predicates; 
e.g. `q.John is married` is typically said when the speaker believes 
that, in fact, the predicate `q.married` applies to the 
speaker `q.John`/.  Switching to a more 
`q.procedural` perspective involves intuiting word-maning 
more in terms of interpretive procedures than logical predicates.  
The change in perspective may not yield substantially different 
analyses in all cases, but I believe it does affect many cases.
`p`

`p.
Even a simple predicate like `q.married` reveals a spectrum of 
not-entirely-logical cases in ordinary language:

`sentenceList,
`sentenceItem.John is married to a woman from his home country, but 
he had to get the marriag legally recognized here.` 
`sentenceItem.John married his boyfriend in Canada, but they 
live in a state that does not recognize same-sex marriages.`
`sentenceItem.John has been married for five years, but in 
many ways he's still a bachelor.`
`sentenceItem.Every married man needs a bachelor pad somewhere, 
and wherever yours is, you need a mini-fridge.`
`sentenceList`

We can make sense of these sentences because we do not conceptually 
define lexemes like `q.married` or `q.bachelor` via exhaustive 
logical rules, like `q.a bachelor is an unmarrid man`/.  Instead we 
have some combination of prototypical images %-- familiar 
characteristics of bachelors, say, which even married men 
can instantiate %-- and a conceptual framework recognizing 
(and if needed distinguishing) the various legal, societal, and 
dispositional aspects of being married.   
`p`

`p.
Intuitionwise, then, we should look beyond potential logical 
glosses on meanings %-- even when these are often accurate 
%-- and theorizee semantics as the mapping of lexical 
(as well as morphosyntactic phenomena) to interpretive scripts 
and conceptual or perceptual/enactive schema %-- which 
we can collectively designate, for sake of discussion, as 
`q.cognitive procedures`/.
`p`

`p.
The truth-theoretic mapping of words to predicates 
(and so phrasal and sentence units to propositional structures) 
provides an obvious way to formalize linguistic structur by 
borrowing the analogous structuration from 
predicate complexes.  Substituting a procedural semantic model 
allows a comparable formalization of linguistic structure 
through theories exploring procedural integration, 
for instance the interactions between computational procedures.  
Analysis of `i.computational` produres can yield interesting 
idas for linguistic theories of `i.cognitive` procdures 
%-- without endorsing a reductive metaphysics of 
cognitive procedures as `q.nothing but` computational 
procedures implemented in some sort of mental software.
`p`


`p.
The notion of computational procedures is certainly 
not foreign to symbolic logic or to analyses oftn 
associated with logistical models (in linguistics and philosophy, 
say), like Typed Lambda Calculus.  In those paradigms 
we certainly have, say, an idea of applying formulae to 
input values (where formulae can be seen as referring to or 
as compact notations for computational procedures).  Comparing 
the results of two formulae yields predicates 
(e.g., equalities or greater/less-than relations of 
quantity) that undergirds predicate systems.  Furthermore, 
combining formulae %-- plugging results of one formula 
into a free variable in another %-- yields new formulae 
that can in turn be reduced to different forms or to single 
values, either for all inputs or for particular inputs.  
In short, a large class of computational procedures can 
be modeled in predicate and equational or formula-reduction 
terms.  I point this out because shifting to 
a `q.procedural` intuitive model does not `i.by itself` 
take us outside of logistic and truth-thoretic paradigms.
`p`

`p.
In computer science, theories of procedural networks 
begin to branch out from formal logic or 
lambda calculii when we move past the `q.mathematical` 
picture of procedures %-- as computations that accept a 
collection of inputs and return a collection of outputs 
%-- and theorize procedures mor as computational 
modules that input, output, and modify values 
from multiple sources, in different ways.  In practical 
technology, this includes developments like Object-Oriented 
computer languages and programming techniques such as 
Exceptions and mutable references.  Likewise, 
real-world cod can pull information from many sources %-- like 
databases, files, and networks (including the internet) 
%-- in addition to values input as parameters to a function.   
Conversely functions have many ways to modify values 
%-- many ways to manifest side-effects %-- beyond just 
returning values as outputs.  Collectively these various 
channels of communication %-- wherein different procedures 
can exchange values in different ways %-- are often 
described as `q.inpure`/, by contrast 
to `q.pure` functions which use only a single 
input channel for (non-modifiable) input parameters 
and a single output channel. 
`p`

`p.
The design and implementation of computer programming languages 
%-- sometimes called `q.Software Language Engineering` %-- 
is partly a theoretical discipline (because programming languages 
are based on formal systems that can be studied mathematically, 
like Lambda Calculus), but also very pragmatic.  Computer languages 
are judged by practical considerations, like how efficiently they 
allow programmers to create quality software.  The impetus 
for new programming techniques often comes from the practical 
side: mainstream languages began to incoporate features like 
Object-Orientation and Exceptions (which are a mechanism to 
interrupt normal program flow when coding assumptions are violated) 
because these features proved useful to programmers.  At the same 
time, various `q.inpure` features can still be modeled at the theoretical 
level %-- there are extensions to Lambda Calculus 
with Exceptions and/or Objects, as well as `q.effect systems` that 
systematically model functional side-effects via Type Theory.  
In this case, though, the thoretical models are partly isolated or 
even lag behind the evolution of practical coding techniques.
`p`

`p.
It is also true that `q.inpure` code can always be refactored 
into a code base that uses only `q.pure` functional-programming 
techniques.  That is, at least if we exclude the `q.cyberphysical` 
layer %-- the body of code that directly measures or alteres 
physical data, like writing text to a computer screen or 
reading the position of a mouse %-- any self-contained 
collection of `q.inpure` functions can always be re-implemented 
as a system of pure `q.mathematical` functions, using 
only one-dimensional, immutable input and output channels 
(this is not necessarily true of singl functions, but 
abstractly true of complete code `i.bodies` encompassing many 
functions, i.e., many implemented procedures).  Analogously, any 
computing environment based on a type system that incorporates 
impure function-types can be recreated on top of a different 
type system that recognizes only pure functions.  For sake of 
discussion, define a `i.procedure space` as a self-contained 
network of computational procedures, where each procedure has 
the ability to invoke other procedures in the network.  We can then 
say that any procedure space developed with the benefit of 
`q.impure` coding styles (like Objects, Exceptions, and 
side-effects) is `i.computationally isomorphic` to a procdure space 
implemented exclusively in a pur-functional paradigm.
`p`

`p.
This principle might suggest that the pure/impure distinction 
is superficial, and so that any computing environment is 
actually manifesting a purely logical framework, because 
it is (in one sense) isomorphic to a structur which 
`i.can` be thoroughly modeled via formal logic.  However, this 
reductionism actually reveals the limits of `q.computational` 
isomorphism in the abstract.  Impure procedure-spaces are not 
`i.equivalent` to their pure isomorphs.  The whole rationale for 
`q.impure` coding techniques %-- Object-Orintation, functions 
with side-effects, etc. %-- is that the resulting code better 
models empirical phenomena and/or how programmers themselvs 
reason about software design.  Any notion of 
computational isomorphism which is `i.itself` prejudiced 
toward formal logic will fail to model how systems 
which are isomorphic `i.on these` (already logical) 
`i.terms` will nevertheless vary in how 
organically they model empirical and mental phenomena.  
If engineers discover that Object-Oriented paradigms 
produce more effective software for managing biomedical 
data %-- that is, biomedical concepts are usefully 
modeled in terms of semiautonomous computational `q.objects` 
whose properties and functionality are enginereed via 
many discrete, partially isolated software components %-- this 
suggests how Object-Oriented structures may be a more natural 
systematization of the underlying biomedical reality.  
Object-Oriented software can `i.in principle` be redesigned 
without Objects, but the resulting code base would thn be a weaker 
semantic `q.fit`/, a less faithful computational encoding, 
of the external information spaces that the software 
needs to modl and simulate.
`p`

`p.
I contend that this duality between Object-Oriented and 
pure-functional programming is analogous to the paradigmatic 
contrast between cognitiv and truth-theoretic semantics.  
On this analogy, pure-functional programming can be associated 
with truth-theoretic semantics in that both are founded on formal 
systems that more or less straightforwardly operationalize 
first-order logic %-- a logic with quantification and with sets 
or types modeling collections, but whose foundational units are 
boolean predicates and elementary formulae rather than 
proceures with multiple `q.channels of communication` and 
potentially many intermediate states.  Reducing impure 
(e.g., Object-Oriented) code to pure-functional procedure spaces 
is analogous to providing truth-theoretic reconstructions 
of linguistic meanings, as in (revisiting examples from last section):

`sentenceList,
`sentenceItem.People had fed the rescued dogs (i.e.: before they 
were rescued).` 
`sentenceItem.New Yorkers vote democratic (i.e.: the majority 
of registered voters in New York do so).`
`sentenceList`

Propositional reconstructions of sentences lik thse can capture a 
logical substrate that contributes to their meaning, but completely 
`i.reducing` their semantics to such logical substrata jettisons, 
I contend, interpretive and contextual schemata which ar equally 
essential to their meaning.  Analogously %-- considered 
in relation to data that it must curate and model (so that pople 
may manage and aggregate data; for instance, collect biomedical data 
to ensure proper diagnoses and implement tratments) %-- software 
does not only represent the logical substrata undrlying information 
systems.  It needs to capture conceptual and operational/logistical 
phenomena as well.
`p`

`p.
Along these lines, note that Object-Oriented techniques originally 
emerged from scientific-computing environments, to model 
complex systems, with multiple levels of organization, mrgent 
structure, and so forth (in the 1990s these techniques 
were then applied to more enterprise-style applications, 
for User Interface design, business operations workflows, etc.).  
The effectiveness of Object-Oriented models 
suggests that as a form of computing environment, and a 
framework for coding procedure-spaces, Object-Orientation 
captures structural properties of complex systems 
more richly than paradigms that reduce procedural implementations 
to a purely logical substratum.  This observation 
can then generalize to the whole range of modern-day programming 
techniques and all the various domains where software is used 
to monitor, simulate, and manage information about natural and 
human phenomena.  
`p`

`p.
Technically, the multi-dimensional structure of computational 
procedures implmented with modern coding techniqus %-- 
multi-dimensional in the sense of multiple forms of channels of 
communication and functional side-effects %-- demands new 
concptualizations of the formal architecture of computing nvironments 
as such.  Rather than seeing computer software, for example, as a 
complex system built architecturally from predicate logic, we 
need to undrstand software as a procedure-space or procedural 
network receptivvto external information.  On this account, a 
software application internally possesses a set of capabilities, 
and achieves this functionality by invoking different 
procedures which are implemented in the software; the application 
as a whole is the aggregate of its procedural building-blocks.  
Which procedures get invoked depends on user actions 
%-- we use a mouse, keyboard, and other input devices to 
trigger responses from the software.  Applications are therefore built 
around an `q.event loop`/: the software can enter a passive 
state, performing no operations apart from monitoring 
physical devices to detect user actions.  Onc an action does 
occur %-- we press a letter on a keyboard, say, or press a mous button 
%-- the application responds by invoking a specific procedure, which 
in turn may trigger a complex cascade of other procedures, to complete 
whatever operation we requested via our action (saving a file, 
selecting one record to visualize from a database, etc.).  
Via such networks of procedures and functionality, software 
models and simulates empirical phenomea %-- at least enough so that we can 
maintain databases, remotely operat machines, and in other ways track 
and manipulate physical states.  
`p`

`p.
Consequently, computer models of empirical phenomena can take the 
form of `i.procedural networks`/, and we can designate the design and 
theory of such models as `i.procedural modeling`/.  I have argued that 
procedural networks are not reducible to logical predicate systems 
even though procedure-spaces in general are (in a certain sense) 
isomorphic to procedure-spaces which are more purely logical, 
or employ pure-functional type systems.  The `q.isomorphism` relevant 
here, I am arguing, eliminates semantic and simulative structures 
that make procedural models effective constructions conveying 
the organization and behavior patterns of complx, real-world 
phenomena.  On this theory, procedural modeling is a more 
effective tool for representing real-world systems than 
are computing environments built more mchanistically from 
predicate logic.
`p`

`p.
Interestingly, however, the predominant contemporary 
paradigms for modeling real-world information systems %-- 
particularly what has come to be called the `q.Semantic Web` 
%-- is built on a framework of description logic and 
`q.knowledge graphs` rather than (at least except very indirectly) 
anything that could be called `q.procedural data modeling`/.  
There is, of course, a robust ecosystem of network-based 
code-sharing that provides a viable infrastructure for 
a more procedural data-sharing paradigm.  To illustrate the contrast, 
consider the problem of biomedical data integration: of 
merging the information known to two different mdical entrprises, 
as when a patient moves between two different hospital systems.  
We can assume that each hospital uses a different set of software 
tools to store patient records and manage data from their various 
secondary or constituent sources (diagnostic labs, specialized 
departments, outside clinics, etc.).  Such multi-faceted data then 
needs to be translated from formats native to the prior hospital's 
software to formats needed by the new hospital.    
`p`

`p.
Such data-intgration problems tend to be concptualized in one 
of two ways.  One approach is to seek a common reprsentation, 
or an overarching unified model, which can reconcile structural 
differences between the two systems.  If both hospitals use the 
same biomedical `q.Ontologies`/, for example, then those Ontologies 
serve as a logical paradigm through which the distinct systems can 
be bridged.  In effect, structural differences between the systems are 
treated as superficial variations on a common or unified 
logical `q.deep structure` %-- analogous to translating between 
natural languages by mapping sentences to a propositional core.  
Indeed, the field of `q.Ontology Engineering` can be seen as a 
way of marshaling abstract logic into a form that is practically 
useful for information storage and extraction, in open-ended 
environments where data may be aggregated from many heterogeneous 
sources.  The term `q.Ontology` in this context is not wholly 
unrelated to its philosophical meaning, bcause `q.knowledge 
engineers` assuming that the primordial structuring paradigms 
of such `q.universal` logical systems are relations and 
distinctions investigatd by the philosophical ontological tradition 
%-- substance and attribute, part and whole, time-point vs. 
time-span, spatial region vs. spatial boundary, subtype and 
supertype, and so forth.  Abstart Ontological systems are 
then formalized into logical rules that provide an 
axiomatic core %-- sometims called `q.Upper` Ontologies 
%-- which are then extended via empirical models 
or `q.domain-specific` rules to create Domain-Specific Ontologies 
used to integrate data in concrete enterpris/scientific fields 
(medical information, government records, and so forth). 
`p`

`p.
The overarchong paradigm of such Ontology-based integration 
is the idea of a logical model that recognizes superficial 
differencs between incompatible (or at least not fully 
compatible) ata sources.  An alternative approach to data 
integration problems is to treat these as issues of 
procedural capability.  In order to import data from one 
hospital into a second hospital system, for example, assuming 
their respective software and databases are at least somewhat 
incompatible, you need to perform certain procedures that translate 
data from the first format to the second.  Insofar as this is 
possible, the two software systems gave a certain procedural 
synergy: the capabilities of the first system include exporting 
data in a format the second can use, and the capabilities of the 
second system include integrating data presented in formats that 
the first can export.  This synergy does not need to be theorized 
in terms of an overarching logical ur-space which both 
systems indirectly manifest; instead, it reflcts how some 
subset of the overall procedural network germane to each 
respctive software system includes %-- either by 
deliberate cooperative enginering or because the two 
systems are guided by similar standards and technical orientations 
%-- procedures on the two ends that can be aligned in terms 
of the kind of data and data-formats they produce and/or consum.  
In short, targeted (and potentially cooperatively-engineered) 
procedural alignments %-- rather than overarching logical 
commonalities %-- form the bedrock of data integration.
`p`

`p.
These contrasting paradigms are not only theoretical.  
Computer scientists hafve actually tried to promote 
data sharing and data integration to improve governance, 
health care, urban infrastructure (via `q.smart cities`/), 
sustainable development, and so on.  Insofar as we see data 
integration in terms of Ontology Engineering, these 
practical goals can be met with the curation and publication 
of formal Ontologies that can guide software and database 
development.  Data sharing is then driven by Ontological 
alignment %-- the Semantic Web, for example, designed 
as an open repository of raw data annotatd and structured in accord 
with publishd Ontologies, data which can bereusedand integrated into 
different projects more readily because it adopts these published 
models.  On the other hand, insofar as we see data sharing in terms 
of `i.procedural alignment`/, we prioritize th curation and 
publication of procedure implementations, in the form of software 
libraries, open-source code repositories, and other building-blocks 
of modular software design.  Consider again the case of data 
integration between two hospitals: to integrate data between the two, 
programmers may need to implement special `q.bridge functions` that 
reconcile their rspective formats.  This implementation is 
simplifid insofar as the respective software systems are well-organized 
and transparent %-- so that programmers can examine inport and 
export functions in the two code bases and write new procedures, 
extending thir respective capabilities, so that import functionality 
on one end can be aligned to export functionality on the other.  
These new procedures can then be shared so that similar 
data integration problems %-- for instance the first 
hospital sharing data with a third %-- can be solved 
more readily, reusing some of the code thereby developed. 
This approach to data integration emphasizes transparency, 
modularity, and code-sharing.
`p`

`p.
Rather than seeing the Semantic Web as a network of raw data 
conforming to published Ontologies, the more procedural 
perspective would see endeavors like a Semantic Web as driven 
by code sharing: open repsoitories of procdural 
implementations that can be used to reconcile 
data incompatibilities, pull data from different sources, 
document data structure and representation formats, etc.  
Decentralized networks like the Semantic Web would then 
be characterized by the free exchange of procedural 
implementations, so that enginers can pull procdures providing 
different capabilities togther, to assemble fully-featured 
software platforms.  Code libraries would play 
a homogenizing role in this paradigm analogous to  
Ontologies in the Semantic Web.  And, indeed, there 
exists a mature and sophisticated technical infrastructure 
for publishing software componenrs and maintaining 
code repositories, forming the productive underbelly 
of the Open-Source ecosystem (`q.git` version control, the 
GNU Compiler Collection, Linux distributions, etc.).  
However, the Semantic Web and Open Source development 
communities are largely separate, apart from the practical 
given that many Semantic Web technologies are distributed 
as Open-Source software.  Despite the `i.practical` 
adoption of Open-Source norms, the Semantic Web community 
has arguably not engaged the Open-Source community 
at a deeper theoretical level, in terms of how th curation 
of public, collaborative code libraries can promote 
data integration analogous in effect to (but arguably 
semantically more accurate than) Semantic Web Ontologies 
%-- in light of critiques that the logical intuitions 
behind the Semantic Web create a distorted and 
oversimplified theory of what semantics is all 
about (in the words of Peter Gardenfors, 
`q.The Semantic Web is not very Semantic`/).
`p`

`p.
These questions bear directly on Phenomnology, because 
philosophers in the phenomenological tradition have directly 
influenced the evolution of the Semantic Web %-- notably 
Barry Smith, who has both published sophisticated 
theoretical work on Ontologies in the Semantic Web sense 
and also spearheaded practical initiatives like the 
OBO (Open Biological and Biomedical Ontology) Foundry.  
The OBO Foundry merged in the mid-2000s, on the heels 
of a renewed interest in Phenomnology as a philosophical 
basis for Cognitiv Scienc and other practical/technical 
disciplines, like knowledge engineering.  It is not 
hard to see practical artifacts like the OBO system as 
concrete realizations of theoretical goals articulated 
in volumes such as 1999's `i.Naturalizing Phenomnology`/.  

`p`


`p.

`p`
