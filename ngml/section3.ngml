`section.Procedures and Integration`
`p.
So far I have criticized paradigms which try to account for linguistic 
meaning via concordance between linguistic and proppositional 
structur; the shape of predicate complexes.   This critique has two dimensions: 
first, although a predicate structure, a predicative specificity, does indeed 
permeate  states of affairs insofar as we engage thm rationally, such 
logical order is not modeled by language itself so much as by cognitive 
pictures we develop via interprtive processes `i.triggred` by language dtails 
but, I believe, to some not insubstantial degree pre- or extra-linguistic.  
Moreover, second, insofar as we `i.can` decelop formal models of 
language, these are not going to be models of prdicate structure 
in any conventional sense.  Cognitive-interpretive processes may 
have formal structure %-- structure which may even show a lot of overlap 
with propositional forms %-- but these are not `i.linguistic` structures.   
Insofar as language triggers but does not constitute interpretive 
`q.scripts`/, the scripts themselves (i.e., conceptual prototypes and 
perceptual schema we keep intelectually at hand, to interpret and 
act constructively in all situations, linguistic and otherwise) are 
not linguiatic as such %-- and neither is any propositional order they 
may simulate.  Language `i.does`/, however, structure the `i.integration` 
of `i.multiple` interpretive scripts, so the structure of this 
integration `i.is` linguistic structure per se %-- and formally 
modeling such integration can be an interesting tactic for 
formally modeling linguistic phenomena.  However, 
we should not assume that such a formal model will reemble or 
be reducible to formal logic in any useful way %-- formalization 
does not automatically entail some kind of de facto isomorphism 
to a system of logic (if not first-order then second-order, modal, etc.).  
`p`

`p.
Instead, I want to focus in on branches of computer science and 
mathematics (such a process algebra, which I have already referenced) 
as part of our scientific background insofar as the `i.structural 
integration` of diverse `q.processes` (computational processes 
in a formal sense, but perhaps analogously 
cognitive processes in a linguistic sense) can be technically represented.  
`p`

`p.
In `q.truth-theoretic` semantics, artifacts of language 
are intuitively pictured in terms of propositional structure.  
The guiding intuition compares word meanings to logical predicates; 
e.g. `q.John is married` is typically said when the speaker believes 
that, in fact, the predicate `q.married` applies to the 
speaker `q.John`/.  Switching to a more 
`q.procedural` perspective involves intuiting word-maning 
more in terms of interpretive procedures than logical predicates.  
The change in perspective may not yield substantially different 
analyses in all cases, but I believe it does affect many cases.
`p`

`p.
Even a simple predicate like `q.married` reveals a spectrum of 
not-entirely-logical cases in ordinary language:

`sentenceList,
`sentenceItem.John is married to a woman from his home country, but 
he had to get the marriag legally recognized here.` 
`sentenceItem.John married his boyfriend in Canada, but they 
live in a state that does not recognize same-sex marriages.`
`sentenceItem.John has been married for five years, but in 
many ways he's still a bachelor.`
`sentenceItem.Every married man needs a bachelor pad somewhere, 
and wherever yours is, you need a mini-fridge.`
`sentenceList`

We can make sense of these sentences because we do not conceptually 
define lexemes like `q.married` or `q.bachelor` via exhaustive 
logical rules, like `q.a bachelor is an unmarrid man`/.  Instead we 
have some combination of prototypical images %-- familiar 
characteristics of bachelors, say, which even married men 
can instantiate %-- and a conceptual framework recognizing 
(and if needed distinguishing) the various legal, societal, and 
dispositional aspects of being married.   
`p`

`p.
Intuitionwise, then, we should look beyond potential logical 
glosses on meanings %-- even when these are often accurate 
%-- and theorizee semantics as the mapping of lexical 
(as well as morphosyntactic phenomena) to interpretive scripts 
and conceptual or perceptual/enactive schema %-- which 
we can collectively designate, for sake of discussion, as 
`q.cognitive procedures`/.
`p`

`p.
The truth-theoretic mapping of words to predicates 
(and so phrasal and sentence units to propositional structures) 
provides an obvious way to formalize linguistic structur by 
borrowing the analogous structuration from 
predicate complexes.  Substituting a procedural semantic model 
allows a comparable formalization of linguistic structure 
through theories exploring procedural integration, 
for instance the interactions between computational procedures.  
Analysis of `i.computational` produres can yield interesting 
idas for linguistic theories of `i.cognitive` procdures 
%-- without endorsing a reductive metaphysics of 
cognitive procedures as `q.nothing but` computational 
procedures implemented in some sort of mental software.
`p`


`p.
The notion of computational procedures is certainly 
not foreign to symbolic logic or to analyses oftn 
associated with logistical models (in linguistics and philosophy, 
say), like Typed Lambda Calculus.  In those paradigms 
we certainly have, say, an idea of applying formulae to 
input values (where formulae can be seen as referring to or 
as compact notations for computational procedures).  Comparing 
the results of two formulae yields predicates 
(e.g., equalities or greater/less-than relations of 
quantity) that undergirds predicate systems.  Furthermore, 
combining formulae %-- plugging results of one formula 
into a free variable in another %-- yields new formulae 
that can in turn be reduced to different forms or to single 
values, either for all inputs or for particular inputs.  
In short, a large class of computational procedures can 
be modeled in predicate and equational or formula-reduction 
terms.  I point this out because shifting to 
a `q.procedural` intuitive model does not `i.by itself` 
take us outside of logistic and truth-thoretic paradigms.
`p`

`p.
In computer science, theories of procedural networks 
begin to branch out from formal logic or 
lambda calculii when we move past the `q.mathematical` 
picture of procedures %-- as computations that accept a 
collection of inputs and return a collection of outputs 
%-- and theorize procedures mor as computational 
modules that input, output, and modify values 
from multiple sources, in different ways.  In practical 
technology, this includes developments like Object-Oriented 
computer languages and programming techniques such as 
Exceptions and mutable references.  Likewise, 
real-world cod can pull information from many sources %-- like 
databases, files, and networks (including the internet) 
%-- in addition to values input as parameters to a function.   
Conversely functions have many ways to modify values 
%-- many ways to manifest side-effects %-- beyond just 
returning values as outputs.  Collectively these various 
channels of communication %-- wherein different procedures 
can exchange values in different ways %-- are often 
described as `q.inpure`/, by contrast 
to `q.pure` functions which use only a single 
input channel for (non-modifiable) input parameters 
and a single output channel. 
`p`

`p.
The design and implementation of computer programming languages 
%-- sometimes called `q.Software Language Engineering` %-- 
is partly a theoretical discipline (because programming languages 
are based on formal systems that can be studied mathematically, 
like Lambda Calculus), but also very pragmatic.  Computer languages 
are judged by practical considerations, like how efficiently they 
allow programmers to create quality software.  The impetus 
for new programming techniques often comes from the practical 
side: mainstream languages began to incoporate features like 
Object-Orientation and Exceptions (which are a mechanism to 
interrupt normal program flow when coding assumptions are violated) 
because these features proved useful to programmers.  At the same 
time, various `q.inpure` features can still be modeled at the theoretical 
level %-- there are extensions to Lambda Calculus 
with Exceptions and/or Objects, as well as `q.effect systems` that 
systematically model functional side-effects via Type Theory.  
In this case, though, the thoretical models are partly isolated or 
even lag behind the evolution of practical coding techniques.
`p`

`p.
It is also true that `q.inpure` code can always be refactored 
into a code base that uses only `q.pure` functional-programming 
techniques.  That is, at least if we exclude the `q.cyberphysical` 
layer %-- the body of code that directly measures or alteres 
physical data, like writing text to a computer screen or 
reading the position of a mouse %-- any self-contained 
collection of `q.inpure` functions can always be re-implemented 
as a system of pure `q.mathematical` functions, using 
only one-dimensional, immutable input and output channels 
(this is not necessarily true of singl functions, but 
abstractly true of complete code `i.bodies` encompassing many 
functions, i.e., many implemented procedures).  Analogously, any 
computing environment based on a type system that incorporates 
impure function-types can be recreated on top of a different 
type system that recognizes only pure functions.  For sake of 
discussion, define a `i.procedure space` as a self-contained 
network of computational procedures, where each procedure has 
the ability to invoke other procedures in the network.  We can then 
say that any procedure space developed with the benefit of 
`q.impure` coding styles (like Objects, Exceptions, and 
side-effects) is `i.computationally isomorphic` to a procdure space 
implemented exclusively in a pur-functional paradigm.
`p`

`p.
This principle might suggest that the pure/impure distinction 
is superficial, and so that any computing environment is 
actually manifesting a purely logical framework, because 
it is (in one sense) isomorphic to a structur which 
`i.can` be thoroughly modeled via formal logic.  However, this 
reductionism actually reveals the limits of `q.computational` 
isomorphism in the abstract.  Impure procedure-spaces are not 
`i.equivalent` to their pure isomorphs.  The whole rationale for 
`q.impure` coding techniques %-- Object-Orintation, functions 
with side-effects, etc. %-- is that the resulting code better 
models empirical phenomena and/or how programmers themselvs 
reason about software design.  Any notion of 
computational isomorphism which is `i.itself` prejudiced 
toward formal logic will fail to model how systems 
which are isomorphic `i.on these` (already logical) 
`i.terms` will nevertheless vary in how 
organically they model empirical and mental phenomena.  
If engineers discover that Object-Oriented paradigms 
produce more effective software for managing biomedical 
data %-- that is, biomedical concepts are usefully 
modeled in terms of semiautonomous computational `q.objects` 
whose properties and functionality are enginereed via 
many discrete, partially isolated software components %-- this 
suggests how Object-Oriented structures may be a more natural 
systematization of the underlying biomedical reality.  
Object-Oriented software can `i.in principle` be redesigned 
without Objects, but the resulting code base would thn be a weaker 
semantic `q.fit`/, a less faithful computational encoding, 
of the external information spaces that the software 
needs to modl and simulate.
`p`

`p.
I contend that this duality between Object-Oriented and 
pure-functional programming is analogous to the paradigmatic 
contrast between cognitiv and truth-theoretic semantics.  
On this analogy, pure-functional programming can be associated 
with truth-theoretic semantics in that both are founded on formal 
systems that more or less straightforwardly operationalize 
first-order logic %-- a logic with quantification and with sets 
or types modeling collections, but whose foundational units are 
boolean predicates and elementary formulae rather than 
proceures with multiple `q.channels of communication` and 
potentially many intermediate states.  Reducing impure 
(e.g., Object-Oriented) code to pure-functional procedure spaces 
is analogous to providing truth-theoretic reconstructions 
of linguistic meanings, as in (revisiting examples from last section):

`sentenceList,
`sentenceItem.People had fed the rescued dogs (i.e.: before they 
were rescued).` 
`sentenceItem.New Yorkers vote democratic (i.e.: the majority 
of registered voters in New York do so).`
`sentenceList`

Propositional reconstructions of sentences lik thse can capture a 
logical substrate that contributes to their meaning, but completely 
`i.reducing` their semantics to such logical substrata jettisons, 
I contend, interpretive and contextual schemata which ar equally 
essential to their meaning.  Analogously %-- considered 
in relation to data that it must curate and model (so that pople 
may manage and aggregate data; for instance, collect biomedical data 
to ensure proper diagnoses and implement tratments) %-- software 
does not only represent the logical substrata undrlying information 
systems.  It needs to capture conceptual and operational/logistical 
phenomena as well.
`p`

`p.
Along these lines, note that Object-Oriented techniques originally 
emerged from scientific-computing environments, to model 
complex systems, with multiple levels of organization, mrgent 
structure, and so forth (in the 1990s these techniques 
were then applied to more enterprise-style applications, 
for User Interface design, business operations workflows, etc.).  
The effectiveness of Object-Oriented models 
suggests that as a form of computing environment, and a 
framework for coding procedure-spaces, Object-Orientation 
captures structural properties of complex systems 
more richly than paradigms that reduce procedural implementations 
to a purely logical substratum.  This observation 
can then generalize to the whole range of modern-day programming 
techniques and all the various domains where software is used 
to monitor, simulate, and manage information about natural and 
human phenomena.  
`p`

`p.
Technically, the multi-dimensional structure of computational 
procedures implmented with modern coding techniqus %-- 
multi-dimensional in the sense of multiple forms of channels of 
communication and functional side-effects %-- demands new 
concptualizations of the formal architecture of computing nvironments 
as such.  Rather than seeing computer software, for example, as a 
complex system built architecturally from predicate logic, we 
need to undrstand software as a procedure-space or procedural 
network receptivvto external information.  On this account, a 
software application internally possesses a set of capabilities, 
and achieves this functionality by invoking different 
procedures which are implemented in the software; the application 
as a whole is the aggregate of its procedural building-blocks.  
Which procedures get invoked depends on user actions 
%-- we use a mouse, keyboard, and other input devices to 
trigger responses from the software.  Applications are therefore built 
around an `q.event loop`/: the software can enter a passive 
state, performing no operations apart from monitoring 
physical devices to detect user actions.  Onc an action does 
occur %-- we press a letter on a keyboard, say, or press a mous button 
%-- the application responds by invoking a specific procedure, which 
in turn may trigger a complex cascade of other procedures, to complete 
whatever operation we requested via our action (saving a file, 
selecting one record to visualize from a database, etc.).  
Via such networks of procedures and functionality, software 
models and simulates empirical phenomea %-- at least enough so that we can 
maintain databases, remotely operat machines, and in other ways track 
and manipulate physical states.  
`p`

`p.
Consequently, computer models of empirical phenomena can take the 
form of `i.procedural networks`/, and we can designate the design and 
theory of such models as `i.procedural modeling`/.  I have argued that 
procedural networks are not reducible to logical predicate systems 
even though procedure-spaces in general are (in a certain sense) 
isomorphic to procedure-spaces which are more purely logical, 
or employ pure-functional type systems.  The `q.isomorphism` relevant 
here, I am arguing, eliminates semantic and simulative structures 
that make procedural models effective constructions conveying 
the organization and behavior patterns of complx, real-world 
phenomena.  On this theory, procedural modeling is a more 
effective tool for representing real-world systems than 
are computing environments built more mchanistically from 
predicate logic.
`p`

`p.
Interestingly, however, the predominant contemporary 
paradigms for modeling real-world information systems %-- 
particularly what has come to be called the `q.Semantic Web` 
%-- is built on a framework of description logic and 
`q.knowledge graphs` rather than (at least except very indirectly) 
anything that could be called `q.procedural data modeling`/.  
There is, of course, a robust ecosystem of network-based 
code-sharing that provides a viable infrastructure for 
a more procedural data-sharing paradigm.  To illustrate the contrast, 
consider the problem of biomedical data integration: of 
merging the information known to two different mdical entrprises, 
as when a patient moves between two different hospital systems.  
We can assume that each hospital uses a different set of software 
tools to store patient records and manage data from their various 
secondary or constituent sources (diagnostic labs, specialized 
departments, outside clinics, etc.).  Such multi-faceted data then 
needs to be translated from formats native to the prior hospital's 
software to formats needed by the new hospital.    
`p`

`p.
Such data-intgration problems tend to be concptualized in one 
of two ways.  One approach is to seek a common reprsentation, 
or an overarching unified model, which can reconcile structural 
differences between the two systems.  If both hospitals use the 
same biomedical `q.Ontologies`/, for example, then those Ontologies 
serve as a logical paradigm through which the distinct systems can 
be bridged.  In effect, structural differences between the systems are 
treated as superficial variations on a common or unified 
logical `q.deep structure` %-- analogous to translating between 
natural languages by mapping sentences to a propositional core.  
Indeed, the field of `q.Ontology Engineering` can be seen as a 
way of marshaling abstract logic into a form that is practically 
useful for information storage and extraction, in open-ended 
environments where data may be aggregated from many heterogeneous 
sources.  The term `q.Ontology` in this context is not wholly 
unrelated to its philosophical meaning, bcause `q.knowledge 
engineers` assuming that the primordial structuring paradigms 
of such `q.universal` logical systems are relations and 
distinctions investigatd by the philosophical ontological tradition 
%-- substance and attribute, part and whole, time-point vs. 
time-span, spatial region vs. spatial boundary, subtype and 
supertype, and so forth.  Abstart Ontological systems are 
then formalized into logical rules that provide an 
axiomatic core %-- sometims called `q.Upper` Ontologies 
%-- which are then extended via empirical models 
or `q.domain-specific` rules to create Domain-Specific Ontologies 
used to integrate data in concrete enterpris/scientific fields 
(medical information, government records, and so forth). 
`p`

`p.
The overarchong paradigm of such Ontology-based integration 
is the idea of a logical model that recognizes superficial 
differencs between incompatible (or at least not fully 
compatible) ata sources.  An alternative approach to data 
integration problems is to treat these as issues of 
procedural capability.  In order to import data from one 
hospital into a second hospital system, for example, assuming 
their respective software and databases are at least somewhat 
incompatible, you need to perform certain procedures that translate 
data from the first format to the second.  Insofar as this is 
possible, the two software systems gave a certain procedural 
synergy: the capabilities of the first system include exporting 
data in a format the second can use, and the capabilities of the 
second system include integrating data presented in formats that 
the first can export.  This synergy does not need to be theorized 
in terms of an overarching logical ur-space which both 
systems indirectly manifest; instead, it reflcts how some 
subset of the overall procedural network germane to each 
respctive software system includes %-- either by 
deliberate cooperative enginering or because the two 
systems are guided by similar standards and technical orientations 
%-- procedures on the two ends that can be aligned in terms 
of the kind of data and data-formats they produce and/or consum.  
In short, targeted (and potentially cooperatively-engineered) 
procedural alignments %-- rather than overarching logical 
commonalities %-- form the bedrock of data integration.
`p`

`p.
These contrasting paradigms are not only theoretical.  
Computer scientists hafve actually tried to promote 
data sharing and data integration to improve governance, 
health care, urban infrastructure (via `q.smart cities`/), 
sustainable development, and so on.  Insofar as we see data 
integration in terms of Ontology Engineering, these 
practical goals can be met with the curation and publication 
of formal Ontologies that can guide software and database 
development.  Data sharing is then driven by Ontological 
alignment %-- the Semantic Web, for example, designed 
as an open repository of raw data annotatd and structured in accord 
with publishd Ontologies, data which can bereusedand integrated into 
different projects more readily because it adopts these published 
models.  On the other hand, insofar as we see data sharing in terms 
of `i.procedural alignment`/, we prioritize th curation and 
publication of procedure implementations, in the form of software 
libraries, open-source code repositories, and other building-blocks 
of modular software design.  Consider again the case of data 
integration between two hospitals: to integrate data between the two, 
programmers may need to implement special `q.bridge functions` that 
reconcile their rspective formats.  This implementation is 
simplifid insofar as the respective software systems are well-organized 
and transparent %-- so that programmers can examine inport and 
export functions in the two code bases and write new procedures, 
extending thir respective capabilities, so that import functionality 
on one end can be aligned to export functionality on the other.  
These new procedures can then be shared so that similar 
data integration problems %-- for instance the first 
hospital sharing data with a third %-- can be solved 
more readily, reusing some of the code thereby developed. 
This approach to data integration emphasizes transparency, 
modularity, and code-sharing.
`p`

`p.
Rather than seeing the Semantic Web as a network of raw data 
conforming to published Ontologies, the more procedural 
perspective would see endeavors like a Semantic Web as driven 
by code sharing: open repsoitories of procdural 
implementations that can be used to reconcile 
data incompatibilities, pull data from different sources, 
document data structure and representation formats, etc.  
Decentralized networks like the Semantic Web would then 
be characterized by the free exchange of procedural 
implementations, so that enginers can pull procdures providing 
different capabilities togther, to assemble fully-featured 
software platforms.  Code libraries would play 
a homogenizing role in this paradigm analogous to  
Ontologies in the Semantic Web.  And, indeed, there 
exists a mature and sophisticated technical infrastructure 
for publishing software componenrs and maintaining 
code repositories, forming the productive underbelly 
of the Open-Source ecosystem (`q.git` version control, the 
GNU Compiler Collection, Linux distributions, etc.).  
However, the Semantic Web and Open Source development 
communities are largely separate, apart from the practical 
given that many Semantic Web technologies are distributed 
as Open-Source software.  Despite the `i.practical` 
adoption of Open-Source norms, the Semantic Web community 
has arguably not engaged the Open-Source community 
at a deeper theoretical level, in terms of how th curation 
of public, collaborative code libraries can promote 
data integration analogous in effect to (but arguably 
semantically more accurate than) Semantic Web Ontologies 
%-- in light of critiques that the logical intuitions 
behind the Semantic Web create a distorted and 
oversimplified theory of what semantics is all 
about (in the words of Peter Gardenfors, 
`q.The Semantic Web is not very Semantic`/).
`p`

`p.
These questions bear directly on Phenomnology, because 
philosophers in the phenomenological tradition have directly 
influenced the evolution of the Semantic Web %-- notably 
Barry Smith, who has both published sophisticated 
theoretical work on Ontologies in the Semantic Web sense 
and also spearheaded practical initiatives like the 
OBO (Open Biological and Biomedical Ontology) Foundry.  
The OBO Foundry emerged in the mid-2000s, on the heels 
of a renewed interest in Phenomnology as a philosophical 
basis for Cognitive Scienc and other practical/technical 
disciplines, like knowledge engineering.  It is not 
hard to see practical artifacts like the OBO system as 
concrete realizations of theoretical goals articulated 
in volumes such as 1999's `i.Naturalizing Phenomnology`/.  
This association is not only at the level of scholarship 
%-- the academic papers describing OBO, for example -- 
but also the design and organization of Semantic Web tools like 
the OBO Foundry, as technologies and platforms. 
`p`

`p.
Seen in those terms %-- and if we restrict attention to work 
done by scientists like Barry Smith and his colleagues who are 
actively enagaged in both the phenomenological and 
computer-science communities %-- Semantic Web technology suggests 
a paradigm wherein `q.Naturalizing` Phenomenology involves 
isolating logical structures which are at once essential 
to modeling empirical data and also emerge organically from 
phenomenological accounts of perception and cognition 
%-- that is, capturing the logical order of our experiencing 
the world as well as of the facts experienced.  A case in point 
is formalizing systems of `q.mereotopology` %-- combining 
the mereological account of part vs. whole with `q.topological` 
models of spatial continuity, locality, and bounday %-- 
which reflect both perceptual schema and information gestalts.  
So one the one hand the fusion of mereological and topological 
relations gives us a vocabulary for describing how we 
experientially apprehend spatial forms %-- the continuity, 
regionality, intersections, and disjunctions between 
visual (and sometimes tactile) elements that 
gives logical articulation to visual/tactil sense 
data (never experienced as `q.raw` sensation sinc we fundamentally 
experience space and visual continuity in these structured forms).  
Meanwhile, on the other hand, Mereotopology provides a  
semantic matrix for representing facts and relations in biological, 
geographical, and other scientific data, so it serves a 
practical information-management role.  In short, isolating 
logical gestalts and then codifyung them in practically useful 
forms serves to `q.Naturalize` Phenomenology by anchoring 
phenomenological reflection in applied science.  
`p`

`p.
The general implication of this `q.Naturalizing` strategy 
is that Phenomenology becomes naturalized, or reconcild 
with the physical sciences, insofar as structures of consciousness 
can be aligned with logical systms.  This strategy seems to extend 
beyond just the Semantic Web projects I have highlighted; it is 
likewise evident for example in Kit Fine's logical 
mereology in Barry Smith and David Woodruff Smith 
`i.Cambirdge Companion to Husserl` (`q.Part-Whole`/, 
pp. 463).  Indeed Husserl himself invites us to consider the 
logical formalization of phenomenological systems in works 
like the `i.Formal and Transcendental Logic`/, which appears 
to suggest that human conceptual systems are an even 
more refined manifestation of logical systems than are 
`q.formal` logics which get entangled in model-thoretic 
problems like the Lowenheim-Skolem thorem.  In other words, 
systems of formal logic are codifications of a mental order 
and therefore natural cadidates for a technical representation 
of the mental realm in its structure and specificity.
`p`


`p.
However, we can also observe that Husserl was writing at a time
when abstract logic was still the preeminent phenotype for 
systematic exposition of formal structures in general 
%-- this was before computer programming and even before 
mathematical developments like Category Theory.  In the 
first half of the last century, someone hoping to 
create a formal and systematic representation 
of cognitive processes would gravitate toward symbolic 
logic simply because mathematicians and philosophers 
at the time followed the general intuition that 
`i.any` formal structure was essentially characterized by 
its logical/axiomatic foundations.  A century later, 
formal logic has been displaced from the germinal origins it 
was once assigned.  For mathematicians, logic itself is 
revealed to be a kind of emergent system that depends on 
Categorial definitions like limits and colimits, and can 
vary across Categories %-- there are different logics 
for different kinds of Categories.  As such it is not 
logic itself but the properties and contrasts between 
Categories which is the truly primordial foundation 
of logico-mathematical thought.  A contemporary 
logico-mathematical formulation of Phenomenology 
may therefore try to establish a Category-Theoretic 
grounding rather than a logical one as ordinarily understood.  
A case in point would be how Jean Petitot situates 
phenomnological analysis in certain soecific genres of Categories, 
like sheaves and presheaves, in his `q.Morphological Eidetics` 
chapter in `i.Naturalizing Phenomenology` and elsewhere.
`p`

`p.
Petitot's and Barry Smith's formalizing projects were parallel 
and collaborative to some extent.  Maxwell James Ramstead  
in a 2015 master's thesis reviews the history elegantly: 
`q.Now, the `q.science of salience` 
proposed by Petitot and Smith (1997) illustrates the
kind of formalized analysis made possible through the direct 
mathematization of phenomenological descriptions.  
Its aim is to account for the invariant descriptive
structures of lived experience (what Husserl called `q.essences`/) 
through formalization, providing a descriptive geometry of 
macroscopic phenomena, a `q.morphological eidetics` of the 
disclosure of objects in conscious experience (in Husserl's 
words, the `q.constitution` of objects).  
Petitot employs differentiai geometry and morphodynamics 
to model phenomenal expenence, and Smith uses formai structures from
mereotopology (the theory of parts, wholes, and their boundaries) 
to a similar effect.`  Except, there are interesting 
contrasts between the Cognitive-Phenomenological 
adoption of mereotopology 
(by Barry Smith and also Roberto Casati, Achille C. Varzi, 
Kit Fine, Thomas Bittner, etc.) %-- which 
stays within a more classical logical paradigm 
%-- and Petitot's Morphological Eidtics, which 
is more Category-Theoretic.
`p`

`p.
Meanwhile, contemporary formalizations of phenomenological 
analyses can also gravitate toward a more concrete and 
computational framework %-- simulating cognitive 
processes via software or comparing artificial 
constructions of perceptual objects (via Virtual Reality, 
3D graphics, 3D printing, robotics, etc.) to lived 
exprience.  In particular, graphics engineers have a rich theory 
about how to create realistic (albeit not truly life-like) 3D 
models and scenes.  The mathematical and computational 
elements in this theory %-- triangular, quadrilateral, and 
polygonal meshes; shader algorithms; `q.NURBS` 
(Non-Uniform Rational Basis Spline) surfaces; textures and 
`i.uv`/-mapping; camera matrices; diffusion and stochastic 
processes %--  create a formal model of perceptual phenomena 
insofar as these can simulated `i.to a close approcimation`/: 
visual phnomena artificially built with these techniques 
can be `i.almost` realistic.  The gaps between such 
`q.virtal` scenes and real life may suggest that there are 
additional facets suffusing `q.ral life` perception, or 
even that despite their realism the mathematical 
building-blocks of CGI-like scenes  
are fundamentally different from the formal 
structures governing nurophenomenological perception.  
But in any case the almost life-lik realism that 
`i.can` be achieved via Computer Graphics is a data-point 
that Phenomenology should acknowledge: that a 
perceptual world built out of certain rigorously 
mathematical constitunts can feel almost 
lifelike when apprehended as if it were 
a real visual-perceptual surrounding.  In 
particular, Computer-Generated Imagery can evoke the 
same embodied engagement and intentional patterns as 
non-artificial, ambient perception so long as 
we accept a certain `q.suspension of disbelief`/, or 
mentally adjust to the phenomenological limitations 
of seeing visual tableau on a two-dimensional screen 
%-- analogous to watching movies or television.  
Despite the phenomenological chiasma
of directing visual attention to a 2D screen %-- it feels 
`q.not quite right` %-- we can still become engaged and 
largely immersed in the visual scenes before us; which means 
that full phenomenological realism is not prerequisite for 
our intentional comportment toward visual (and auditory) 
phenomena.  We can then observe that constructed scenes 
built entirely from mathematical carry comparable 
potential for intentional engagement.  Moreover, Panoramic 
Photography and Immersive Visual Reality present another 
genre of phenomenological immersion that transcends the 
limitations of the 2D screen %-- though still without 
full realism, because however lifelike the visual 
content we may perceive with, say, 3D 
goggles, we still are not engaging tactilely and 
kinaesthetically with the world in the usual ways.
`p`

`p.
In short, one route toward a formal framework for 
elucidating perceptual-phenomenological content is to 
examine realistic simulations of visual contents as 
computational artifacts %-- not in terms of abstract 
formulations (logical or otherwise) but in 
terms of concrete computer code and software.  With 
reference to Merotopology, for example, we can 
contrast the logical groundwork set out by 
Barry Smith (for example) with the differential-geometric 
and category-theoretic landscape considered by 
Jean Petitot `i.and also` a more `q.experimental`/, 
software-driven intuition associated with, 
for example, Virtual Reality research.  Looking at 
mereotopology in particular, this more computational 
approach can examine how part-whole relations 
are created within CGI and Computer Aided Design 
by mesh alignment or texture mapping, or how 
texture and diffusion algorthms create effects of material 
continuity and locality.  In this case mereotopological 
notions are not embedded in logical systems %-- or 
even, from a computational perspective, in formal 
Ontologies %-- but rather latent in graphics code. 
`p`

`p.
For scholars pursuing a `q.Naturalized` Phenomenology, then, we 
(in the 2010s and 2020s) have several avenues to choose from, 
including logical formalism (including as practically leveraged 
in the Semantic Web) but also mathematical 
formalization (as with Category Theory and Differential 
Geomtry) and, also, computationally.  Computer 
Aided Design and Computer Generatd Design point to a 
theory of formal structures producing life-like 
(if not perfectly realistic) perceptual content.  Analogously, 
computational models of linguistic structure can help 
represent the organizing principles of our reasoning 
toward language understanding %-- even if these formal models 
are not proposed as direct simulations of natural 
linguistic cognition.
`p`

`p.
Simultaneously, real-world applied projects 
%-- like the Semantic Web, Virtual Reality, 
or CGI/CAD technology %-- can be seen as practical 
test-beds for the realism and analytic potential 
of formal-phenomenological frameworks.  In some 
cases, like the OBO Foundary, the link between 
applied technology and Phenomnology is explicit; in 
others, as with VR and CGI, this link 
is more implicit and thematic (but still 
addressed by interdisciplinary research grounded in 
Phenomenology and Husserl scholarship).  But in any 
case technological experience retroactively seems to 
shape the direction of phenomenological research, 
while at the same time Phenomenology has, for some 
researchers, provided a metatheortic and 
metaphysical guideline.
`p`

`p.
Here the Semantic Web presents an interesting case-study, 
because the manifestation of phenomenological themes 
(e.g., Mereotopology) in a practically 
useful resource like Biomedical Ontologies suggests an 
`i.ex-post-facto` vindication of `i.logical` formalization 
as a `q.Naturalizing Phenomenology` project.  
On the other hand, critiques of the Semantic Web 
%-- which have emerged, among elsewhere, from Cognitive 
Linguistics %-- can accordingly be studied as potntial 
indications of how classical log formalism is 
limited in the phenomenological context.  Peter Gardenfors, 
for example, has critiqued the Semantic Web on thoretical 
grounds while also developing a model of Natural 
Language semantics (via a thory of `q.Conceptual Spaces`/) 
that we may find more phenomenologically realistic than 
the Semantic Web's (as Gardenfors puts it) 
`q.syllogistic` paradigms.  In this paper, I propose a 
critiquefrom a more `q.procedural` angle.  
From my perspective, the foundational characteristic of 
`q.information systems` is the existence of `i.procedures` 
(say, cognitive and/or computational procedures) which `q.act on` 
(aggregate, interpret, reshape) the data at hand.  I believe 
it is a fair critique to say that Semantic Web technology 
has unduly discounted the procedural dimension of 
information management %-- not only in a thoretical 
sense, but also quite practically.  For example, the 
OBO Foundary does not include a mechanism for code sharing 
with the goal of curating software libraries that 
implement atatypes conformant to the various 
published Ontologies.  In the Semantic Web paradigm, 
defining logical formalizations of standardized concept-systems 
is considered orthogonal to implementing software 
components where these formal criteria ar eralized in practic.  In 
short, `i.logical specification` is trated as distinct 
from `i.implementation`/.  This is not a universal 
approach: many trvhnological standards are published 
at least in part via `q,reference implementations` which demonstrate 
standardized concepts and guide other implementations, 
helping to enforce compatibiloty btween different 
software components.  Moreover, code sharing 
and code reuse can promote interoperability no less effectively 
than alignment relative to logically defined standards.  
So there `i.are` technological trends that emphasize 
`q.procedural alignment` and code-sharing as important 
contributors to data integration.  However, these branches 
of technology do not appear to have exerted a strong intuitive 
influence on the Semantic Web.  
`p`

`p.
I argued above that technology has a retroactive influence 
on Phenomenology, or at least on the threads of research 
that follow the `q.Naturalizing` project and the 
reconciliation of Phenomenology with Analytic Philosophy.  
Even if this effect is rather modest, it still bars on 
the topic that is my primary emphasis here, namely 
the integration of Phenomenology with Cognitive Grammar.  
One of the most prominent practical domains that has 
influenced Phenomenology has been the Semantic Web, insofar 
as Semantic Web technology (via Ontologies) show 
some evidence that formal models influencd by Phenomenology 
can be practically useful, which is one criteria to 
suggest that the models have philosophical or epistemological 
merit.  On the other hand, Cognitive Linguists have tended 
(if anything) to be critical of the Semantic Web (in 
contrast to other linguistics branches, which are generally 
sympathetic to Semantic Web paradigms and incorporate 
Ontologies into Computational Linguistics software).  
So a potentially fertile ground for collaboration between 
Phenomenology and Cognitive Linguists has arguably been 
overlooked insofar as the respective communities have 
taken competing lessons from th succsses and 
limitations of the Semantic Web, particularly how 
the Semantic Web leverages classical logical formalisms 
(particularly Description Logics) rather than 
`q.procedural` and/or Category-Theortic foundations. 
`p`

`p.
This problem is not intrinsic to the Semantic Web as a 
data-sharing platform, however, only to the 
paradigms through which the current Semantic Web 
has been conceptualized and implmntd.  There are competing 
intellectual frameworks that embrace parallel goals but 
present alternative technical foundations, and Phenomenology 
would benefit from thematizing these frameworks in a role 
analogous to the Semantic Web, both a practical application 
and a retroactive intuition-guide.  A case in point would be 
the OpenCog project, that presents a Hypergraph-based modeling 
paradigm related but technically distinct from Semantic Web 
labeled graphs (one which is also consistent with 
procedural-network models) and which also embraces a specific 
linguistic model (based on Link Grammar).  Philosophically, 
OpenCog appears to celebrate a vision of `q.Artificial 
General Intelligence` which I (and I suspect most phenomenologists 
would) find problemmatic; underestimating the context-sensitivity 
and empathic intersubjectivity intrinsic to human cognition 
and intelligence (and difficult to simulate with machins).  
Nevertheless, formal models designed to `i.replicate` intelligent 
behavior can still be useful as structural `i.models` of cognitive 
phenomena, even if we believe in a metaphysical gap between the 
model and the reality.  Implementing software on the basis of 
explanatorarily useful cognitive models does not 
guarantee that the software will realistically approximate human 
cognition, but articulating and fine-tuning the models 
themselves can notwithstanding be a valuable exercise. 
`p`

`p.
In the case of OpenCog, the impetus toward `q.Artificial 
General Intelligence` has motivated that project to explore 
cognitive models that coalesced around several key structures, 
including Directed Hypergraphs, Link Grammar, and 
(in my terms) Procedural Networks (as unerlying mols 
for Information Spaces).  This aggregate of theories 
can be juxtaposed to Description Logic, Directed Labeled 
Graphs, and formal Ontologies as the groundwork for 
the Semantic Web.  I'd also argue that the OpenCog model 
can potentially be a technological improvement over the existing 
Semantic Web, in the sense that semantic networks built around 
OpenCog-like structures can be more effective `visavis; several 
important practical concerns, like application design, 
data integration, and Human-Computer Interaction.  
These comments do not necesarily apply to the actual OpenCog 
software %-- the major OpenCog component, `q.AtomSpace`/, 
is in a practical sense harder to compile and use than most 
Semantic Web components %-- but instead to a 
potential standardization of the core OpenCog data structures 
as modeling paradigms that can be adopted by heterogeneous 
information sources and data-sharing initiatives.  In 
this eventuality, the OpenCog model can provide a 
test-bed for applied Phenomenology that stands on a 
different formal foundation %-- one less inured to 
symbolic logic.
`p`

`p.
Along these lines, then, I contend that a circle of 
data models analogous to the OpenCog architcture can provide 
a formal structuration for phenomenological accounts 
of linguistic processing and information spaces comparable  
in analytic roles to 3D modeling primitives in a Phenomenology 
of Perception.  On one side, mathematical elements like 
mesh geomtry, NURBS surfaces, and texture mapping/generation 
point toward formal theories of perceptual `i.cognition` 
by allowing for the construction of perceptually 
realistic `i.scenes`/.  These mathematical elements are 
building-blocks for an (artificial) perceptual `i.content` 
rather than (as far as we know) actual neurocognitive 
subvenants of perceptual `i.experience`/, but their 
formal specificity still gives us material to 
work with when trying to consolidate a `q.scientific` 
phenomenological research programme.  Analogously, 
I suggest that OpenCog-like structures such as 
Directed Hypergraphs, Procedural Network models, and 
Link Grammar are potential building-blocks for formal 
models of Cognitive Linguistics and `q.information 
management` %-- for our procssing of both linguistic 
content and the contxts and situations wherein language 
artifacts are grounded.  In other words, these 
procedural/hypergraph/link-grammar structures can model 
linguistic `q.deep structure` and linguistic environments 
by analogy to how mesh geometry, texture mapping, 
and so forth model 3D spatial primitives and 
visual-perceptual environments.
`p`

`p.
This idea is still somewhat hypothetical because the 
`q.procedural/hypergraph/link-grammar` nexus has not 
been consolidated into a general to the same degree 
as a common vocabulary of 3D modeling primitives has 
been incoporated into disparate software and research 
projects.  Having said that, Link Grammar itself does 
have standing as a distinct and institutionally 
circumscribed body of research, so it is a reasonable 
starting point for integration with phenomenological 
and cognitive-linguistic approaches, an integration 
which can then be extended to related structures like 
Procedural Networks and Directed Hypergraphs. 
`p`

`p.
For such reasons, I will conclude this paper with 
a focus on Link Grammar and on how the Link Grammar 
perspective relates to the phenomenological and 
cognitive-grammar positions and case studies I 
outlined in earlier studies.  Further extensions to 
Directed Hypergraphs and the technical foundations 
of multidimensional `q.procedural` type theories 
defined on Directed Hypergraphs %-- which thematically 
complement Link Grammar to the degree that Link Grammar 
and Directed Hypergraphs are naturally paired up as 
in OpenCog %-- are beyond the scope of 
this paper (though I have presented some form 
of this anlysis in other publications).
`p`

`p.

`p`
