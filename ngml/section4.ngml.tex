\section{Procedural Networks and Link Grammar}
\p{My goal in this section is to incorporate Link Grammar into 
a phenomenological and Cognitive Grammar perspective, more 
than to offer a neutral exposition of Link Grammar theory.  
Therefore I will accept some terminology and exposition 
not necessarily originating from the initial 
Link Grammar projcts (though influenced by 
subsequnt research, e.g. [expectation]).  I also 
want to wed Link Grammar to my own semantic intuitions, 
set forth earlier, that word-meanings and morphosyntactic 
interpretations should be grounded on pre- or para-linguistic 
cognitive \q{scripts} that are activated (but not 
structurally replicated, the way that according 
to truth-thoretic semantics linguistic form 
evokes-by-simulating propositional form) by linguistic 
phenomena.    
}
\p{Link Grammar is, depending on one's perspective, either related 
to or a variant of Dependency Grammar, which in turn is contrasted 
with \q{phrase structure} grammars (linguists tnd to designate 
competing schools with acronyms, lik \q{DG} for Dependency Grammar 
and \q{HPSG} for Head-Driven Phrase Structure Grammar).  
Link and Dependency Grammars define syntactic structures in 
terms of word-pairs; phrase structure may be implicit to 
inter-word relations but is not explicitly modeled by 
DG formalisms \mdash{} there is typically no representation 
of \q{noun phrass} or \q{verb phrases}, for example.  
Phrase structure is instead connoted via how relations 
fit together \mdash{} in \q{rescued dogs were fed}, for instance, 
the adjectival \q{rescued}-\q{dogs} relation interacts 
with the \q{dogs}-\q{fed} (or \q{dogs}-\q{were} plus 
\q{were}-\q{fed}) predication, an interaction that in a 
phrase-structure paradigm is analyed as the noun-phrase 
\q{rescued dogs} subsuming the noun \q{dogs}.  Dependency 
analyses often seem more faithful to real-world semantics 
because, in practic, phrases do not \i{ntirely} subsume 
their constitunt parts.  Linguistic structure is 
actually multi-layered, where semantic and morphosyntactic 
connections resonate between units within phrases separate and 
apart from how linguistic structure is organized into 
phrasal units themselves.
}
\p{Except for phrases that coalesce into pseudo-lexemes 
or proper names (like \q{United Nations} or \q{Member 
pf Parliament}), or indeed become shortened to single 
words (like \q{waterfall}), we perceive phrases both 
as signifying units and as aggregate structures 
whose detailed combinative rationale needs contectualization 
and interpretation.  In short, phrases are not \q{canned} 
semantic units but instead are context-sensitive performances 
that requir interpretive undrstanding.  This interpretive 
dimension is arguably better conveyed by DG-style models 
whose consituent units are word-relations, as opposed 
to phrase-structure grammars which (even if only by notational 
practice) give the impression that phrases conform 
to predetermined, conventionalized gestalts. 
}
\p{While Link and Dependency Grammars are both contrastd 
with phrase-structure grammars, Link Grammar is also 
distinguished than mainstream DG in terms of how 
inter-word relations are conceived.  Standard DG 
recognizes an assymetry between the elements in word-relations 
\mdash{} one element (typically but not exclusively a word) is 
treated as \q{dependent} on another.  The most common case is where 
one word carries greater information than a second, which 
in turn adds nuance or detail \mdash{} say, in \q{rescued dogs} 
the second word is more essential to the sentence's meaning.  
This potentially raises questions about how we 
can actually quantify the centrality of one word or another 
\mdash{} in many cases, for instance, the conceptual 
significance of an adjctive is just as trenchant as the 
noun which it modifies.  In practice, however, the salient 
aspect of \q{head} vs \q{dependent} assymetry is that any 
inter-word pair is \q{directed}, and one part of the relation 
defined as dependent on another, however this 
dependency is understood in a given case.
}
\p{By contrast, Link Grammar dos not identify a head-dependent 
assymetry within inter-word relations.  Instead, words 
(along with other lexically signifant units, like 
certain morphemes, or punctuation/prosodic units) are 
seen as forming pairs based on a kind of mutual 
incompleteness \mdash{} each word supplying some structural 
or signifying aspect that the other lacks.  Words, then, 
carry with them different varieties of \q{incompleteness} 
which primes them to link up with other modls.  Semantic 
and grammatical models then revolve around tracing 
the \i{gaps} in information content, or syntactic 
acceptability, which \q{pull} words into relation 
with other words.  This approach also integrates 
semantic and syntactic details \mdash{} unlike frameworks 
such as Combinatory Categorical Grammar, which 
also treats certain words as \q{incomplete} but 
identifies word connctions only on 
surface-level grammatical terms \mdash{} Link Grammar 
invites us to explore how semantic and 
syntactic \q{completion} intersects and overlaps. 
}
\p{Words can be incomplete for different reasons 
and in different ways.  Both verbs and adjectives 
generally need to pair up with nouns to form a 
complete idea.  On the other hand, nouns may be 
incomplete as lexical spotlights on the 
extra-linguistic situation: the important point 
is not that people feed dogs in general, but that 
\i{the rescued} dogs were fed prior to their rescue.  
So \q{dogs} \q{needs} \i{rescue} for conceptual 
specificity as much as \q{rescue} needs \q{dogs} 
for anchoring \mdash{} while also 
\q{dogs} needs \i{the} for \i{cognitive} specificity, 
because the discourse is about some prarticular dogs 
(presumed known to the addressee), signald by 
the definitive article.  In other cases, 
incompleteness is measured in terms of syntactic 
propriety, as in: 
\begin{sentenceList}\sentenceItem{We learned that people fed the rescued dogs.} 
\sentenceItem{No-one seriously entertained the belief that 
he would govern in a bipartisan manner.}
\end{sentenceList}
In both cases the word \q{that} is needed because 
a verb, insofar as it forms a complete predicate with 
the proper subject and objects, cannot 
always be inserted into an enclosing sentence.  
\q{People fed the rescued dogs} is complete as a 
sentence unto itself, but is not complete as a grammatical 
unit when the speaker wishes to reference the 
signified predicate as an epistemic object, 
something believed, told, disputed, etc.  A 
connector like \q{that} transforms the 
word or words it is paired with syntactically, 
converting across part-of-speech boundaries 
\mdash{} e.g. converting a proposition to 
a noun \mdash{} so that the associated words can be 
included into a larger aggregate.
}
\p{The interesting resonance between Link Grammar 
and Cognitiv Grammar is that this perspective allows 
us to analyze how syntactic incompleteness 
mirrors semantic incompletenss, and vice-versa.  
\q{Incompleteness} can also often be characterized 
as \i{expectation}: an adjective \q{expects} a noun 
to produce a more tailored and situationally 
refined noun (or nominal \q{idea}); a verb expects 
a noun, to form a proposition.  Analogously, 
when we have in discourse an adjctive or 
a verb we expectv a corresponding noun \mdash{} so 
via syntactic norms language creates certian expectations 
in us and then communicates larger ideas by 
how these expectations are met.  Is a noun-expectation 
fulfilled by a single noun or a complex phrase?  
The notion of smantic and syntactic expectations 
also coordinates nicely with type-theoretic 
semantics; for example, the verb \q{believe} 
pairs with a semantic unit that can be 
interpreted in epistemic terms \mdash{} not 
any noun but a noun of a kind that can be the 
subject of propositional attitudes (beliefs, 
opinions, assertions, arguments, etc.).
}
\p{The syntactic incompleteness of propositional 
phrases modified by \q{that} can therefore 
be traced to the semantic expectations 
raised by \q{believe}, and analogous 
verbs (opine, argue, claim, testify).  
Th object of \i{testify}, say, is a statement 
of potential fact which we know not 
to take as necessarily true or honestly 
made (part of the nature of testijony is that 
it may be deliberately or accidentally 
fallacious).  But to properly pair with 
\q{testify}, then, phrases must be 
semantically reinterpreted as nominalizations 
of propositions, rather than as mere linguistic 
exprssion of propositional content via 
complete sentences.  The \q{epistemic} 
context transforms sentential contnt into 
nominal contnt available for further refinement: 
\begin{sentenceList}\sentenceItem{The Trump campaign colluded with Russia.} 
\sentenceItem{Several witnesses testified that 
the Trump campaign colluded with Russia.}
\sentenceItem{Reputable newspapers have reported that 
the Trump campaign colluded with Russia.}
\sentenceItem{Most Democrats believe that 
the Trump campaign colluded with Russia.}
\end{sentenceList}
The grammatical stipulation that a modifier like 
\q{that} is often necessary in such formulations correlates 
with the semantic detail that the \q{claimed}, 
\q{testified}, or \q{believed} content is not being 
directly asserted by the spaker as if in a 
unadorned declarative expression, as in the 
first sentence.
}
\p{Morphosyntactic transformation similarly modls 
th corrlation btween semantic and syntactic 
expectation \mdash{} as can be demonstrated by a 
variant of the \q{believe} forms, via the phrase 
\q{believe in}:
\begin{sentenceList}\sentenceItem{I believed in Father Christmas.} 
\sentenceItem{I believed in Peace on Earth.}
\sentenceItem{I believed in Obama.}
\sentenceItem{I believed in lies.}
\end{sentenceList}
Whereas \q{that} (after \q{believe}) 
\q{nominalizes} propositions, \q{in} reconceives 
(type-theoretically we would say \q{coerces}) 
ordinary nouns into epistemic nouns (compatible 
with propositional attitudes).  Obama is not an 
\i{idea}, but the connector \i{in} triggers an 
interprtation where we have to read \q{Obama} as 
something believed \mdash{} effectively a type-theoretic 
tension resolved by understanding \q{Obama} in this 
context to designate either his platform or his 
ability to implement it.  Intrpretive \i{tension} 
is a natural correlary to a mismatch in xpectations: 
\q{believe} expects something epistemic, but the 
discourse gives us a proper name.  Analogously 
\q{budge} expects a brute physical ntity in 
its simplest meaning, but in \q{Obama wouldn't 
budge on reproductive rights} we get a \q{sentient} 
noun, and have to read \q{budge} metaphorically.  
In short, \i{expectation}, \i{interpretive tension}, 
and \i{incompleteness} are interlocking facets of 
semiotic primitives that gestate into discursive 
maneuvers via which ideas are communicated 
economically and context-sensitively.
}
\p{Link Grammar, proper, represents only the 
most immediate (mostly grammatical) facet of 
word links.  For sake of discussion, 
I will discuss links in general as markers of 
\i{mutual dependency} between words, so a 
\q{link grammar} is essentially a 
\q{bi-directional} Dependency Grammar.  Mutual 
dependencies manifest syntactic norms and 
contextual details that make individual 
words inadequate signifying vehicles for 
a particular communicating content.  This overall 
principle becomes concrete in one form via 
grammatical relations, which is the layer 
modeled by Link Grammar proper.  
I have mentioned several ready examples \mdash{} 
the syntactic dependency of verbs and adjectives 
on nouns for them to enter correctly into 
discourse (correlate with a semantic dependency 
in the other dirction, to shape noun-ideas to the 
proper context and signifying intent); also 
part-of-speech or \q{subtyping} dependencies 
reflecting mandates that (in my examples) 
propositional phrases are coerced to nouns or 
nouns coerced to \q{epistemic} nouns.  Technical 
Link Grammars recognize a broad spectrum of 
\q{link relationship} \mdash{} between 50 and 100 
for different languages.  Parsing a sentence 
is then a matter of identifying 
all of the mutual dependencies \mdash{} at least those 
evident on a syntactic level \mdash{} that appear as 
inter-word links in the sentence.  Phrase 
structure may be implicit in some links in 
combination \mdash{} for example verb-subject plus 
verb-object links generate propositional phrases 
\mdash{} but the technical parse is a \q{graph} of 
inter-word links rather than a \q{tree} of 
phrases ordered heirarchically.  The parse-graph 
itself is only a provisional rading of the 
sentence, and linguistic understanding 
exists only insofar as its skeletal outline 
is filled out with semantic and situational 
details.  But the graph layer articulated by 
a Link Grammar still provids a useful 
inermediat representation, showing 
mutual dependencies in their syntactic manifestations 
that then point toward thir deeper semantic and 
situational origins. 
}
\p{For each \i{syntactic} bi-dependnecy, on this theory, 
there is a concordant semantic and signifying 
bi-dependency, partly conventionalized as a feature 
of the language and partly hewn to the current 
discourse context.  To leverage Link Grammar 
in an overall Cognitive Linguistic environment, 
then, we need to examine the semantic relations 
that drive syntactic bi-dependency: how the 
grammatical structure of one word completing another 
is a codification of of \i{semantic} mutual dependency.  
The \i{semantic} bi-dependencies operate on both 
abstract and concrete levels.  Abstractly, 
it is obvious that an adjective or a verb dependents 
on a linked noun to complete an idea.  This 
abstract prototype of bi-dependency then takes 
concrete forms in each specific discourse, 
acquiring detail and specificity from 
situational contexts.
}
\p{The crucial dimension in this theory is 
neither abstract nor concrete bi-dependency 
but the intersection of the two.  The 
conventionalized lexical, syntactic, and 
morphosyntactic norms of a language present 
abstract prototypes of mutual word 
dependencies.  The concrete instantiation of 
these forms \mdash{} via word-pairs whose 
surface presentation indicates the 
presence of specific link relations 
(often with the aid of morphology, agreement, 
spoken inflection, and other 
morphosyntactic cues) \mdash{} invites us 
to consider how an abstract bi-dependency 
becomes situationally concretized in the 
present, momentary context.  In essence, 
abstract mutual dependencies are the raw 
materials from which situational appraisals 
are creates.  A pairing like \q{rescued dogs} uses 
a certain abstract-bidependent prototype 
\mdash{} here the double-refinement of a noun and adjective 
grounding each other \mdash{} to trigger 
the listener's awareness that the spakr's 
discourse is centered on one specific aspect of the 
dogs (that they were at some point rescued) 
with its concptual corrolaries and unstated assumptions 
(that, being in need of rescue, they were abandoned, 
in danger, and so on).  Similarly 
the further link to the definit article 
\mdash{} \q{\i{the} dogs} \mdash{} evokes the prototype of a 
definite article grounding a noun, which 
in turn communicates the speaker's beliefs 
about the current state of the discourse. 
}
\p{This last \q{bi-dependency} deserves further comments, 
because nouns more often than not reveal some 
dependency on an article: \q{some dog(s)}, 
\q{the dog(s)}, \q{a dog}, \q{many dogs}.  
These pairings paint a picture both through the 
choice of article and the presence or absence of a 
plural.  This picture is partly situational 
\mdash{} obviously whether the speaker is talking 
about one or multiple dogs is situationally 
important \mdash{} but it is also meta-discursive: 
selecting the definite article indicates the 
speaker's belief that the listeners know which 
dogs are on topic.  The lexeme \q{the} thereby 
signifies a meta-discursive stance as well 
as a cognitive framing \mdash{} both that the 
collection has enough coherence to function as 
a conceptual unit in context, as \i{the} dogs 
(and not something less specific like 
\q{\i{some} dogs}) \i{and also} that the 
speaker and listeners share compatible 
cognitive pictures as a result of prior 
course of the discourse.  This also introduces 
several avenues for future discursive evolution 
\mdash{} the listeners can respond to the 
speaker on both cognitive and meta-discursive 
levels.  A direct question like \i{which dogs?} 
signifies that the first speaker's meta-discursive 
were flawed \mdash{} the referent of \q{the dogs} has 
not been properly settled by the discourse to that 
point.  Or questions for clarification 
\mdash{} like \i{how many dogs are there?} \mdash{} indicate 
the listeners' sense that all parties' respectiv 
construal of the situation needs to be more neatly 
aligned for the discourse to proceed optimally.
}
\p{The point I particularly want to emphasize here, 
though, is that these discursive/cognitive effects 
inhere not only in the word \q{the} but in its pairing 
with other words, like \i{the dogs}.  We tend to see 
the lexical substratum of a language as the ground 
level of its signifying potential, but we should 
perhaps recognize bidependent prototypes as 
equally originating.  Th communicative 
content of \q{the dogs} is ultimate;y 
traced not only to the lexical potentialities 
of \q{the} and \q{dogs} as word-senses, but to the 
abstract prototype of the definite-article 
bidependence, which becomes concretized in the 
\q{the dogs} pairing at the same time as the 
individual words do.
}
\p{In order to bring this account full-circle, I 
would then add that lexical units mutually 
completed by an instantiated bidependence 
can also be seen as a tie-in between 
interpretive procedures.  Lexical interpretive 
scripts \mdash{} the cognitive processing solicited 
by \q{the} and \q{dogs} in isolation 
\mdash{} are themselves open-ended and un- or 
incompletely grounded.  We can speak metaphorically 
of \q{words} being incomplte, or carrying expectations, 
but it is really the cognitive scripts associated 
with words that are lacking in detail.  The resolution 
of a merely schematic cognitive outline to a 
reasonably complete situational construal 
can be likned to a rough sketch filled out in color 
\mdash{} but we have to imagine that a sketch can be 
completed by pairing it with a second sketch, and 
the content in each one, crossing over, allows a 
completed picture to coalesce.  \q{The} in itself 
evokes an interpretive process that in itslf 
cannot be completed, and likewise \q{dogs}; but 
each script supplies the content missing from 
the other.  In this sense the bidependent form 
concretized by the pair is actualy evoking 
an interpretive phenomenon of mutual completion 
\mdash{} the language structure here is guiding us 
toward an intrpretive interpenetration where 
the two scripts tie each other's open ends.  
Whereas lexical items can be associated with 
single \q{scripts}, prototypes of mutual 
dependency model patterns in how script-pairs 
can become mutually complete.  But unlike 
lexemes, which are notated directly by 
language, the instantiation of 
bidependnet script-pairs occurs indirectly.  
Some paired-up words are of course adjacent, 
but adjacency between words does not have the same 
binding determinacy as sequencing among morphemes 
in \i{one} word.  Instead, word adjaceny 
is only one signal among many others suggesting 
that some prototypical inter-word relation 
applies between two words (which might be 
widely separated in a sentence).  Morphology 
and syntax also point towards the pairings operative 
in a sentence \mdash{} they are to bidependency prototypes 
what word-choice is to the lexicon.  
}
\p{Thus far I have made an admittedly 
\i{philosophical} and speculative case for 
\q{interpretive mutual dependence} as 
a constituent building block of linguistic 
understanding.  This theory will remain troublingly 
incomplete if the more philosophical presentation 
cannot be wed to a more rigorous formal 
methodology.  True, an essential core of this theory 
is that interpretive \q{scripts} are largely 
prelinguistic and so not covered by linguistic 
formalisms in themselves.  However, I have also 
argued that formal linguistic structures \i{do} 
govern how we identify which links apply to 
which word-pairs and the gneral outlines of 
how word-pairing coordinats cognitiv processes associated 
with single words \mdash{} the fully contextualized 
synthesis of lexically triggered cognitive procedures 
may involve extra-linguistic grounding, but 
abstract prototyps of bidependnet relations 
are also prototypes of a synthesis between 
cognitive/interpretive functions.  It would 
accordingly be reassuring if notions like 
\q{bidependency} and \q{mutual completion} 
could be employed as foundations for a formal 
theory of grammar and/or semantics with a 
degree of rigor comparable to, say, Link 
Grammar in its computational form, or type-Thoretic 
Semantics in the sense of Zhaohui Luo, 
James Pustejovsky, etc... 
Such a theory \mdash{} and potentially concrete 
technologies associated with it \mdash{} would also 
then have a reasonable ground of comparison 
to the Semantic Web and, in the contxt of 
Phenomnology, to the formalizing influenc 
which Semantic Web paradigms have exerted on 
projects to unite Phenomenology with science 
and with Analytic Philosophy.
}
\p{Given these considerations, I propose that 
formal grammars with the same underlying 
structure as Natural-Language Link Grammars 
can indeed be used as a foundation for type-theortic 
and programming-language-design methodology.  The 
key step here is to generalize Link Grammar's 
notion of a \q{connector} \mdash{} the aspect of a 
word or lexeme that allows (or requires) compltion 
via another word \mdash{} to a gneric data structure 
where connections can be made between different 
parts of a system on the basis of double 
potentials that must be in some sense \q{compatible} 
for the connection to be valid.  One way to 
visualize such a systm is via graph thory: 
imagine a form of graps where nodes are 
annotated with \q{potentials} or \q{half-edges}; 
a complete edge is then a union of two half-edges.  
Half edges are also classified into different 
families, and there are rules governing when a 
hald-edge of one family may link with a half-edge 
of another.  In the case of Link Grammar, these 
classifications are based on surface language structure 
\mdash{} head/dependent and left-to-right relations \mdash{} 
from which a suite of links and connection rules 
are define (for instance abstractly a head/right 
word must link to a dependent/left word, a rule that 
then becomes manifest in specific syntactic rules, 
like how a verb links to its subject).  For a 
more generic model, however, we can stipulate only 
that there is \i{some} classification of connectors 
governs by \i{some} linkage rules, to be specified 
in different details for different modeling domains.
}
\p{Such a graph model expands upon the notion 
of \i{labeled} graphs, where edges are annotated 
with labels that characterize the kind of relation 
modeled via the edge itself.  A canonical example 
is Semantic Web graphs: the edges in any 
Semantic Web structure are labeled with \q{predicates}, 
defined in different Ontologies, specifying what sort 
of relation exists between its adjacent nodes.  
That is, in the Semantic Web, nodes are not 
\q{abstractly} linked but rather exhibit 
concrete relations: a person is a citizen 
of a country, two persons are married, and so forth.  
These structurs ar ethen concete instancs of Labeled 
Graps as abstract mathematical structures.  Based on 
Link Grammar, we can then refine this modle 
be splitting labels into two parts, and 
allowing edges to be incomplete: a fully 
formed edge is possible when the label-parts 
on one side are compatible with the label-parts 
on another.  One valid class of graph transforms 
is then a mapping where a graph is altered by unifying 
two half-edges into a complete edge, subject to 
the relevant linkage rules.
}
\p{Another way of modeling this kind of structure is via 
edge-annotations and a rule for unifying two edges into 
an edge-annotation pairing.  For sake of 
discussion, I will express this in terms of 
Directed Hypergraphs: assume that edges are 
\q{hyperedges}, connecting \i{sets} of nodes.  
In Hypergraph theory, the nodes incident to a hyperedge 
are divided into a \q{head set} and \q{tail set}; 
these sts can then aggregate as \q{hypernodes}.  
We can then define a kind uf unification where 
the \q{tail hypernode} of one hyperedges joins 
with the \q{head hypernode} of another, producing 
a new hypredge whose head comes from the 
first former hyperdge and whose tail comes from the 
secon.  The merged hypernodes, in turn, form a new 
hypernode which \q{annotates} the new hyperedge 
(this new hypernode is not connected to the graph 
via other nodes, but is indirectly \q{part} of 
the graph through the hyperedge it annotates).  
\i{Annotated} hyperedges therefore differ from 
\q{non-annotated} hyperdges in that the former are the 
result of a merger between two of the latter.   
The rules governing when such merger is possible 
\mdash{} and how to map a pair of hypernodes into a
single \q{annotative} hypernode (which 
belongs to the graph through the aegis of 
its annotated hyperdge) are not internal 
to the graph theory, but presumed to be 
specified by the modeling environment where 
implemementations of such graphs are 
technologically applied.  Annotated Directed 
Hypergraphs are then \q{complete} in a sense 
if every \q{un-annotated} hyperedge has been 
subsumed into an annotated hyperedge, via a 
fusional process we can call a  
\q{annotative-fusional transform}.
}
\p{Extending this model further, we can say that the 
tail of an \i{un-annotated} hyperedge is a 
\q{tail pre-annotation}, since it is poised to 
be merged into an annotation.  Analogously, 
the head of an un-annotated hyperedge is a 
\q{head pre-annotation}, and \q{annotative fusion} 
is the synthesis of a head and a 
tail pre-annotation (triggering a synthesis of 
their incident hyperedges).  Correlate to annotative 
\i{fusion} we can define a notion of annotative 
\i{partiality}, referring to the \q{incompleteness} 
of pre-annotations which leaves room for their fusion.
}
\p{It turns out that annotative fusion and partiality 
in this sense is a non-trivial 
model for computation in general, and can be extended 
to a form of type theory and process calculus.  
The idea is that computational procedures can be 
modeled as hypergaphs (computer source code 
can certainly be modeled as hypergraphs which 
are productions of a certain class of parsing 
engines).  Each \q{value} affected by a 
procedure \mdash{} or more technically the source code symbols 
and memory addresses that \q{carry} a value \mdash{} is 
then modeled as a hypernode that can link with 
other hypernodes in the scenario where one procedure 
calls a different one.  Annotative fusion is then 
a phenomenon of values being transferred from 
one excuction environment (associated with 
the caller procedure) to a second one (associated with 
the callee).  The \q{annotations} themselves 
are then in this context the full set of type 
coercions, type checks, synchronization 
(e.g. resource locks or thread blocks depending on 
whether or not the caller waits for the callee to 
finish), and any other validations to ensure that 
the procedure call is appropriate.  Annotative fusion 
also provides a formal basis for developing the 
intuition that \q{procedural networks} are rigorous 
representations of information spaces \mdash{} 
annotative fusions capture the precise details 
of procedures linking (via caller/callee relations) 
with other procedures.   
}
\p{The constituent units of procedural networks are 
inter-procedure calls \mdash{} but procedural networks 
also reveal dimensions of connectivity and 
and clustering characteristic of large, complex 
networks (and the graphs that represent them) in general.  
What appears as one function-call in source code can 
actually represent many different inter-procedure connections, 
a phenomenon reflecting \q{overloading} and \q{genericity} 
in programming language theory.  Functions are generic 
in the sense that any one of their arguments can take 
multiple types \mdash{} either because the function is 
explicitly declared to tak a \q{typeclass} or a single type 
for that argument, or because an instance of a given 
type may actually be at runtime an instance of some subtype.  
The engine which actually implements inter-procedure calls 
\mdash{} i.e., the programming-language implementation \mdash{} 
needs to factor this genericity into runtime decisions, so 
a single expression in one function body can branch 
to many different called procedures.  This is the 
essential core of the \q{semantics} of programming 
languages: data structures manipulated by computer code do 
not \i{intrinsically} represent real-world, 
non-digital phenomena, though they 
ar enginered to model external data when used properly.  
However a code base does \i{internally} posess a space 
of implementd functions, and a symbol at one 
place in source can match to some set of other 
functions so as to effect a procedure call.  This 
\q{matching}, and the rules governing how 
\q{overload resolution} occurs \mdash{} \q{overload} 
meaning that a given notated procedure call can actually 
branch to multiple functions, so runtime 
information is needed to select the right one \mdash{} are 
the essential formal principles governing the 
semantics of computer code.
}
\p{From this basis, all the same, computer code can model 
a wide range of empirical phenomena.  Generic code 
represents generic patterns of functional organization, 
allowing models to be built from varying layers of 
abstraction.  From this perspective, to 
describe an empirical system it is neessary to 
identify important behaviors and functional patterns 
via which the system's observed behavior can be 
notated and/or simulated.  To the degree that 
systems take on functional organizations that can be 
abstractly described, similar to the functional 
dynamics of other systems, their behavior 
can be represented and/or simulated via gneric 
cod.  To the degree that thre are particular 
details of a system's behavior that are more 
idiosyncratic to that system, and need to be modeled 
precisely, procedures can be crafted specifically 
for obsevring and emulating that exact behavior.  
More generic and more exact procedural implementations 
can coexist in a single code base, with generic 
functions calling granular functions narroed to 
precise types, and vice-versa.  The coexistence of 
generalization and specificty is an essential fatures 
of code bases and, by extension, of procedural networks, 
ensuring the flexibility of these ntworks 
as tools to model information spaces. 
}
\p{Unfortunately, this kind of \q{procdural} modeling 
is hard to intgrate with the more static techniques 
reprsentd by the Semantic Web and the current 
\q{Big Data} fad.  The latter paradigms tend to treat 
data as a static repository to be mined for patterns 
and insights, rather than a digital 
simulation or encoding of dynamic real-world systems.  
The Semantic Web, as a large, collaborative modeling 
project, evolved largely apart from the technological 
community concerned with computer simulations and 
the programming techniques which emerged from there, 
like Object-Orientation.  This divergnce is relevant 
for linguistics and cognitive science, because I would 
argue that the more \q{dynamic} paradigm is actually 
more \q{Semantic} in a Natural Language sense.  
In other words, our cognitive dispositions 
when interpreting empirical phenomena \mdash{} and 
matching these interpretations to linguistic 
cues \mdash{} are more like procedural networks 
capturing functional patterns and layers of genericity 
in observed phenomena, rather than an accretion of 
static data.  The techniques of procedural data 
modeling may therefore be relevant for Cognitive 
Linguistics and Cognitive Phenomenology 
because they aspire to something which, arguably, 
the mind does instinctively: build 
cognitive or computational models of 
dynamic, functionally organized phenomena.
}
\p{As a corrolary, the theoretical building-blocks 
of Procedural Data Modeling \mdash{} how it leverages 
type theory, programming language semantics, and 
so forth \mdash{} can provide at least 
analogs or case-studies for corresponding cognitive 
phenomena.  Here I would argue that generalizing 
Link Grammar from Natural Language to formal 
languages, type systems, and lambda calculii 
yields added structures to type thory that 
are useful toward a more rigorous \q{theory} of 
Procedural Data Modeling \mdash{} a thory of natural 
linguistics generalized to a theory of gneral data 
reprsentation which, in turn, may offer 
insights onto the cognitive dynamics underlying 
(prelinguistic) situational/perceptual 
comportments and interpretation.
}
\p{Type-theoretically, annotative partiality \mdash{} which 
recall is my terminology for the abstract 
generalization of the mutual \q{incompleteness} in 
Link Grammar connctors, driving their 
link-fusion \mdash{} extends conventional applied type theory 
(as in the Typed Lambda Calculus) in parallel 
to partial-labels extending labeled graph theory.  
It is paradigmatic in the theory of typed procedures 
and of \q{effect systems} that the type of a procedure 
is determined (up to certain equivalences that may discard 
overly granular type distinctions) by the types of 
all values affected by the procedure (including but 
not necessarily limited to the types of input 
and output parameters).  We can then superimpose 
on this model an account of annotative partiality.  
Specifically, on the paradigm that procedure-calls 
are structurally represented as annotative fusions over 
Directed Hypergraphs, the values manipulated by 
a procedure are pre-annotations: they are not 
(in the \i{implementation} of a procedure, 
as a formal object) single values but rather 
typed spaces that can take on a spectrum of 
possible values depending on the inhabitants of 
their types.  When a procedure is \i{called} these values 
become concretized, but as a formal system procedural 
networks model software in terms of its capabilities 
and expected behavior, rather than the state at 
any moment when the software is actually running.  
Partiality therefore models how precedures 
(as formal objects) deal with potentialities 
\mdash{} we do not know what values will \i{actually} 
be present at runtime (e.g. what specific 
values passed to a procedure as arguments), 
so procedural analysis is essentially characterized 
by a partiality of information.  When one procedure 
calls another, the caller must build an 
\i{expression} \mdash{} a gathering of values that provide all 
the information the callee requires \mdash{} thereby 
creating a case of mutual-completion: the caller has 
values but not an algorithm 
to operate on them; the callee has an algorithm 
(that's what it implements) but needs concrete 
values so to produce concrete values.  
This dual partiality allows the caller to 
call the callee, via an \i{expression} which is part 
of the callee's implementation (represented as a 
hypergraph) which must in turn match the 
callee's signature \mdash{} epigrammatically, we 
can say that \q{expressions are annotative-fusional 
duals of signatures}.  The point here is that whereas 
signatures are conventionally understood to be 
type-declarations assigning types to procedures, 
with annotative partially we can more 
precisely recognize signatures as stipulating 
\i{pre-annotative} types.  The values carried 
within expressions also have pre-annotative types, 
but there is a distinction between types in the context 
of expressions and types in the context of signatures 
\mdash{} and moreover this distinction is precisely the 
manifestation of the abstract head-pre-annotation 
and tail-pre-annotation contrast in the specific 
context of procedural networks.  Just as 
signatures unify multiple types into one 
profile, we can analogously define  
\q{expression types} as the aggregate of all types 
from values affecting the expression \mdash{} note that 
this is different from the type of the exprssion's 
calculated \i{result}, just as the type of 
a function is different from the type of its 
return value.  Expression-types and signature-types 
are almost exact duals (the complication being 
default values for optional parameters, which 
are not directly represented in exprssions 
\mdash{} obviously, since then they would not be 
missing).  The \q{duality} involved here 
derives from partitioning a type system into 
\q{expression annotative partials} and 
\q{signature annotative partials}, a projection 
of head/tail duality in an abstract theory 
of Annotated Directed Hypergraphs
(and analogous to head/dependent 
and left/right partiality in Link Grammar). 
}
\p{This notion of expression/signature duality is not 
just speculative; I believe it has rich potential 
for new techniques in data modeling, program analysis, 
and code verification, and is a core dimension 
of software distributed as part of a research-sharing 
platform, currently used in connection with 
several publications spanning multiple publishers 
and academic disciplines.\footnote{See Rodger2016, Prague2015, BHP2018, Neustein2019, 
and the \q{ScignScape} repository on github: 
most of these are examples where authors or 
publishers supplemented a prior book or article 
with a curated data set, or \q{Research Object}, 
where the data set is downloaded along with 
code from which customized software can be compiled specific 
to the data set and text being shared, 
software I devloped in part as a prototype or 
experimental sandbox for computational models 
mentioned here (in the case of the 2019 volume, part 
of Springer's \q{Advances of Ubiquitous Sensing in 
Healthcare} series, the data sets and publication 
are developed in consort).
}
The path I have sketched here from Link Grammar 
to a theory of Procedural Networks is similar, 
if not technically equivalent to, analogous 
analyses associated with OpenCog, for example 
(particularly in Ben Goertzel's two-volume 
\i{Engineering General Intellignce}).
}
\p{The software I just dscrribed in rlation to certain 
published data sets certainly is narrower and 
more nascent than OpenCog components like AtomSpace.  
At the same time, it has been more directly 
formulated in relation to academic and publishing 
workflows and is more precisely sited 
in academic and research-sharing domains (and moreover 
remains agnostic \visavis{} the more extravagant 
\q{Artifical General Intelligence} claims).
It is also almost entirely self-contained, 
with few external software dependencies, which 
arguably makes this software a more accessible 
test-bed for refining and analyzing formal 
properties of structures I have outlined here 
\mdash{} in particular a generalization 
of Link Grammar to a working model 
of computation, perhaps via what I have dubbed 
\q{annotative fusion}.  In any case, 
both OpenCog and, I would like to believe, 
this published software (designed to emulate some 
of OpenCog's architectural features but on a 
smaller-scale and mor self-contained foundation) 
are concrete technologies that take Link Gramar 
and Procedural Networks (in one sense or another) 
as modeling and linguistic paradigms.  In this 
sense they may be contrasted with Semantic Web 
technologies, which are based on labeled directed 
graphs and Description Logics.  Phenomenologically, 
we can then give formal analogs to back up the 
comparisons I intimated earlier: between the Semantic 
Web as a formal case-study and retroactive intution for 
Phenomenology, but one paradigmatically grounded 
in classical logic and truth-theortic semantics, 
and other paradigms \mdash{} here developd via 
Annotated Hypergraphs, annotative fusion, and 
annotative partiality add a new structural layer to 
lambda-calculus-style type thory \mdash{} informed 
via linguistic (rather than logical) methods like 
Link Grammar and arguably more compatible 
with Cognitive Grammar (and Cognitive Phenomenology).  
Here I make the implicit assumption that such an 
\q{annotative partiality} and hypergaph-based type 
theory prsents a formal foundation fundamentally different, 
in some philosophically interesting sense, from classical 
formal logic.  Fully defending this claim is outsude 
the scope of this paper, but I will offer a 
few comments on this topic in conclusion.
}
\p{}
