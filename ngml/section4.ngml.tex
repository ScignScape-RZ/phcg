\section{Procedural Networks and Link Grammar}
\p{My goal in this section is to incorporate Link Grammar into 
a phenomenological and Cognitive Grammar perspective, more 
than to offer a neutral exposition of Link Grammar theory.  
Therefore I will accept some terminology and exposition 
not necessarily originating from the initial 
Link Grammar projcts (though influenced by 
subsequnt research, e.g. [expectation]).  I also 
want to wed Link Grammar to my own semantic intuitions, 
set forth earlier, that word-meanings and morphosyntactic 
interpretations should be grounded on pre- or para-linguistic 
cognitive \q{scripts} that are activated (but not 
structurally replicated, the way that according 
to truth-thoretic semantics linguistic form 
evokes-by-simulating propositional form) by linguistic 
phenomena.    
}
\p{Link Grammar is, depending on one's perspective, either related 
to or a variant of Dependency Grammar, which in turn is contrasted 
with \q{phrase structure} grammars (linguists tnd to designate 
competing schools with acronyms, lik \q{DG} for Dependency Grammar 
and \q{HPSG} for Head-Driven Phrase Structure Grammar).  
Link and Dependency Grammars define syntactic structures in 
terms of word-pairs; phrase structure may be implicit to 
inter-word relations but is not explicitly modeled by 
DG formalisms \mdash{} there is typically no representation 
of \q{noun phrass} or \q{verb phrases}, for example.  
Phrase structure is instead connoted via how relations 
fit together \mdash{} in \q{rescued dogs were fed}, for instance, 
the adjectival \q{rescued}-\q{dogs} relation interacts 
with the \q{dogs}-\q{fed} (or \q{dogs}-\q{were} plus 
\q{were}-\q{fed}) predication, an interaction that in a 
phrase-structure paradigm is analyed as the noun-phrase 
\q{rescued dogs} subsuming the noun \q{dogs}.  Dependency 
analyses often seem more faithful to real-world semantics 
because, in practic, phrases do not \i{ntirely} subsume 
their constitunt parts.  Linguistic structure is 
actually multi-layered, where semantic and morphosyntactic 
connections resonate between units within phrases separate and 
apart from how linguistic structure is organized into 
phrasal units themselves.
}
\p{Except for phrases that coalesce into pseudo-lexemes 
or proper names (like \q{United Nations} or \q{Member 
pf Parliament}), or indeed become shortened to single 
words (like \q{waterfall}), we perceive phrases both 
as signifying units and as aggregate structures 
whose detailed combinative rationale needs contectualization 
and interpretation.  In short, phrases are not \q{canned} 
semantic units but instead are context-sensitive performances 
that requir interpretive undrstanding.  This interpretive 
dimension is arguably better conveyed by DG-style models 
whose consituent units are word-relations, as opposed 
to phrase-structure grammars which (even if only by notational 
practice) give the impression that phrases conform 
to predetermined, conventionalized gestalts. 
}
\p{While Link and Dependency Grammars are both contrastd 
with phrase-structure grammars, Link Grammar is also 
distinguished than mainstream DG in terms of how 
inter-word relations are conceived.  Standard DG 
recognizes an assymetry between the elements in word-relations 
\mdash{} one element (typically but not exclusively a word) is 
treated as \q{dependent} on another.  The most common case is where 
one word carries greater information than a second, which 
in turn adds nuance or detail \mdash{} say, in \q{rescued dogs} 
the second word is more essential to the sentence's meaning.  
This potentially raises questions about how we 
can actually quantify the centrality of one word or another 
\mdash{} in many cases, for instance, the conceptual 
significance of an adjctive is just as trenchant as the 
noun which it modifies.  In practice, however, the salient 
aspect of \q{head} vs \q{dependent} assymetry is that any 
inter-word pair is \q{directed}, and one part of the relation 
defined as dependent on another, however this 
dependency is understood in a given case.
}
\p{By contrast, Link Grammar dos not identify a head-dependent 
assymetry within inter-word relations.  Instead, words 
(along with other lexically signifant units, like 
certain morphemes, or punctuation/prosodic units) are 
seen as forming pairs based on a kind of mutual 
incompleteness \mdash{} each word supplying some structural 
or signifying aspect that the other lacks.  Words, then, 
carry with them different varieties of \q{incompleteness} 
which primes them to link up with other modls.  Semantic 
and grammatical models then revolve around tracing 
the \i{gaps} in information content, or syntactic 
acceptability, which \q{pull} words into relation 
with other words.  This approach also integrates 
semantic and syntactic details \mdash{} unlike frameworks 
such as Combinatory Categorical Grammar, which 
also treats certain words as \q{incomplete} but 
identifies word connctions only on 
surface-level grammatical terms \mdash{} Link Grammar 
invites us to explore how semantic and 
syntactic \q{completion} intersects and overlaps. 
}
\p{Words can be incomplete for different reasons 
and in different ways.  Both verbs and adjectives 
generally need to pair up with nouns to form a 
complete idea.  On the other hand, nouns may be 
incomplete as lexical spotlights on the 
extra-linguistic situation: the important point 
is not that people feed dogs in general, but that 
\i{the rescued} dogs were fed prior to their rescue.  
So \q{dogs} \q{needs} \i{rescue} for conceptual 
specificity as much as \q{rescue} needs \q{dogs} 
for anchoring \mdash{} while also 
\q{dogs} needs \i{the} for \i{cognitive} specificity, 
because the discourse is about some prarticular dogs 
(presumed known to the addressee), signald by 
the definitive article.  In other cases, 
incompleteness is measured in terms of syntactic 
propriety, as in: 
\begin{sentenceList}\sentenceItem{We learned that people fed the rescued dogs.} 
\sentenceItem{No-one seriously entertained the belief that 
he would govern in a bipartisan manner.}
\end{sentenceList}
In both cases the word \q{that} is needed because 
a verb, insofar as it forms a complete predicate with 
the proper subject and objects, cannot 
always be inserted into an enclosing sentence.  
\q{People fed the rescued dogs} is complete as a 
sentence unto itself, but is not complete as a grammatical 
unit when the speaker wishes to reference the 
signified predicate as an epistemic object, 
something believed, told, disputed, etc.  A 
connector like \q{that} transforms the 
word or words it is paired with syntactically, 
converting across part-of-speech boundaries 
\mdash{} e.g. converting a proposition to 
a noun \mdash{} so that the associated words can be 
included into a larger aggregate.
}
\p{The interesting resonance between Link Grammar 
and Cognitiv Grammar is that this perspective allows 
us to analyze how syntactic incompleteness 
mirrors semantic incompletenss, and vice-versa.  
\q{Incompleteness} can also often be characterized 
as \i{expectation}: an adjective \q{expects} a noun 
to produce a more tailored and situationally 
refined noun (or nominal \q{idea}); a verb expects 
a noun, to form a proposition.  Analogously, 
when we have in discourse an adjctive or 
a verb we expectv a corresponding noun \mdash{} so 
via syntactic norms language creates certian expectations 
in us and then communicates larger ideas by 
how these expectations are met.  Is a noun-expectation 
fulfilled by a single noun or a complex phrase?  
The notion of smantic and syntactic expectations 
also coordinates nicely with type-theoretic 
semantics; for example, the verb \q{believe} 
pairs with a semantic unit that can be 
interpreted in epistemic terms \mdash{} not 
any noun but a noun of a kind that can be the 
subject of propositional attitudes (beliefs, 
opinions, assertions, arguments, etc.).
}
\p{The syntactic incompleteness of propositional 
phrases modified by \q{that} can therefore 
be traced to the semantic expectations 
raised by \q{believe}, and analogous 
verbs (opine, argue, claim, testify).  
Th object of \i{testify}, say, is a statement 
of potential fact which we know not 
to take as necessarily true or honestly 
made (part of the nature of testijony is that 
it may be deliberately or accidentally 
fallacious).  But to properly pair with 
\q{testify}, then, phrases must be 
semantically reinterpreted as nominalizations 
of propositions, rather than as mere linguistic 
exprssion of propositional content via 
complete sentences.  The \q{epistemic} 
context transforms sentential contnt into 
nominal contnt available for further refinement: 
\begin{sentenceList}\sentenceItem{The Trump campaign colluded with Russia.} 
\sentenceItem{Several witnesses testified that 
the Trump campaign colluded with Russia.}
\sentenceItem{Reputable newspapers have reported that 
the Trump campaign colluded with Russia.}
\sentenceItem{Most Democrats believe that 
the Trump campaign colluded with Russia.}
\end{sentenceList}
The grammatical stipulation that a modifier like 
\q{that} is often necessary in such formulations correlates 
with the semantic detail that the \q{claimed}, 
\q{testified}, or \q{believed} content is not being 
directly asserted by the spaker as if in a 
unadorned declarative expression, as in the 
first sentence.
}
\p{Morphosyntactic transformation similarly modls 
th corrlation btween semantic and syntactic 
expectation \mdash{} as can be demonstrated by a 
variant of the \q{believe} forms, via the phrase 
\q{believe in}:
\begin{sentenceList}\sentenceItem{I believed in Father Christmas.} 
\sentenceItem{I believed in Peace on Earth.}
\sentenceItem{I believed in Obama.}
\sentenceItem{I believed in lies.}
\end{sentenceList}
Whereas \q{that} (after \q{believe}) 
\q{nominalizes} propositions, \q{in} reconceives 
(type-theoretically we would say \q{coerces}) 
ordinary nouns into epistemic nouns (compatible 
with propositional attitudes).  Obama is not an 
\i{idea}, but the connector \i{in} triggers an 
interprtation where we have to read \q{Obama} as 
something believed \mdash{} effectively a type-theoretic 
tension resolved by understanding \q{Obama} in this 
context to designate either his platform or his 
ability to implement it.  Intrpretive \i{tension} 
is a natural correlary to a mismatch in xpectations: 
\q{believe} expects something epistemic, but the 
discourse gives us a proper name.  Analogously 
\q{budge} expects a brute physical ntity in 
its simplest meaning, but in \q{Obama wouldn't 
budge on reproductive rights} we get a \q{sentient} 
noun, and have to read \q{budge} metaphorically.  
In short, \i{expectation}, \i{interpretive tension}, 
and \i{incompleteness} are interlocking facets of 
semiotic primitives that gestate into discursive 
maneuvers via which ideas are communicated 
economically and context-sensitively.
}
\p{Link Grammar, proper, represents only the 
most immediate (mostly grammatical) facet of 
word links.  For sake of discussion, 
I will discuss links in general as markers of 
\i{mutual dependency} between words, so a 
\q{link grammar} is essentially a 
\q{bi-directional} Dependency Grammar.  Mutual 
dependencies manifest syntactic norms and 
contextual details that make individual 
words inadequate signifying vehicles for 
a particular communicating content.  This overall 
principle becomes concrete in one form via 
grammatical relations, which is the layer 
modeled by Link Grammar proper.  
I have mentioned several ready examples \mdash{} 
the syntactic dependency of verbs and adjectives 
on nouns for them to enter correctly into 
discourse (correlate with a semantic dependency 
in the other dirction, to shape noun-ideas to the 
proper context and signifying intent); also 
part-of-speech or \q{subtyping} dependencies 
reflecting mandates that (in my examples) 
propositional phrases are coerced to nouns or 
nouns coerced to \q{epistemic} nouns.  Technical 
Link Grammars recognize a broad spectrum of 
\q{link relationship} \mdash{} between 50 and 100 
for different languages.  Parsing a sentence 
is then a matter of identifying 
all of the mutual dependencies \mdash{} at least those 
evident on a syntactic level \mdash{} that appear as 
inter-word links in the sentence.  Phrase 
structure may be implicit in some links in 
combination \mdash{} for example verb-subject plus 
verb-object links generate propositional phrases 
\mdash{} but the technical parse is a \q{graph} of 
inter-word links rather than a \q{tree} of 
phrases ordered heirarchically.  The parse-graph 
itself is only a provisional rading of the 
sentence, and linguistic understanding 
exists only insofar as its skeletal outline 
is filled out with semantic and situational 
details.  But the graph layer articulated by 
a Link Grammar still provids a useful 
inermediat representation, showing 
mutual dependencies in their syntactic manifestations 
that then point toward thir deeper semantic and 
situational origins. 
}
\p{For each \i{syntactic} bi-dependnecy, on this theory, 
there is a concordant semantic and signifying 
bi-dependency, partly conventionalized as a feature 
of the language and partly hewn to the current 
discourse context.  To leverage Link Grammar 
in an overall Cognitive Linguistic environment, 
then, we need to examine the semantic relations 
that drive syntactic bi-dependency: how the 
grammatical structure of one word completing another 
is a codification of of \i{semantic} mutual dependency.  
The \i{semantic} bi-dependencies operate on both 
abstract and concrete levels.  Abstractly, 
it is obvious that an adjective or a verb dependents 
on a linked noun to complete an idea.  This 
abstract prototype of bi-dependency then takes 
concrete forms in each specific discourse, 
acquiring detail and specificity from 
situational contexts.
}
\p{The crucial dimension in this theory is 
neither abstract nor concrete bi-dependency 
but the intersection of the two.  The 
conventionalized lexical, syntactic, and 
morphosyntactic norms of a language present 
abstract prototypes of mutual word 
dependencies.  The concrete instantiation of 
these forms \mdash{} via word-pairs whose 
surface presentation indicates the 
presence of specific link relations 
(often with the aid of morphology, agreement, 
spoken inflection, and other 
morphosyntactic cues) \mdash{} invites us 
to consider how an abstract bi-dependency 
becomes situationally concretized in the 
present, momentary context.  In essence, 
abstract mutual dependencies are the raw 
materials from which situational appraisals 
are creates.  A pairing like \q{rescued dogs} uses 
a certain abstract-bidependent prototype 
\mdash{} here the double-refinement of a noun and adjective 
grounding each other \mdash{} to trigger 
the listener's awareness that the spakr's 
discourse is centered on one specific aspect of the 
dogs (that they were at some point rescued) 
with its concptual corrolaries and unstated assumptions 
(that, being in need of rescue, they were abandoned, 
in danger, and so on).  Similarly 
the further link to the definit article 
\mdash{} \q{\i{the} dogs} \mdash{} evokes the prototype of a 
definite article grounding a noun, which 
in turn communicates the speaker's beliefs 
about the current state of the discourse. 
}
\p{This last \q{bi-dependency} deserves further comments, 
because nouns more often than not reveal some 
dependency on an article: \q{some dog(s)}, 
\q{the dog(s)}, \q{a dog}, \q{many dogs}.  
These pairings paint a picture both through the 
choice of article and the presence or absence of a 
plural.  This picture is partly situational 
\mdash{} obviously whether the speaker is talking 
about one or multiple dogs is situationally 
important \mdash{} but it is also meta-discursive: 
selecting the definite article indicates the 
speaker's belief that the listeners know which 
dogs are on topic.  The lexeme \q{the} thereby 
signifies a meta-discursive stance as well 
as a cognitive framing \mdash{} both that the 
collection has enough coherence to function as 
a conceptual unit in context, as \i{the} dogs 
(and not something less specific like 
\q{\i{some} dogs}) \i{and also} that the 
speaker and listeners share compatible 
cognitive pictures as a result of prior 
course of the discourse.  This also introduces 
several avenues for future discursive evolution 
\mdash{} the listeners can respond to the 
speaker on both cognitive and meta-discursive 
levels.  A direct question like \i{which dogs?} 
signifies that the first speaker's meta-discursive 
were flawed \mdash{} the referent of \q{the dogs} has 
not been properly settled by the discourse to that 
point.  Or questions for clarification 
\mdash{} like \i{how many dogs are there?} \mdash{} indicate 
the listeners' sense that all parties' respectiv 
construal of the situation needs to be more neatly 
aligned for the discourse to proceed optimally.
}
\p{The point I particularly want to emphasize here, 
though, is that these discursive/cognitive effects 
inhere not only in the word \q{the} but in its pairing 
with other words, like \i{the dogs}.  We tend to see 
the lexical substratum of a language as the ground 
level of its signifying potential, but we should 
perhaps recognize bidependent prototypes as 
equally originating.  Th communicative 
content of \q{the dogs} is ultimate;y 
traced not only to the lexical potentialities 
of \q{the} and \q{dogs} as word-senses, but to the 
abstract prototype of the definite-article 
bidependence, which becomes concretized in the 
\q{the dogs} pairing at the same time as the 
individual words do.
}
\p{In order to bring this account full-circle, I 
would then add that lexical units mutually 
completed by an instantiated bidependence 
can also be seen as a tie-in between 
interpretive procedures.  Lexical interpretive 
scripts \mdash{} the cognitive processing solicited 
by \q{the} and \q{dogs} in isolation 
\mdash{} are themselves open-ended and un- or 
incompletely grounded.  We can speak metaphorically 
of \q{words} being incomplte, or carrying expectations, 
but it is really the cognitive scripts associated 
with words that are lacking in detail.  The resolution 
of a merely schematic cognitive outline to a 
reasonably complete situational construal 
can be likned to a rough sketch filled out in color 
\mdash{} but we have to imagine that a sketch can be 
completed by pairing it with a second sketch, and 
the content in each one, crossing over, allows a 
completed picture to coalesce.  \q{The} in itself 
evokes an interpretive process that in itslf 
cannot be completed, and likewise \q{dogs}; but 
each script supplies the content missing from 
the other.  In this sense the bidependent form 
concretized by the pair is actualy evoking 
an interpretive phenomenon of mutual completion 
\mdash{} the language structure here is guiding us 
toward an intrpretive interpenetration where 
the two scripts tie each other's open ends.  
Whereas lexical items can be associated with 
single \q{scripts}, prototypes of mutual 
dependency model patterns in how script-pairs 
can become mutually complete.  But unlike 
lexemes, which are notated directly by 
language, the instantiation of 
bidependnet script-pairs occurs indirectly.  
Some paired-up words are of course adjacent, 
but adjacency between words does not have the same 
binding determinacy as sequencing among morphemes 
in \i{one} word.  Instead, word adjaceny 
is only one signal among many others suggesting 
that some prototypical inter-word relation 
applies between two words (which might be 
widely separated in a sentence).  Mrorphology 
and syntax also point towards the pairings operative 
in a sentence \mdash{} they are to bidependency prototypes 
what word-choice is to the lexicon.  
}
\p{}
\p{}
