
`section.Conclusion`

`p.
I contend we need to tease apart the pursuit of valuable computational 
tools and models from an (often reductionistic) paradigm of 
seeking artifical, computationally engineered replicas of 
human cognition.  `i.Computational` does not have to equal 
`i.AI`/.  
`p`

`p.
Holmqvist's and Selway's research that I have cited are good examples 
of paradigm-overlap between cognitive and computational 
linguistics.  I will cite other scholarship which 
also finds philosophical inspiration in cognitive linguists 
like Langacker, G\"ardensfors, George Lakoff, and 
Eleaonr Rosch, but which also target cognitive-science 
formalizations and `q.cognitiv architecture`/: 
`cite<AntonioLieto>; etc.  A recurring pattern in this 
scholarship is to `i.first` propose a structural intermediate 
representation %-- a model of intellectual structures which 
plausibly embody the processing of language and 
cognitive-perceptual content, partly abstracted from 
surface-level sensory or signifying details %-- and 
`i.second` propose algorithmic or software 
models of how our minds translate linguistic and perceptual 
givens to abstract, or partly-abstract, schema.
`p`

`p.
I have argued that we bring abstract situational prototypes 
to bear on understanding all of the world and social situations 
around us, and that language taps into these models so that 
people can coordinate situation-appropriate activity.  Given 
that there is an abstract and scehmatic dimension to 
how we understand situations, we should expect a 
partially abstract sheen to how we intellectually 
engage objcts and concepts once they are situationally 
`q.located`/.  Having identified objects as 
butter or carving knives, pitchers or glasses of 
water, wine or beer bottles, corkscrews and bottle openers %-- 
identifications themselves mediated by situational 
awareness, viz. if we are hosting or attending a dinner 
party %-- we no longer often attend actively to 
sense-perceptual minutiae.  Our mental map of our 
surroundings %-- there's the corkscrew, there's 
the carving knife %-- pulls these referents 
outside the register of sensate consciousness and 
into the pragmatic hum of worldly activity.  Insofar 
as they nestle in our intellectual faculties in that 
semi-abstract state, it seems fair to capture the 
schematic, structural appearance they have in 
this intellectual register %-- phenomena without the 
full-cloth phenomenology.   
`p`

`p.
This in turn seems to invite us to imagine how the 
structural essentials of such `q.pragmatic appearance` 
may be captured by computers.  We do not need to endow 
computers with human consciousness or emotions, because 
our mental traffic with the corkscrew or carving 
knife at some point evolves outside the sensate and 
passionate fabric of momentary consciousness.  There 
is a schematic and mechanical dimnsion of 
human action, and we can imagine computers 
simulating human intellligence at least on 
`i.that` theatrical level.
`p`

`p.
Or at least, such seems to be the intuition behind attemps 
to model our human representations of objects and 
concepts in terms of abstract structures.  But even a feasible 
theory of these semi-abstract layers of cognitive processing 
is only half the story.  Suppose we agree that there 
are legitimate cognitive insights in Holmqvist's model of cognitive 
frames, incorporating (but also extending, including in a more pragmatist 
direction) Conceptual Space Theory %-- employing 
a generalized mereology that renders objects and concepts 
as `i.parts` of situations (I have suggested a 
more conceptual-role account for analogous phenomena).  
Suppose also we find plausible cognitive-frame 
models in Selway's intermediate representation 
for natural language, via which 
his proposed implementation can potentially 
map natural language to formal specifications.  
In these cases we hagve potentially 
valuable Intermediate Representations which capture 
cognition, in effect, mid-stream, or in-the-act: 
neither conscious phenomenology nor neurophysical hardware. 
`p`

`p.
Howevere, Holmqvist's and Selways' work appears to 
operate in an environment where these 
Intermediate Representations are valued 
primarily because and insofar as they allow 
human cognition to be mechanically recapitulated.  
This of course demands not only that 
compuers `i.represent` IR models, but also `i.create` 
them %-- that is, when presented with an artifact of 
natural language, or the visual data of a scene, that 
computers should `i.automatically` map these givens 
to the theorized IR models, as if retracing 
the steps of human intelligence.
`p`

`p.
But just because IR models can be given 
computational form and representations, it 
does not automatocically follow that automated 
generation of IRs is possible or effective.  
We can and should thereby distinguish the computational 
`i.study` of cognitive Intermediate Representation 
from the AI vision of programming computers 
not just to `i.host` but to `i.derive` 
Intermediate Representations.
I am sympathetic to the former methodology 
but skeptical of the latter.
`p`

`p.
I also believe that most research in, e.g., computational 
linguistics, ends up conflating those two goals.  In that 
case, IR models are judged based on whether 
they facilitate automated, AI-driven generation 
of IR, not on whether the IRs are insightful 
suggestions of how human cognition itself 
builds an intermediary cognitive register %-- 
paricularly if we accept Vakarelov's overall 
picture of language as an interface between 
speech-givens and prelinguistic cognitive 
fsculties.  Interface theories and Intermediate 
Representations tend to go together %-- the IR is 
the representation of some input during 
intermediate processing yielding an 
output; a structure between two other structures, 
where the role of the interface is to bridge 
the structures as well as to activate the correct 
capabilities via the output.  This is the 
architecture of an `q.interface theory`/, in 
science or computer programming; it carries 
over to linguistics of we take Vakarelov's 
ITM seriously.  
`p`


`p.
An equally intrinsic aspect of interface theories, 
however, is that the processes operative at the intermediate 
level aree theoretically distinct from the realms which the 
interface bridges.  For example, the theory of programming-language 
compilers and runtimes is distinct both from 
the theory of programming-language parsers and 
specifications, and from the theory of CPU 
architecture and system-kernel development.  Runtime 
engineers can work through the medium of IR 
models, and compiler design itself is split between 
parsing surface-level source code `i.to` IR and 
mapping IR structures to their proper runtime 
paths of execution.  It would be a breach of 
design architecture to attempt to solve 
source-to-IR problems within modules devoted to 
IR-to-runtime engineering.
`p`

`p. 
Unfortunately, I get the sense that AI research does 
not respect a comparably disciplined Separation Of 
Concerns.  There are multiple parts to a 
typical AI platform %-- modules for representing information 
(or knowledge/facts/beliefs, or the state of the system's 
physical or digital environs, etc.); for populating 
these reprsentations with data deliberately introduced 
by human users or absorbed via some real-time engineering 
from the outside world; for analyzing 
reprsentations to glean insights or calculate a course 
of action.  Individual parts of the overall architecture 
can evince notreworthy engineering 
achievements, separate from the goals of 
the overall system.  In this sense the pursuit of 
AI can yield positive contributions in other 
branches of computer science and other disciplines, 
without the stated rationale of AI realizing 
(and monetizing) systems that exhibit 
humanlike intelligence.
`p`

`p.
So perhaps `q.AI` is 
best understood as shorthand for a suite of 
research agendas across several aspects of 
computer science, not restricted to the 
fields %-- like Machine Learning, Robotics, 
and Artificials Neural Networks %-- that are 
publicly associated with the term.  This is 
not, however, how AI seeems to be represented by 
companies and institutions (including in academia) 
who have a vested interest in the products AI may 
yield.  A benevolent reading would be that 
institutions understand the diversity of research 
that can be loosely aggregated under the AI umbrealla, 
but use the particularly science-fictional facets of 
this science to excite public support: visions 
of humanoid robots and conversationalists provide a compact 
story to that is more meaningful to non-experts 
than technical outlines of the intermediate machinery 
beneath the hopefully-intelligent surface.  
However, a more cynical interpretation 
is that AI is valued as a cash cow, and 
residual disciplines which contribute to the 
engineering infrastructure that AI requires 
%-- but are agnostic as to the AI vision 
itself %-- are appreciated only so much as 
needed to keep the AI project moving forward.  On that 
interpretation support for AI-agnostic 
research becomes lukewarm and transactional, and 
actual innovation in such areas may not be properly 
celebrated.
`p`

`p.
This situation is not irrelevant to either linguistics or to Cognitive 
Phenomenology.  In Computational Linguistics, 
for example, linguistic IR models seem 
to be valued based on their utility in AI-driven 
Natural Language Processing.  This presents a disciplinary 
bifurcation, where potential computational models 
are `i.either` connoted rather informally as part of 
a thoretical investigation among linguists 
(or philosophers of language, etc.), `i.or` 
concretely implemented, in some kind of software package, 
but then measured as components in a Natural Language Processing 
system: assessed on the basis of how the system overall 
approximates human language understanding via artifical means.  
Another genre of formal models, 
such as the type- or monadic theories I have alluded to here, 
may also have potential software incarnations but 
tend to be developed instead in a mathematical 
style, effectively `q.programming` in the abstract 
space of theorems and syllogisms rather than actual 
compuers.  Each of these methodologies skirt 
around the potential intermediary tie: 
concrete computational systems that are 
designed as exemplifications of semantic, grammmatical, 
or pragmatic theories, presented as hands-on 
software to anchor theoretical discussion but also intended 
as tools to advance the human study of language, rather 
than as steps toward synthetic avatars. 
`p`

`p.
At the same time, there is another side to the story: software 
implementations offer a focal center for research, something 
tangible that scholars can experiment and collaborate on.  
The AI story provides a target goal; it helps developers understand 
the local, technical code they are working on by 
connecting it to a larger system.  
Whatever philosophical objection one may have to AI 
initiatives, we should recognize the value of 
expanding academic and institutional practice beyond 
just writing and reading research papers.  Insofar as 
part of one's scholarly modus operandi can include 
writing computer code, and studying code 
repositories developed by others, we can benefit from a 
hands-on, even trial-and-error kind of experimentation.
`p`

`p. 
In effect: software which can be given concrete tasks %-- if it 
does `i.this` properly (whatever `i.this` is), 
then there is some larger thoretical point that 
is demonstrated %-- and then, incrementally, evolves 
to realize those tasks, provides a distinct form 
of intellectual engagement.  We get `i.that` to work, then 
`i.that`/, then fix `i.that`/.  This kind of 
`q.code-and-fix` cycle is quicker than conventional 
research, especially in the humanities, where the 
routines of authorship and publication and conferences can 
feel like they are unfolding in slow motion.  
`p`

`p.
Perhaps for this reason, some of the most interesting 
cognitive models hacve come from 
computational and academic environemnts informed by 
ambitious `q.Artificial General Intelligence` 
programmes, like Carnegie Melon University's 
OpenCog, and the `q.lmnTal` project at Waseda University 
in Tokyo.  These projects both employ 
formal-semantically expressive, hypergraph-oriented 
data systems that embody both the structural 
and procedural dimensions of computer systems %-- 
manifesting theories of both the execution 
of computational processes and the representation of 
formalized information.  These are important 
models even outside the Artificial General Intelligence 
ideology.
`p`

`p.
In fact, these are models which in linguistics 
and phenomenology may deserve more attention 
than Artificial General Intelligence `i.qua` 
philosophy.  But we should not discredit the 
role that Artificial General Intelligence may 
provide as a kind of intellectual compass helping 
scholars and engineers reason through the 
intrestitial machinery which may in fact be 
more real than the philosophical vision, but 
also less effective as theoretical `i.vie ferrate`/.  
Metaphors can triangulate research 
whereas analogies guide transfers of theories 
or methods between fields %-- i.e., analogies are 
more trustworthy landmarks than metaphors 
for surveying the envisioned future of a science 
%-- but metaphors can still be intuitive guides; 
maybe AI and Artificial General Intelligence can 
stabilize into our overall science and 
philosophy as a modest but suggestive metaphor.
`p`

`p.
I myself am most committed, however, to practical software that 
suggests interesting cognitive-humanistic paradigms without 
endorsing reductive AI hypotheses.  At the risk of 
seming to conclude with an infomercial, I'll 
cite as an example the Conceptual Space Type Theory 
and Type Expression Language (CSTX), which is 
currently used in the context of scientific data publication 
(see my forthcoming chapter in `cite<CyberphysicalSystems>;).  
CSTX presents a flexible type theory that can model 
both natural-langauge phenomena (such as  
Link Grammar, the internal parsing formalism in 
OpenCog, and the type-theoretic semantics favored by 
linguists such as Zhaohuu Luo or 
James Pustejovsky) as well as formal-language 
specifications for Software Language 
Engineering and Requirements Engineering.  
CSTX allows linguists to consider a type-theoretic 
representation of linguistic data, or language-as-interface
`q.intermediate structures`/, `i.without` presuming 
that automated (AI/machine-learning) systems 
could necessarily generate Intermeiate Representations 
without human intervention.  It is not 
a `q.practical` software system in the AI sense of 
enabling useful human-like behavior.
`p`

`p.
On the other hand, since CSTX `i.also` provides concrete 
software-development tools, it does have practical uses 
outside the AI paradigm.  In this sense it 
perhaps serves as a case-study in 
concrete software whose practical 
dimension spurs hands-on experimentation and decentralized, 
extra-institutional open-source collaboration, but whose 
theoretical commitments gravitate to 
cognitive linguistics and phenomenology 
%-- while bypassing an AI paradigm 
that underestimates the cognitive 
importance but complexity of social-situational 
awareness and of sensate consciousness.  
AI is not a canonical arbiter of software 
practicality (our contemporary 
instinct toward measuring all software 
around AI-driven analytics and `q.Big Data` 
reflects a clever marketing 
campaign by companies with financial incentives to 
prioritize AI research over other disciplines).  
Nor is AI a value-neutral or politically progressive 
vision of what human mind and society are like.
`p`

`p.
Sophisticated but philosophically and 
morally responsible cognitive-computational 
paradigms are probably more likely to 
arise from adding formal methodology and 
open-sourc experimentalism to a 
fundamentally humanities foundation, rather 
than bringing sensitivity to human 
nuance to a natural-science 
academic tradition.  The reasons for 
this are institutional as well as 
intellectual: insofar as formal-computational 
models are still rather unfamiliar in 
humanities contexts, practitioners in a 
hybrid cognitive-computational-humanities 
orientation can have a level of autonomy 
that helps us distinguish sophisticated computational 
models from simplistic philosophical 
(and commercial) paradigms.  
And the affordances of open-source code 
and digital publishing supports a 
robust but low-cost technological environment, 
tangential to academic laboratories and 
hierarchies.
`p`

`p.
Perhaps this open-source 
ecosystem is a worthy 21st-century field 
wherein to continue 20th-century phenomenology.  Let's 
not forget that phenomenology began as a philosophy 
of mathematics but evolved into a 
moral, political, and Existential system.  
Honoring the subtelty of human consciousness is a way 
to respect the technical priorities of 
phenomenological philosophy but also 
the political activism that %-- certainly 
often rendered into praxis by the 
intersectionality of lived experience with 
race, class, and gender %-- follows  
from phenomenological ethics.  
`p`

`p.

`p`

`p.

`p`














