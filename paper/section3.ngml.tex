\section{Channels and Edge Diversification}
\p{From here forward I will focus on the demo 
Virtual Machine as a case-study in multi-component 
hypergraph ecosystems.  
Graph representations have obvious applications 
to modeling computer code and computations.  
Intuitively, consider procedures as nodes; 
so edges incoming to or outgoing from nodes 
represent values passed to or 
returned from procedures.  In order to be a 
reasonable code representation, however, this 
model has to capture further details via 
edge-annotations; for example, grouping nodes and/or edges 
into higher-level units.
}
\p{As I argued last section, hypergraphs are useful targets 
for general-purpose programming language parsers because 
these general languages tend to have relatively complex 
grammars and special syntactic contexts.  While 
programming languages in a theoretical sense may 
have an expected translation to Abstract Syntax Trees, 
real-world languages embrace syntactic forms 
that complicate this abstract picture.  Earlier I cited 
\Csharp{}'s \LINQ{} as an example of an intra-linguistic 
special syntax; other examples include embedded \XML{} in 
Scala; attribute markings in \Csharp{} and \Java{}; 
template metaprogramming in \Cpp{}; regular-expression 
syntax in languages like \Perl{} and \Ruby{}; and so 
forth.  I have argued that a hypergraph (rather than 
graph or tree) paradigm is a more natural foundation 
for representing code in these practically evolving 
languages, which are designed around semantic expressiveness 
rather than an \i{a priori} commitment to theoretical 
compilation models. 
}
\p{As I will focus on in this section, similar comments apply to the 
semantics of procedures and function-calls.  The \q{mathematical} 
picture of procedures as functions mapping inputs to outputs is 
only approximately accurate in the context of modern programming 
languages.  One issue is that the distinction between input and output 
parameters is imprecise, given that input parameters can be 
modifiable references.  A common \Cpp{} idiom, for example, 
is to pass a non-constant reference instead of returning 
relatively large data structures \mdash{} mutable references 
are technically an input rather than output value, even 
if the intent is for a called procedure to supply data for 
the mutable input (this technique avoids extra copying of 
data).  Another complication is that procedures can have multiple 
sorts of input and output: throwing an exception, rather than 
returning a value, signals \q{exceptional} output 
(some \mdash{} presumably abnormal \mdash{} circumstance 
prohibiting a computation from proceeding as usual).   
Likewise, Object-Oriented languages' 
\q{message receivers} are input parameters with distinguished 
properties as compared to normal inputs.
}
\p{Since programming notation visually distinguishes procedure inputs 
from outputs, it is easy to think of the input/output distinction 
as mostly syntactic.  The significant differences, however, 
are more semantic.  Consider a \Cpp{} function which returns an 
integer: it is a compiler error to return from such a function 
without supplying a value; also, the called procedure has 
no access to the memory which would hold the return value.  
By contrast, a function which takes a non-constant 
integer reference is not forced by the compiler to 
modify that value; nor is it prohibited from accessing the 
input's value.  This points to the practical differences 
between returning values and altering input references, but it 
also indicates that inputs can be treated as \i{de facto} 
outputs by adhering to usage conventions \mdash{} a reference 
mimics the behavior of a return value to the extent that 
procedures initialize the reference on all execution paths, and 
refrain from using any value which the reference might 
have before the procedure starts.  Generalizing this point, 
the basic contrast between input and output can be 
seen as a semantic matter of usage protocol, rather 
than a syntactic contrast between passing valued to a 
function vs. accessing a function's returned result.
}
\p{Of course, input and output are usually both distinguished 
and interconnected by how outputs are used as inputs to 
other functions.  With respect to procedures which take 
one parameter, the general function-composition operator 
\fOfG{} takes the output of \gFun{} as the input to 
\fFun{}.  We could of course define a different operator 
where \gFun{} and then \fFun{} both run, taking the same 
value (potentially a reference-type value that \gFun{} 
could potentially modify before \fFun{} runs).  The 
former (composition) operator establishes an inevitable 
dependence of \fFun{} on \gFun{}: \fFun{} needs \gFun{}'s 
output for an input value.  The second case is less obvious. 
In a sequence like \fAfterG{} (\gFun{} runs then 
\fFun{} runs \mdash{} I notate the relation with an inverted 
semi-colon to highlight the parallels with \fOfG{}, but 
the analogous case in practice would be \gThenF{}), 
there may or may not be a \gFun{} side-effect that 
influences \fFun{} (even if they are passed the same 
inputs).  So inputs and outputs are distinguished 
by how output-to-input links \mdash{} outputs from one procedure 
becoming inputs to another \mdash{} have certain semantic 
properties in the context of functional composition, and by 
extension the relations between procedures which 
run in sequence.  Again, though, this call for a semantic 
analysis which looks beyond languages' surface syntax for 
function calls and return-value usage.
}
\p{Mutable references and procedures-with-side-effects expose limits 
in the representational expressivity of graphs (even hypergraphs) 
for modeling inter-procedure relationships.  Consider a case 
such as represented in Figure~\ref{fig:figGF}, 
\input{figGF} where \gFun{} modifies 
input $n$ that is then also input to \fFun{}.  Here \gFun{}'s 
effect on \fFun{}'s input is marked by a \gFun{}-to-\fFun{} 
arrow, but this directed edge implies a different semantic 
relation than \gFun{}'s output being \fFun{}'s input 
(as in \fOfG{}).  The specific difference can be visualized 
in a scenario like Figure~\ref{fig:figGFcomplex},\input{figGFcomplex} 
based on code such as written below the 
graph.  The situation here is that \fFun{} both uses 
\gFun{}'s output return (via the symbol $z$ the value is assigned to) 
and also the values held by $x$ and $y$ which are altered by 
\gFun{}.  I notate these dependencies by grouping the \gFun{}-to-\fFun{} 
edges through $x$ and $y$ separately from the edge 
with $z$ \mdash{} the latter edge conforms to the relation of 
a procedure taking another procedure's output as input 
(although here the relation is indirect through a 
lexically-scoped symbol), but the former edges reflect a 
different inter-procedure pattern.  The point of this 
example is that separating input and output edges is not 
enough to express program-flow semantics in anything but 
pure functional languages \mdash{} even given the \q{hypergraph categories} 
strategy of grouping input an output values into hypernodes.
}
\p{For situations like these I propose a notion of \i{channels}, which are 
higher-scale groupings of edges analogous to how hyperedges 
span groupings of nodes.  In Figure~\ref{fig:figGFcomplex}, 
the $x$ and $y$ edges are considered a separate 
channel than the $z$ edge, because $x$ and $y$ bridge \fFun{} and \gFun{} 
according to one sort of program-flow semantics (with side effects and 
mutable inputs passed to sequential procedures) while $z$ follows a 
different pattern (with output values and lexical symbols).  
The shaded box illustrates channels' role in 
gathering edges that have some particular connection 
which could not be expressly identified by other means (like 
shared annotations) and also isolating edges and edge-groups 
from one another as appropriate (here, isolating the $x$-and-$y$ pairing 
from the $z$ edges).   
}
\p{This case illustrates the basic premise of channels as 
hyperedge-groups, and also one modeling 
domain where I contend channels can be applied: specifically, 
representing details in computer code (in a general source-code hypergraph 
model) such as program flow.  
To discuss channels as graph elements further, I will 
start by examining channels in the specific computer-code 
context, which calls for a detour review of 
certain semantic structures central to the semantic of computer programs. 
}
\p{\parlead{Typed Values and \q{Careers}}  It is commonly accepted that a 
theory of types \mdash{} with at least some details adopted from mathematical 
type theory \mdash{} is a necessary component of a rigorous treatment of 
computer software and programming languages.  However, we should not 
neglect to keep in mind the specific realities of programming 
environments that complicate such basic notions as types' 
instances, the nature of entities that we should deem to be 
bearers of types, the status of special (non-)values like 
\lstinline{null} or \q{Not A Number}, and related details that 
contrast programming \q{types} from mathematical sets or \q{spaces} 
in various incarnations (sheaves, topoi, discrete topologies, etc.).  
}
\p{In particular, the \q{instances} of types (at least in the programming-language 
context) are not particularly suited to the intuitions of \q{members} of 
sets, or \q{points} of spaces.  One reason is that there is no fixed theory 
of types' extension, or \q{set} of possible instances,
in a given computing environment.  For example, types without a fixed size 
in memory \mdash{} say, lists of numbers \mdash{} are constrained be a computer's 
available memory.  We cannot be certain which lists are small enough that a 
given environment can hold them in memory, so that they are available 
as typed values for a program.  Conceptually, then, 
types are quite different from sets of values. 
}
\p{More significantly, types' instances do not have a kind of 
abstract self-sufficiency like points in a mathematical space.  
Consider how computational procedures deal with \q{values}, the 
things that are representatives of types.  Most values which 
procedures encounter are produced by other procedures, either 
passed as parameters from calling procedures to called procedures, 
or returned as outputs from called procedures to calling procedures.  
True, programs do execute in a world of surrounding data \mdash{} on computer 
drives and in memory, plus \q{CyberPhysical} data from devices 
hooked up to computers (at a minimum, devices crafted to be 
part of a computer set-up, like mice and keypads; plus often 
\q{smart} devices which software accesses to manage an oversee).
}
\p{Yet from a programmer's point of view, this data is accessed by 
calling specific functions, albeit ones which 
(at the lowest level) programmers themselves do not 
implement (viz., the procedures exposed in device \q{drivers}).  
Analogously, many values are presented directly in source-code 
literals; but we should think of these values as likewise 
yielded from procedures that read source tokens or strings 
and interpret the intended type-specific value (e.g., converting the 
character-string \q{123} to the corresponding integer).  
Typically these \q{reader} procedures occur behind-the-scenes; 
although, which helps demonstrate the point that source literals 
involve implicit function-calls, some languages support 
\q{user-defined} literals with nonstandard syntax \mdash{} 
consider \Lisp{} \q{reader macros} and \Cpp{}'s 
\lstinlinestyle{\operatorqq{}}.
} 
\p{Consequently, analysis of programs and computer code should 
proceed from the perspective that all typed values are introduced 
into a running program as results of procedure-calls.  We therefore 
have extra detailing in any context where we would talk 
about \q{typed values}.  The situation could be analogously 
compared to settings like approximation theory \mdash{} consider an 
analysis according to which a spatial magnitude, say, is not 
just a real number but the result of a measurement operation.  
By extension, quantities that on purely mathematical 
terms might be treated as points in spaces such as the real 
number line are \q{packed} into \i{observations} which can be 
sites carrying further information, such as an approximation 
factor.  My point is not that computer programs fail to work 
with exact values \mdash{} in a typical case a single typed value 
does not need an approximation \q{window} unless the software 
is manifestly interacting with physical systems where measurement 
uncertainty is taken into consideration.  However, approximation 
as \q{extra structure} is a useful analogy for the 
added detail latent in programming theory given that 
typed values are not \q{freestanding} mathematical entities 
but are always packaged into digital structures that make 
them available for software procedures.
}
\p{For sake of discussion, I will use the term \i{carriers} to 
describe the structures that \q{package} typed values.  
When a procedure returns a value, said value is embedded 
in a carrier which \q{transports} it to other procedures.  The 
\q{things} exchanged between procedures are actually, then, 
carriers rather than values \mdash{} we should picture 
carriers traveling on \q{routes} laid out in source-code 
hypergraphs.  Carriers, in particular, are more general 
than values.  Consider a function which, on one occasion, 
throws an exception rather than returning a value.  The carrier 
arranged to hold the return value is therefore empty.  Carriers, 
then, can be in a \i{state} other than that of holing a valid, 
initialized value.  Some programming language accommodate these 
cases via special \lstinline{null} or \lstinline{Nothing} values, 
but these are still constructs inside the language's type 
system.  Thinking in terms of carriers, instead, removes 
certain \i{states} from the type system: an empty or 
pre-initialized carrier does not have some special \i{empty} 
value; it does not have a value at all.  Similar comments 
apply to carriers which hold values which were once valid but are 
no longer so, like deleted pointers.  Any carrier potentially has a 
\i{career} which starts with no value; then is initialized 
with a value (and maybe gets updated over time); and may then 
\i{retire} and cease to hold a legitimate value.  Only in the 
middle phase of this trajectory does the carrier's range of 
possible states correspond with potential instantiations of 
a type (I assume here that each carrier is associated with 
one canonical type).  
}
\p{Carriers are central to my treatment of channels, so 
I will discuss carriers on their own terms in more detail.
}
\p{\parlead{Carriers and Categories of Types}  In some contexts it is 
useful to examine a type system \tys{} via a category \tyCat{} of 
types (such as, the category \HaskCat{} of Haskell types).  
Carriers are an extra structure which rest \q{on top of} 
types, calling for the foundation (including Category-Theoretic 
treatment if applicable) to be reconsidered.  First, 
the Haskell-like interpretation of \tyCat{} morphims 
should be redefined.  On the usual account, \tyOneToTyTwo{} morphisms 
are functions mapping type \tyOne{} to \tyTwo{}.  This 
picture is complicated by procedures deviating from pure functions: 
as I discussed earlier in this section, the input/output distinction 
is complex and does not precisely map to the mathematical model 
of morphisms between Category-objects.  Instead, inter-type 
morphisms (to the degree that these are identified as categorial 
morphisms establishing type systems as categories) should be restricted 
to specialized semantic associations between types, such as 
casts from integers to floating-point values (hence, morphisms between 
integer types to floating-point types).
}
\p{Moreover, reasonable type systems have a nontrivial collection of 
endofunctors marking inter-type associations.  A good example 
is the \q{unreferencing} operator which recovers the underlying 
type \ty{} from a reference type \tyRef{}.  This operator naturally 
translates to a function \tyRef{} $\mapsto$ \ty{} that retrieves the 
value held by a reference.  It is appropriate to analyze this 
function as a \mbox{\tyRef{} $\rightarrow$ \ty{} morphism}.
}
\p{In a general case, though, implemented procedures should be construed 
apart from such basic morphisms that tie a type system together: a 
general procedure which takes a \tyOne{} input and outputs \tyTwo{} 
is not a \tyOne{} $\rightarrow$ \tyTwo{} morphism but a more complex 
structure operating on carriers.  For each input and output parameter 
there is a carrier accessible in the environment of the procedure 
(its program code and its runtime environment) that provides the 
procedure access to an underlying value.  The specific access can be 
modeled around different sorts of carriers \mdash{} carriers holding 
return values should allow procedures to \q{write upon} (initialize or 
modify) their values but not read them (conformant to the expected 
semantics of \q{procedural output}).  In this theory, the 
output carrier from one procedure is distinguished from the 
input carrier for another procedure, even in a case where one's 
output is directly used as the other's input.  To illustrate, 
Figure~\ref{fig:figGFc} \input{figGFc} shows a modified 
version of Figure~\ref{fig:figGF} where the \gFun{}-to-\fFun{} 
edge is annotated by a pair of carriers (output and input 
differentiated by thick vs. thin borders).  Similar to 
Link Grammar (see the last section), a single edge bears a 
double-annotation to indicate that the edge depends on some 
notion of compatibility: in particular, output-to-input 
carriers can only be linked if they hold the same type 
(or at least interconvertible types).  As such, a double-annotated 
edge captures a situation where the value in one carrier is 
transferred into a second carrier \mdash{} potentially with a type-cast or, 
cf. in \Cpp{}, a behind-the-scenes call to a copy-constructor.  
I will refer to such a value-transfer as a \i{handoff}.  
A \gFun{}-to-\fFun{} (hyper-)edge therefore expresses added details 
because its annotations convey properties of carriers whose state 
and compatibility determine the value-transfers that enable 
inter-procedure relations, such as one procedure calling and/or 
using the output from a different procedure.  
}
\p{In lieu of mostly theorizing type categories, then, we can 
also consider a category \CarCatS{} of carrier \i{states}, which 
generalize typed values.  A morphism in \CarCatS{} represents either a 
change in state or a mapping between values.  When a carrier \cCar{} is 
initialized, it transitions from being in a state with no value 
to being in a state where it holds an instance of its type 
(when a carrier is already initialized its state can change 
by replacing its held value with another value; or by 
becoming \q{retired}).  A \cCarOne{}-to-\cCarTwo{} \q{handoff} then 
corresponds to a morphism in \cCarTwo{}'s state wherein 
\cCarTwo{} (perhaps transitions out of a pre-initialized state and) 
acquires a concrete value derived from \cCarOne{}'s.  
This case \mdash{} I'll call it a \q{handoff-induced morphism} 
\mdash{} should play a foundational role in analysis via carriers and 
hypergraphs, analogous to \tyOne{} $\rightarrow$ \tyTwo{} morphisms 
in conventional category-theoretic program semantics. 
}
\p{After accounting for this change in perspective, many concepts of hypergraph 
categories appear to carry over to this theory.  Rather than morphisms 
in a type category \tyCat{}, (hyper-)nodes in this case represent 
procedures, and their surrounding edges represent (via annotations) 
carriers accessible (as inputs or outputs) to the procedure's 
implementation.  Morphisms are represented \q{within} these 
edges insofar as a procedure being called induces a change in state 
for carriers along the procedure's incoming edges 
(likewise, returning values induces state-changes among 
outgoing edges).  
}
\p{In practice, procedures call other procedures.  A procedure's 
input carriers can be augmented by a \q{lexical scope}; 
a repository of local carriers (syntactically speaking, 
lexically-scoped source-code symbols) which start as uninitialized but 
can hold outputs from other procedures called from the current 
procedure's implementation body.  Each implementation is then a 
sequence of further procedure call wherein the calling procedure 
assembles the requisite local carriers to provide inputs to 
the called procedure (via handoffs).  The calling procedure 
must also provision carriers to hold the called procedure's 
output.  In effect, the calling procedure must orchestrate a 
suite of handoffs among its own carriers and the those 
of the called procedure.  The semantics of this multi-faceted 
sum of carrier-transfers can be represented within a source-code 
graph; and so, the structures and semantics of these hypergraph 
representations should be codified in a manner which 
renders (a de-facto Ontology) the patterns of aggregate 
carrier handoffs). 
}
\p{To illustrate, Figure~\ref{fig:figGFcomplexc}\input{figGFcomplexc} modifies 
Figure~\ref{fig:figGFcomplex} so as to show carriers and handoffs, 
analogous to Figure~\ref{fig:figGFc} adding carriers to Figure~\ref{fig:figGF}.  
Each case where one procedure calls another can be modeled via a 
graph which groups together the carriers (seen in the context of the 
calling procedure) that are packaged together to enable the call; 
and concordantly the links between these carriers and those 
in the called procedure's signature and implementation which 
embody handoffs executed in the transition in program flow between 
the calling procedure to the called procedure, and back.
}
\p{\parlead{Carriers and Channels}  The theory of channels comes 
into effect insofar as we combine this carriers and handoffs 
model with my earlier discussion of differentiating input and 
output varieties.  For instance, inputs may be Object-Oriented 
message-receivers or normal parameters; outputs may be 
thrown exceptions or normal returns.  In the framework 
I just outlined, hyperedges carry double-annotations to 
mark the pairs of carriers involved in a handoff within a 
procedure-call.  Grouping carriers based on their common 
semantics \mdash{} separating the \this{} object from non-distinguished 
parameters, let's say \mdash{} involves a further level of organization, 
grouping hyperedges into channels.  Taken together, then, the 
overall hypergraph system exhibits two generalizations from 
simpler code-graphs: double-annotations on edges (based 
on carrier theory) and the use of channels to reflect 
higher-level programming semantic, such as the distinct status 
of method-receivers as non-standard input and of exceptions 
as non-standard output.\footnote{My forthcoming chapter in 
\cite{NathanielChristenCyberphysical} includes examples of other 
more \q{exotic} kinds of channels as well.}  
}
\p{To develop this unified theory, we can consider a \i{channel} as 
(semantically) a (possibly empty) collection of (one or more, 
if not empty) carriers \mdash{} the corresponding hypergraph representation 
is a channel grouping edges whose annotations refer to these 
carriers.  Each channel is then associated with a specific 
\q{protocol}, an idiosyncratic method for procedures to 
interact with carriers (and by extension with one another).  
For example, the protocol for regular input and output channels 
is that one output carrier's value can be directly \q{handed off to} 
an input carrier, as in \fOfG{}-like composition.  In many languages a 
procedure can have at most one output value, and also at most one 
\q{message receiver} (\this{}) object.  Accordingly,
a procedure-call can bundle the output channels from multiple 
other procedures to populate some further called procedure's 
input parameters.  Since there is only one \q{\this{}}, 
two methods can be combined (modulo type compatibility) by inserting 
the prior method's output as the next method's message-receiver 
(handing off the output value to the next \this{} carrier), 
giving rise to \q{method chaining} (such call-sequences can continue 
over multiple steps).  Meanwhile \i{exceptions} cannot be treated 
as normal values (you cannot in most cases hand off between an 
\q{exception channel} and normal carriers).  Instead, 
procedures which want to \q{catch} exceptions need special 
structures, like catch-blocks, which provide the atypical 
carriers that can actually hold exception values (and so 
receive exception-channel handoffs.
}
\p{All of these are customary coding structures and design patterns which 
regulate how modern programming languages handle features like 
method-calls and exceptions.  These patterns, which I am theorizing here 
according to channels' semantic protocols, are not primarily grounded 
in a language's type system but rather in regulations on procedural 
flow and program organization that are orthogonal to type restrinctions.  
I contend that a theory of channels can unify the regulations enforced 
by strong typing (e.g. not calling procedures with incompatible 
arguments \visavis{} expected types) and those due to inter-channel 
semantic protocols.  What the two semantic regimes have in common 
is that they ultimately manifest as rules on carrier-transfers \mdash{} 
both carriers' types and the kinds of channels to which they 
respectively belong influence whether a purported handoff 
(needed by a call between procedures) is permissible; if not, 
the code declaring the relevant procedure-call should not 
be accepted (it should not compile).  So, enumerating 
channel-semantic protocols is a way to formally model 
what inter-procedure relations are legal accoring to the 
program-flow and organizational principles of the 
implementation language, analogous to how procedure 
calls are evaluated for compliance with the 
language's type system.  The syntactic rules for how 
procedure-calls are notated are driven in part by 
inter-channel protocols (for example, using a dot 
to separate a \this{} object from a method-name 
allows method-chains to be compactly written), but 
a rigorous model should take syntactic details as 
epiphenomena \mdash{} the important details for hypergraph 
source-code models is the kinds of channels recognized 
by a language for inter-procedure communication, and the 
protocols associated with various channel-kind pairs 
governing how and when carriers in one channel may be linked 
by handoffs to carriers on other channels (of the same or 
other kinds).
}
\p{One benefit of the channel-based framework I have presented here is 
that it provides an intuitive breakdown of procedure-calls into 
smaller steps.  A procedure call generally requires a structure involving 
multiple carriers to be present in the context of the calling procedure 
(while, on the called side, the procedure's signature provisions a 
set of carriers that receive values from the calling context).  
Preparation for the call therefore entails building up a set of 
carriers and establishing their respective links to carriers on the 
receiving end.  It is possible to divide this process into a 
series of minimal sets, each involving operations on a single carrier.
Via this interpretation, graphs modeling computer code 
with channels an carriers can be translated into 
sequences of minimal carrier-related instructions, which in turn can 
be executed as a kind of virtual machine.  
The scripting environment I am using in this paper for 
demonstration \mdash{} which in the last chapter I covered 
mostly from a grammar and parsing angle \mdash{} therefore 
culminates in data structures constituting an Intermediate 
Representation used by a runtime engine to translate the 
compiled code to internal (mostly \Cpp{}) function-calls.  
This involves two software components \mdash{} an algorithm to 
walk through code graphs to generate Virtual Machine code 
and then a runtime library to execute the code thereby 
generated.  I will briefly summarize both components 
as case-studies in how hypergraph code representations can 
be transformed into concrete execution environments.
}
\p{\parlead{A Hypergraph-Based Virtual Machine}  According to channel theory, 
an operational execution environment is, first an foremost, a 
\q{virtual machine} which defines and operates on carriers, packages them 
into channels, and then uses bundles of channels 
as specifications for procedure-calls.  When \i{creating} virtual machine 
code, we can then proceed by stepping through graph elements representing 
channels and carrier, emitting data as needed to reconstruct 
channels via minimal operations.  When \i{running} virtual machine 
code, the important step is packaging groups of channels into a 
single data structure (for each distinct procedure call) and then 
mapping the resulting structure to a binary array which mimics the 
\ABI{} (Application Binary Interface) of target procedures \mdash{} so 
as to produce function-calls via the virtual machine which, at the 
binary level, appear to be the result of compiled (e.g., \Cpp{}) functions.
}
\vsftcl{!h}{figPVM}
\p{Listing~\ref{lst:figPVM} depicts part of a virtual machine file generated 
by the demo from a script-like source file.  This kind of file 
(which I call \PVMIR{}, for \q{PhaonGraph Virtual Machine}) 
is the final output of the compiler pipeline described in this and 
the last section.  The basic premise behind this format is that 
every procedure call can be associated with a \i{channel package}, 
each of whose channels is a stack of carriers.  It is straightforward 
to generate \PVMIR{} code tracing a series of instructions to 
recreate a channel package held as a structure (in the demo, 
a \Cpp{} object) in memory.  Moreover, the \q{Phaon Graph} format is 
designed to represent information associated with channels and 
carriers; so generating \PVMIR{} (at least on the level of 
individual procedure calls) is facilitated by 
constructing graphs in the \q{Phaon} format to begin with.  
I use the term \q{RelaeGraph} for graphs generated from 
\q{relae} parsers; in the overall pipeline then the transformation 
from Relae-Graphs to Phaon-Graphs is the important 
step which enables the \PVMIR{} generator to run mechanically.  
}
\p{As described, \PVMIR{} plays a limited role insofar as it is 
generated from channel-package objects \mdash{} yielding 
\PVMIR{} files \mdash{} and then sometime later a virtual machine 
runs the \PVMIR{} code to recreate those objects at runtime.  
In this role \PVMIR{} just allows the channel-package objects 
to be encoded in a text file for later use.  
However, \PVMIR{} is more than only a serialization format 
for channel packages.  The extra roles for \PVMIR{} come 
into play when defining source-code-level procedures 
(which includes code blocks, e.g. \lstinline{if-then-else} 
alternatives, which are implemented as anonymous procedures).  
Because \PVMIR{} represents a seauence of minimal carrier-related 
operations, certain such sequences may be isolated as separate 
subroutines, which in turn can be assigned unique-identifier 
and called within the virtual-machine runtime as 
\q{virtual} functions.  
}
\vsftcl{!h}{figPVMlines}
\p{The \PVMIR{} format relies on the fact that each minimal virtual-machine 
operation has a similar form \mdash{} actually every 
instruction either takes no arguments or a single string argument.  
As such the input text can be compiled into an array of 
pairs comprised of a pointer-to-member-function and a (possibly 
empty) argument (see Listing~\ref{lst:figPVMlines}).  The 
code at \OneOverlay{} converts a string name to a method-pointer and 
appends to an instruction list indexed by the name of the 
procedure currently being compiled; while \TwoOverlay{} shows 
the analogous runtime logic that loads a compiled instruction-list, 
which may then be executed as a called procedure.  In this 
milieu the channel packages whose structure laid out the 
setup for a procedure-call here become itemized into the 
creation of a local, stack-driven context where the 
called procedure can finally be realized, as a virtual-machine 
subroutine.   
}
\p{The final step for a Phaon Virtual Machine is to 
\q{reduce} channel packages to emulate the normal 
\Cpp{} \ABI{}.  The PhaonGraph system is intended to 
support \q{exotic} channel packages, with channel-semantic 
protocols that depart from the basic inputs and returns 
(and also \this{} objects and exceptions) from 
mainstream programming languages.  The specialized 
protocols can provide a more sophisticated basis for 
static or runtime analysis of Phaon-structured source 
code representations.  However, a Phaon environment eventually 
needs to defer to \Cpp{} functions to provide underlying 
functionality, which means that packages based on exotic 
protocols must be mapped to simpler packages shaped around 
\Cpp{}: at most one \this{} and one return value; 
a block to catch exceptions if necessary; and any remaining 
parameters in a conventional input channel.  Mimicking a 
\Cpp{} \ABI{} also requires that the byte-length of input and 
output parameters is anticipated, based on the signature 
of a called \Cpp{} function.  
}
\vsftcl{!h}{figPVMrt}
\p{So far I have talked about channel \i{packages} which are 
structured around points in source-code where procedures are 
called.  The \PVM{} system has a parallel notion of 
channel \i{bundles} which are associated with functions' 
signatures \mdash{} at runtime, \PVM{} can access \Cpp{} functions 
which are described ahead of time in terms of 
channel-bundles.\footnote{Separately, \PVM{} can also call \Qt{}-specific methods according 
to \Qt{}'s internal reflection mechanism, the Meta-Object Protocol.
}  When a \PVM{} instance starts up, the predefined 
bundles are analyzed to produce a record of functions' 
signatures from the perspective of the \Cpp{} \ABI{}.  
The chief task of this analysis is to sort 
\Cpp{} functions into equivalence classes, where two 
functions are equivalent when they can be called via 
identical function-pointer and argument-array binary 
layouts.  In effect, functions are equivalent if 
their argument lists are equally-sized arrays of 
parameters which are pairwise equal in byte-length.  
In such a case the two functions may be jointly 
represented by \lstinline{void*} function-pointers and 
called from the same site in the \PVM{} engine.  
The low-level details are sampled in Listing~\ref{lst:figPVMrt}: 
here \OneOverlay{} marks a point where the engine 
switches over numeric codes standing for certain 
equivalence classes; and \TwoOverlay{} shows how multiple 
target functions, once the equivalence class is identified 
(and the argument array initialized accordingly), 
may be accessed via a single function-pointer.
}
\p{Further details about reducing channel-packages 
around the \Cpp{} \ABI{} are not directly concerned 
with graph representations and so are outside the 
scope of this paper, so with this overview I will 
wrap up my enumeration of stages in the \PVM{} pipeline.  
I will comment, however, that the \PVM{} engine's 
binary manipulation of function-pointers and 
argument-arrays enables the runtime to work with 
complex structures derived from non-stanard channel 
packages.  The end result is that \PVM{} 
provides an execution engine that works with 
sophisticated hypergraph representations of source 
code: whereas a more conventional architecture might 
assume that compilers will eventually reduce source 
input to relatively simple \AST{}-style data, \PVM{} 
anticipates a Virtual Machine runtime operating 
on instruction-sequences that preserve the more 
complex, hypergraph-driven code-representation 
semantics.  In this sense \PVM{} suggests the 
kind of compiler and Virtual Machine architecture 
that could be appropriate in general for the 
overall project of standardizing hypergraph code 
models and leveraging them directly in a runtime 
execution engine.
}
\p{}
\p{}
