\spsubsectiontwoline{Distinguishing Computational Models From AI}
\p{I contend we need to tease apart the pursuit of valuable computational
tools and models from an (often reductionistic) paradigm of
seeking artifical, computationally engineered replicas of
human cognition.  \i{Computational} does not have to equal
\i{AI}.
}
\p{Holmqvist's and Selway's research that I have cited are good examples
of paradigm-overlap between cognitive and computational
linguistics.  I will cite other scholarship which
also finds philosophical inspiration in cognitive linguists
like Langacker, G\"ardensfors, George Lakoff, and
Eleaonr Rosch, but which also target cognitive-science
formalizations and \q{cognitiv architecture}:
\cite{AntonioLieto} etc.  A recurring pattern in this
scholarship is to \i{first} propose a structural intermediate
representation \mdash{} a model of intellectual structures which
plausibly embody the processing of language and
cognitive-perceptual content, partly abstracted from
surface-level sensory or signifying details \mdash{} and
\i{second} propose algorithmic or software
models of how our minds translate linguistic and perceptual
givens to abstract, or partly-abstract, schema.
}
\p{I have argued that we bring abstract situational prototypes
to bear on understanding all of the world and social situations
around us, and that language taps into these models so that
people can coordinate situation-appropriate activity.  Given
that there is an abstract and scehmatic dimension to
how we understand situations, we should expect a
partially abstract sheen to how we intellectually
engage objcts and concepts once they are situationally
\q{located}.  Having identified objects as
butter or carving knives, pitchers or glasses of
water, wine or beer bottles, corkscrews and bottle openers \mdash{}
identifications themselves mediated by situational
awareness, viz. if we are hosting or attending a dinner
party \mdash{} we no longer often attend actively to
sense-perceptual minutiae.  Our mental map of our
surroundings \mdash{} there's the corkscrew, there's
the carving knife \mdash{} pulls these referents
outside the register of sensate consciousness and
into the pragmatic hum of worldly activity.  Insofar
as they nestle in our intellectual faculties in that
semi-abstract state, it seems fair to capture the
schematic, structural appearance they have in
this intellectual register \mdash{} phenomena without the
full-cloth phenomenology.
}
\p{This in turn seems to invite us to imagine how the
structural essentials of such \q{pragmatic appearance}
may be captured by computers.  We do not need to endow
computers with human consciousness or emotions, because
our mental traffic with the corkscrew or carving
knife at some point evolves outside the sensate and
passionate fabric of momentary consciousness.  There
is a schematic and mechanical dimnsion of
human action, and we can imagine computers
simulating human intellligence at least on
\i{that} theatrical level.
}
\p{Or at least, such seems to be the intuition behind attemps
to model our human representations of objects and
concepts in terms of abstract structures.  But even a feasible
theory of these semi-abstract layers of cognitive processing
is only half the story.  Suppose we agree that there
are legitimate cognitive insights in Holmqvist's model of cognitive
frames, incorporating (but also extending, including in a more pragmatist
direction) Conceptual Space Theory \mdash{} employing
a generalized mereology that renders objects and concepts
as \i{parts} of situations (I have suggested a
more conceptual-role account for analogous phenomena).
Suppose also we find plausible cognitive-frame
models in Selway's intermediate representation
for natural language, via which
his proposed implementation can potentially
map natural language to formal specifications.
In these cases we hagve potentially
valuable Intermediate Representations which capture
cognition, in effect, mid-stream, or in-the-act:
neither conscious phenomenology nor neurophysical hardware.
}
\p{However, Holmqvist's and Selways' work appears to
operate in an environment where these
Intermediate Representations are valued
primarily because and insofar as they allow
human cognition to be mechanically recapitulated.
This of course demands not only that
compuers \i{represent} IR models, but also \i{create}
them \mdash{} that is, when presented with an artifact of
natural language, or the visual data of a scene, that
computers should \i{automatically} map these givens
to the theorized IR models, as if retracing
the steps of human intelligence.
}
\p{But just because IR models can be given
computational form and representations, it
does not automatically follow that automated
generation of IRs is possible or effective.
We can and should thereby distinguish the computational
\i{study} of cognitive Intermediate Representation
from the AI vision of programming computers
not just to \i{host} but to \i{derive}
Intermediate Representations.
I am sympathetic to the former methodology
but skeptical of the latter.
}
\p{I also believe that most research in, e.g., computational
linguistics, ends up conflating those two goals.  In that
case, IR models are judged based on whether
they facilitate automated, AI-driven generation
of IR, not on whether the IRs are insightful
suggestions of how human cognition itself
builds an intermediary cognitive register \mdash{}
paricularly if we accept Vakarelov's overall
picture of language as an interface between
speech-givens and prelinguistic cognitive
fsculties.  Interface theories and Intermediate
Representations tend to go together \mdash{} the IR is
the representation of some input during
intermediate processing yielding an
output; a structure between two other structures,
where the role of the interface is to bridge
the structures as well as to activate the correct
capabilities via the output.  This is the
architecture of an \q{interface theory}, in
science or computer programming; it carries
over to linguistics of we take Vakarelov's
ITM seriously.
}
\p{An equally intrinsic aspect of interface theories,
however, is that the processes operative at the intermediate
level aree theoretically distinct from the realms which the
interface bridges.  For example, the theory of programming-language
compilers and runtimes is distinct both from
the theory of programming-language parsers and
specifications, and from the theory of CPU
architecture and system-kernel development.  Runtime
engineers can work through the medium of IR
models, and compiler design itself is split between
parsing surface-level source code \i{to} IR and
mapping IR structures to their proper runtime
paths of execution.  It would be a breach of
design architecture to attempt to solve
source-to-IR problems within modules devoted to
IR-to-runtime engineering.
}
\p{Unfortunately, I get the sense that AI research does
not respect a comparably disciplined Separation Of
Concerns.  There are multiple parts to a
typical AI platform \mdash{} modules for representing information
(or knowledge/facts/beliefs, or the state of the system's
physical or digital environs, etc.); for populating
these reprsentations with data deliberately introduced
by human users or absorbed via some real-time engineering
from the outside world; for analyzing
reprsentations to glean insights or calculate a course
of action.  Individual parts of the overall architecture
can evince noteworthy engineering
achievements, separate from the goals of
the overall system.  In this sense the pursuit of
AI can yield positive contributions in other
branches of computer science and other disciplines,
without the stated rationale of AI realizing
(and monetizing) systems that exhibit
humanlike intelligence.
}
\p{So perhaps \q{AI} is
best understood as shorthand for a suite of
research agendas across several aspects of
computer science, not restricted to the
fields \mdash{} like Machine Learning, Robotics,
and Artificials Neural Networks \mdash{} that are
publicly associated with the term.  This is
not, however, how AI seeems to be represented by
companies and institutions (including in academia)
who have a vested interest in the products AI may
yield.  A benevolent reading would be that
institutions understand the diversity of research
that can be loosely aggregated under the AI umbrealla,
but use the particularly science-fictional facets of
this science to excite public support: visions
of humanoid robots and conversationalists provide a compact
story to that is more meaningful to non-experts
than technical outlines of the intermediate machinery
beneath the hopefully-intelligent surface.
However, a more cynical interpretation
is that AI is valued as a cash cow, and
residual disciplines which contribute to the
engineering infrastructure that AI requires
\mdash{} but are agnostic as to the AI vision
itself \mdash{} are appreciated only so much as
needed to keep the AI project moving forward.  On that
interpretation support for AI-agnostic
research becomes lukewarm and transactional, and
actual innovation in such areas may not be properly
celebrated.
}
