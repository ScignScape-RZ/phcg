`section.Procedures and Integration`
`p.
So far I have criticized paradigms which try to account for linguistic
meaning via concordance between linguistic and proppositional
structure.  The logical dimension to language is real, but
it is not neatly ordered in the syntax/semantics
interface.  My slogan is that `i.syntactic comppsition does not
recapitulate logical form`/.
`p`

`p.
I believe that a lot of linguistics and philosophies of
language obscure this point by virtue
of commitments to two reasonable theses: `i.one`/,
that logical form is an important (and often
the central) dimension of meaning, and
`i.two` that language is compositional.  But
while language expresses propositional content
%-- i.e., in many cases language `q.means` the
way that propositions `q.mean` %-- language
is not `i.compositional`/, in general,
the way that propositions are compositional.
So compositionality and logicality have to be
disentangled, and I contend linguistics and
philosophies of language have failed to do so.
Moreover, as I argued in Section 4,
I think this can be explained by the historical roots
of philosophy of language %-- and by extension
linguistics %-- in Analytic Philosophy,
which in turn has a specific history and
collaboration with formal logic.
`p`

`p.
This critique has two dimensions:
first, although a predicate structure, a predicative specificity, does indeed
permeate  states of affairs insofar as we engage them rationally, such
logical order is not modeled by language itself so much as by cognitive
pictures we develop via interpretive processes
%-- processes `i.triggered` by language details
but, I believe, to some not insubstantial degree pre- or extra-linguistic.
Moreover, second, insofar as we `i.can` develop formal models of
language, these are not going to be models of predicate structure
in any conventional sense.
`p`

`p.
Cognitive-interpretive processes may
have formal structure %-- structure which may even show a lot of overlap
with propositional forms %-- but these are not `i.linguistic` structures.
Insofar as language triggers but does not constitute interpretive
`q.scripts`/, the scripts themselves (i.e., conceptual prototypes and
perceptual schema we keep intellectually at hand, to interpret and
act constructively in all situations, linguistic and otherwise) are
not linguistic as such %-- and neither is any propositional order they
may simulate.  Language `i.does`/, however, structure the `i.integration`
of `i.multiple` interpretive scripts, so the structure of this
integration `i.is` linguistic structure `i.per se` %-- and formally
modeling such integration can be an interesting tactic for
formally modeling linguistic phenomena.  However,
we should not assume that such a formal model will resemble or
be reducible to formal logic in any useful way %-- formalization
does not automatically entail some kind of `i.de facto` isomorphism
to a system of logic (if not first-order then second-order, modal, etc.).
`p`

`p.
Instead, I want to focus in on branches of computer science and
mathematics (such as process algebra, which I have already referenced)
as part of our scientific background insofar as the `i.structural
integration` of diverse `q.processes` (computational processes
in a formal sense, but perhaps analogously
cognitive processes in a linguistic sense) can be technically represented.
`p`

`p.
In `q.truth-theoretic` semantics, artifacts of language
are intuitively pictured in terms of propositional structure.
The guiding intuition compares word meanings to logical predicates;
e.g. `i.John is married` is typically said when the speaker believes
that, in fact, the predicate `i.married` applies to the
speaker `i.John`/.  Switching to a more
`q.procedural` perspective involves intuiting word-meaning
more in terms of interpretive procedures than logical predicates.
The change in perspective may not yield substantially different
analyses in all cases, but I believe it does affect many cases.
`p`

`p.
Even a simple predicate like `q.married` reveals a spectrum of
not-entirely-logical cases in ordinary language:

`sentenceList,
`sentenceItem; John is married to a woman from his home country, but
he had to get the marriage legally recognized here.
`sentenceItem; John married his boyfriend in Canada, but they
live in a state that does not recognize same-sex marriages.
`sentenceItem; John has been married for five years, but in
many ways he's still a bachelor.
`sentenceItem; Every married man needs a bachelor pad somewhere,
and wherever yours is, you need a mini-fridge.
`sentenceList`

We can make sense of these sentences because we do not conceptually
define lexemes like `i.married` or `i.bachelor` via exhaustive
logical rules, like `i.a bachelor is an unmarried man`/.  Instead we
have some combination of prototypical images %-- familiar
characteristics of bachelors, say, which even married men
can instantiate %-- and a conceptual framework recognizing
(and if needed distinguishing) the various legal, societal, and
dispositional aspects of being married.
`p`

`p.
Intuitionwise, then, we should look beyond potential logical
glosses on meanings %-- even when these are often accurate
%-- and theorizee semantics as the mapping of lexical
(as well as morphosyntactic phenomena) to interpretive scripts
and conceptual or perceptual/enactive schema %-- which
we can collectively designate, for sake of discussion, as
`q.cognitive procedures`/.
`p`

`p.
The truth-theoretic mapping of words to predicates
(and so phrasal and sentence units to propositional structures)
provides an obvious way to formalize linguistic structure by
borrowing the analogous structuration from
predicate complexes.  Substituting a procedural semantic model
allows a comparable formalization of linguistic structure
through theories exploring procedural integration,
for instance the interactions between computational procedures.
Analysis of `i.computational` produres can yield interesting
ideas for linguistic theories of `i.cognitive` procedures
%-- without endorsing a reductive metaphysics of
cognitive procedures as `q.nothing but` computational
procedures implemented in some sort of mental software.
`p`

`p.
I earlier used computing procedures roughly
sketched %-- like code which may open a file
(\ref{itm:file}) %-- as loose metaphors for
cognitive processes; my point before
was that both cognitive and computational
processes can be hard to gloss logically
because they are often dealing
with incomplete logical information.
More seriously %-- because `i.incomplete`
logical information is not necessarily
outside of logic; there are logical systems which
can model information partiality in
systematic ways %-- processes which
occur in the midst of logically incomplete
spaces (in the context of message-passing,
function-routing, etc.) do not
nessarily `i.operate` logically.  That is,
local rules for message-passing or function-routing systems
do not seem to be (in their `q.native` semantics)
logical rules.  In the current context, I want
to consider in greater detail the interoperation
`i.between` computational procedures, to consider
how procedures form `q.networks` and whether this is a
useful analogy for cognitive/linguistic processes.
`p`

`spsubsectiontwoline.Procedural Networks as a
Cognitive/Linguistic Metaphor`
`p.
The notion of computational procedures is certainly
not foreign to symbolic logic or to analyses often
associated with logistical models (in linguistics and philosophy,
say), like Typed Lambda Calculus.  In those paradigms
we certainly have, say, an idea of applying formulae to
input values (where formulae can be seen as referring to or
as compact notations for computational procedures).  Comparing
the results of two formulae yields predicates
(e.g., equalities or greater/less-than relations of
quantity) that undergirds predicate systems.  Furthermore,
combining formulae %-- plugging results of one formula
into a free variable in another %-- yields new formulae
that can in turn be reduced to different forms or to single
values, either for all inputs or for particular inputs.
In short, a large class of computational procedures can
be modeled in predicate and equational or formula-reduction
terms.  I point this out because shifting to
a `q.procedural` intuitive model does not `i.by itself`
take us outside of logistic and truth-thoretic paradigms.
`p`

`p.
And yet, in computer science, theories of procedural networks
begin to branch out from formal logic or
lambda calculii when we move past the `q.mathematical`
picture of procedures %-- as computations that accept a
collection of inputs and return a collection of outputs
%-- and theorize procedures more as computational
modules that input, output, and modify values
from multiple sources, in different ways.  In practical
technology, this includes developments like Object-Oriented
computer languages and programming techniques such as
Exceptions and mutable references.  Likewise,
real-world code can pull information from many sources %-- like
databases, files, and networks (including the internet)
%-- in addition to values input as parameters to a function.
Conversely, functions have many ways to modify values
%-- many ways to manifest side-effects %-- beyond just
returning values as outputs.  Collectively these various
channels of communication %-- wherein different procedures
can exchange values in different ways %-- are often
described as `q.inpure`/, by contrast
to `q.pure` functions which use only a single
input channel for (non-modifiable) input parameters
and a single output channel.
`p`

`p.
The design and implementation of computer programming languages
%-- sometimes called `q.Software Language Engineering` %--
is partly a theoretical discipline (because programming languages
are based on formal systems that can be studied mathematically,
like Lambda Calculus), but also very pragmatic.  Computer languages
are judged by practical considerations, like how efficiently they
allow programmers to create quality software.  The impetus
for new programming techniques often comes from the practical
side: mainstream languages began to incoporate features like
Object-Orientation and Exceptions (which are a mechanism to
interrupt normal program flow when coding assumptions are violated)
because these features proved useful to programmers.  At the same
time, various `q.inpure` features can still be modeled at the theoretical
level %-- there are extensions to Lambda Calculus
with Exceptions and/or Objects, as well as `q.effect systems` that
systematically model functional side-effects via Type Theory.
In this case, though, the thoretical models are partly isolated or
even lag behind the evolution of practical coding techniques.
`p`

`p.
It is also true that `q.inpure` code can always be refactored
into a code base that uses only `q.pure` functional-programming
techniques.  That is, at least if we exclude the `q.cyberphysical`
layer %-- the body of code that directly measures or alteres
physical data, like writing text to a computer screen or
reading the position of a mouse %-- any self-contained
collection of `q.inpure` functions can always be re-implemented
as a system of pure `q.mathematical` functions, using
only one-dimensional, immutable input and output channels
(this is not necessarily true of single functions, but
abstractly true of complete code `i.bodies` encompassing many
functions, i.e., many implemented procedures).  Analogously, any
computing environment based on a type system that incorporates
impure function-types can be recreated on top of a different
type system that recognizes only pure functions.  For sake of
discussion, define a `i.procedure space` as a self-contained
network of computational procedures, where each procedure has
the ability to invoke other procedures in the network.  We can then
say that any procedure space developed with the benefit of
`q.impure` coding styles (like Objects, Exceptions, and
side-effects) is `i.computationally isomorphic` to a procdure space
implemented exclusively in a pure-functional paradigm.
`p`

`p.
This principle might suggest that the pure/impure distinction
is superficial, and so that any computing environment is
actually manifesting a purely logical framework, because
it is (in one sense) isomorphic to a structur which
`i.can` be thoroughly modeled via formal logic.  However, this
reductionism actually reveals the limits of `q.computational`
isomorphism in the abstract.  Impure procedure-spaces are not
`i.equivalent` to their pure isomorphs.  The whole rationale for
`q.impure` coding techniques %-- Object-Orientation, functions
with side-effects, etc. %-- is that the resulting code better
models empirical phenomena and/or how programmers themselvs
reason about software design.  Any notion of
computational isomorphism which is `i.itself` prejudiced
toward formal logic will fail to model how systems
which are isomorphic `i.on these` (already logical)
`i.terms` will nevertheless vary in how
organically they model empirical and mental phenomena.
If engineers discover that Object-Oriented paradigms
produce more effective software for managing biomedical
data %-- that is, biomedical concepts are usefully
modeled in terms of semiautonomous computational `q.objects`
whose properties and functionality are enginereed via
many discrete, partially isolated software components %-- this
suggests how Object-Oriented structures may be a more natural
systematization of the underlying biomedical reality.
Object-Oriented software can `i.in principle` be redesigned
without Objects, but the resulting code base would then be a weaker
semantic `q.fit`/, a less faithful computational encoding,
of the external information spaces that the software
needs to model and simulate.
`p`

`p.
I contend that this duality between Object-Oriented and
pure-functional programming is analogous to the paradigmatic
contrast between cognitive and truth-theoretic semantics.
On this analogy, pure-functional programming can be associated
with truth-theoretic semantics in that both are founded on formal
systems that more or less straightforwardly operationalize
first-order logic %-- a logic with quantification and with sets
or types modeling collections, but whose foundational units are
boolean predicates and elementary formulae rather than
proceures with multiple `q.channels of communication` and
potentially many intermediate states.  Reducing impure
(e.g., Object-Oriented) code to pure-functional procedure spaces
is analogous to providing truth-theoretic reconstructions
of linguistic meanings, as in (revisiting
examples from prior sections):

`sentenceList,
`sentenceItem; People had fed the rescued dogs (i.e.: before they
were rescued).
`sentenceItem; New Yorkers vote democratic (i.e.: the majority
of registered voters in New York do so).
`sentenceItem; There is milk in that coffee (i.e.: almond milk).
`sentenceItem; I handed him a bottle opener (i.e.: a butterfly
corskscrew).
`sentenceItem; That wine is Fabernet Franc (i.e.: `q.W is CF`/).
`sentenceList`

Propositional reconstructions of sentences like these can capture a
logical substrate that contributes to their meaning, but completely
`i.reducing` their semantics to such logical substrata jettisons,
I have argued, interpretive and contextual schemata which are equally
essential to their meaning.  Analogously %-- considered
in relation to data that it must curate and model (so that pople
may manage and aggregate data; for instance, collect biomedical data
to ensure proper diagnoses and implement tratments) %-- software
does not only represent the logical substrata undrlying information
systems.  It needs to capture conceptual and operational/logistical
phenomena as well.
`p`

`p.
Along these lines, note that Object-Oriented techniques originally
emerged from scientific-computing environments, to model
complex systems, with multiple levels of organization, emergent
structure, and so forth (in the 1990s these techniques
were then applied to more enterprise-style applications,
for User Interface design, business operations workflows, etc.).
The effectiveness of Object-Oriented models
suggests that as a form of computing environment, and a
framework for coding procedure-spaces, Object-Orientation
captures structural properties of complex systems
more richly than paradigms that reduce procedural implementations
to a purely logical substratum.  This observation
can then generalize to the whole range of modern-day programming
techniques and all the various domains where software is used
to monitor, simulate, and manage information about natural and
human phenomena.
`p`

`p.
Technically, the multi-dimensional structure of computational
procedures implemented with modern coding techniques %--
multi-dimensional in the sense of multiple forms of channels of
communication and functional side-effects %-- demands new
conceptualizations of the formal architecture of computing environments
as such.  Rather than seeing computer software, for example, as a
complex system built architecturally from predicate logic, we
need to undrstand software as a procedure-space or procedural
network receptive to external information.  On this account, a
software application internally possesses a set of capabilities,
and achieves this functionality by invoking different
procedures which are implemented in the software; the application
as a whole is the aggregate of its procedural building-blocks.
Which procedures get invoked depends on user actions
%-- we use a mouse, keyboard, and other input devices to
trigger responses from the software.  Applications are therefore built
around an `q.event loop`/: the software can enter a passive
state, performing no operations apart from monitoring
physical devices to detect user actions.  Once an action does
occur %-- we press a letter on a keyboard, say, or press a mouse button
%-- the application responds by invoking a specific procedure, which
in turn may trigger a complex cascade of other procedures, to complete
whatever operation we requested via our action (saving a file,
selecting one record to visualize from a database, etc.).
Via such networks of procedures and functionality, software
models and simulates empirical phenomena %-- at least enough so that we can
maintain databases, remotely operate machines, and in other ways track
and manipulate physical states.
`p`

`p.
Consequently, computer models of empirical phenomena can take the
form of `i.procedural networks`/, and we can designate the design and
theory of such models as `i.procedural modeling`/.  I have argued that
procedural networks are not reducible to logical predicate systems
even though procedure-spaces in general are (in a certain sense)
isomorphic to procedure-spaces which are more purely logical,
or employ pure-functional type systems.  The `q.isomorphism` relevant
here, I am arguing, eliminates semantic and simulative structures
that make procedural models effective constructions conveying
the organization and behavior patterns of complex, real-world
phenomena.  On this theory, procedural modeling is a more
effective tool for representing real-world systems than
are computing environments built more mechanistically from
predicate logic.
`p`

`p.
Interestingly, however, the predominant contemporary
paradigms for modeling real-world information systems %--
particularly what has come to be called the `q.Semantic Web`
%-- is built on a framework of description logic and
`q.knowledge graphs` rather than (at least except very indirectly)
anything that could be called `q.procedural data modeling`
`cite<LaureVieu>;, `cite<BagetEtAl>;, `cite<Sowa>;.
There is, of course, a robust ecosystem of network-based
code-sharing that provides a viable infrastructure for
a more procedural data-sharing paradigm.  To illustrate the contrast,
consider the problem of biomedical data integration: of
merging the information known to two different mdical entrprises,
as when a patient moves between two different hospital systems.
We can assume that each hospital uses a different set of software
tools to store patient records and manage data from their various
secondary or constituent sources (diagnostic labs, specialized
departments, outside clinics, etc.).  Such multi-faceted data then
needs to be translated from formats native to the prior hospital's
software to formats needed by the new hospital.
`p`

`p.
Such data-intgration problems tend to be conceptualized in one
of two ways.  One approach is to seek a common representation,
or an overarching unified model, which can reconcile structural
differences between the two systems.  If both hospitals use the
same biomedical `q.Ontologies`/, for example, then those Ontologies
serve as a logical paradigm through which the distinct systems can
be bridged.  In effect, structural differences between the systems are
treated as superficial variations on a common or unified
logical `q.deep structure` %-- analogous to translating between
natural languages by mapping sentences to a propositional core.
Indeed, the field of `q.Ontology Engineering` can be seen as a
way of marshaling abstract logic into a form that is practically
useful for information storage and extraction, in open-ended
environments where data may be aggregated from many heterogeneous
sources.  The term `q.Ontology` in this context is not wholly
unrelated to its philosophical meaning, bcause `q.knowledge
engineers` assuming that the primordial structuring paradigms
of such `q.universal` logical systems are relations and
distinctions investigatd by the philosophical ontological tradition
%-- substance and attribute, part and whole, time-point vs.
time-span, spatial region vs. spatial boundary, subtype and
supertype, and so forth.  Abstract Ontological systems are
then formalized into logical rules that provide an
axiomatic core %-- sometims called `q.Upper` Ontologies
%-- which are then extended via empirical models
or `q.domain-specific` rules to create Domain-Specific Ontologies
used to integrate data in concrete enterprise/scientific fields
(medical information, government records, and so forth).
`p`

`p.
The overarching paradigm of such Ontology-based integration
is the idea of a logical model that recognizes superficial
differencs between incompatible (or at least not fully
compatible) data sources.  An alternative approach to data
integration problems is to treat these as issues of
procedural capability.  In order to import data from one
hospital into a second hospital system, for example, assuming
their respective software and databases are at least somewhat
incompatible, you need to perform certain procedures that translate
data from the first format to the second.  Insofar as this is
possible, the two software systems have a certain procedural
synergy: the capabilities of the first system include exporting
data in a format the second can use, and the capabilities of the
second system include integrating data presented in formats that
the first can export.  This synergy does not need to be theorized
in terms of an overarching logical ur-space which both
systems indirectly manifest; instead, it reflcts how some
subset of the overall procedural network germane to each
respctive software system includes %-- either by
deliberate cooperative enginering or because the two
systems are guided by similar standards and technical orientations
%-- procedures on the two ends that can be aligned in terms
of the kind of data and data-formats they produce and/or consume.
In short, targeted (and potentially cooperatively-engineered)
procedural alignments %-- rather than overarching logical
commonalities %-- form the bedrock of data integration.
`p`

`p.
These contrasting paradigms are not only theoretical.
Computer scientists hafve actually tried to promote
data sharing and data integration to improve governance,
health care, urban infrastructure (via `q.smart cities`/),
sustainable development, and so on.  Insofar as we see data
integration in terms of Ontology Engineering, these
practical goals can be met with the curation and publication
of formal Ontologies that can guide software and database
development.  Data sharing is then driven by Ontological
alignment %-- the Semantic Web, for example, designed
as an open repository of raw data annotatd and structured in accord
with publishd Ontologies, data which can be reused and integrated into
different projects more readily because it adopts these published
models.  On the other hand, insofar as we see data sharing in terms
of `i.procedural alignment`/, we prioritize the curation and
publication of procedure implementations, in the form of software
libraries, open-source code repositories, and other building-blocks
of modular software design.  Consider again the case of data
integration between two hospitals: to integrate data between the two,
programmers may need to implement special `q.bridge functions` that
reconcile their respective formats.  This implementation is
simplified insofar as the respective software systems are well-organized
and transparent %-- so that programmers can examine import and
export functions in the two code bases and write new procedures,
extending their respective capabilities, so that import functionality
on one end can be aligned to export functionality on the other.
These new procedures can then be shared so that similar
data integration problems %-- for instance the first
hospital sharing data with a third %-- can be solved
more readily, reusing some of the code thereby developed.
This approach to data integration emphasizes transparency,
modularity, and code-sharing.
`p`

`p.
Rather than seeing the Semantic Web as a network of raw data
conforming to published Ontologies, the more procedural
perspective would see endeavors like a Semantic Web as driven
by code sharing: open repsoitories of procdural
implementations that can be used to reconcile
data incompatibilities, pull data from different sources,
document data structure and representation formats, etc.
Decentralized networks like the Semantic Web would then
be characterized by the free exchange of procedural
implementations, so that enginers can pull procdures providing
different capabilities togther, to assemble fully-featured
software platforms.  Code libraries would play
a homogenizing role in this paradigm analogous to
Ontologies in the Semantic Web.  And, indeed, there
exists a mature and sophisticated technical infrastructure
for publishing software componenrs and maintaining
code repositories, forming the productive underbelly
of the Open-Source ecosystem (`q.git` version control, the
GNU Compiler Collection, Linux distributions, etc.).
However, the Semantic Web and Open Source development
communities are largely separate, apart from the practical
given that many Semantic Web technologies are distributed
as Open-Source software.  Despite the `i.practical`
adoption of Open-Source norms, the Semantic Web community
has arguably not engaged the Open-Source community
at a deeper theoretical level, in terms of how the curation
of public, collaborative code libraries can promote
data integration analogous in effect to (but arguably
semantically more accurate than) Semantic Web Ontologies
%-- in light of critiques that the logical intuitions
behind the Semantic Web create a distorted and
oversimplified theory of what semantics is all
about (in the words of Peter G\"ardenfors,
`q.The Semantic Web is not very Semantic`/).
`p`

`subsection.Phenomnology and the Semantic Web`
`p.
These questions bear directly on phenomenology, because
philosophers in the phenomenological tradition have directly
influenced the evolution of the Semantic Web %-- notably
Barry Smith, who has both published sophisticated
theoretical work on Ontologies in the Semantic Web sense
and also spearheaded practical initiatives like the
OBO (Open Biological and Biomedical Ontology) Foundry.
The OBO Foundry emerged in the mid-2000s, on the heels
of a renewed interest in phenomenology as a philosophical
basis for Cognitive Science and other practical/technical
disciplines, like knowledge engineering.  It is not
hard to see practical artifacts like the OBO system as
concrete realizations of theoretical goals articulated
in volumes such as 1999's `i.Naturalizing Phenomnology`/.
This association is not only at the level of scholarship
%-- the academic papers describing OBO, for example %--
but also the design and organization of Semantic Web tools like
the OBO Foundry, as technologies and platforms.
`p`

`p.
Seen in those terms %-- and if we restrict attention to work
done by scientists like Barry Smith and his colleagues who are
actively enagaged in both the phenomenological and
computer-science communities %-- Semantic Web technology suggests
a paradigm wherein `q.Naturalizing` Phenomenology involves
isolating logical structures which are at once essential
to modeling empirical data and also emerge organically from
phenomenological accounts of perception and cognition
%-- that is, capturing the logical order of our experiencing
the world as well as of the facts experienced.  A case in point
is formalizing systems of `q.mereotopology` %-- combining
the mereological account of part vs. whole with `q.topological`
models of spatial continuity, locality, and boundary %--
which reflect both perceptual schema and information gestalts.
So one the one hand the fusion of mereological and topological
relations gives us a vocabulary for describing how we
experientially apprehend spatial forms %-- the continuity,
regionality, intersections, and disjunctions between
visual (and sometimes tactile) elements that
gives logical articulation to visual/tactile sense
data (never experienced only as `q.raw` sensation since we fundamentally
experience space and visual continuity in these structured forms).
Meanwhile, on the other hand, Mereotopology provides a
semantic matrix for representing facts and relations in biological,
geographical, and other scientific data, so it serves a
practical information-management role.  In short, isolating
logical gestalts and then codifying them in practically useful
forms serves to `q.Naturalize` phenomenology by anchoring
phenomenological reflection in applied science.
`p`

`p.
The general implication of this `q.Naturalizing` strategy
is that phenomenology becomes naturalized, or reconciled
with the physical sciences, insofar as structures of consciousness
can be aligned with logical systems.  This strategy seems to extend
beyond just the Semantic Web projects I have highlighted; it is
likewise evident for example in Kit Fine's logical
mereology in Barry Smith and David Woodruff Smith
`i.Cambirdge Companion to Husserl` `cite<KitFine>;.
Indeed Husserl himself invites us to consider the
logical formalization of phenomenological systems in works
like the `i.Formal and Transcendental Logic`/, which appears
to suggest that human conceptual systems are an even
more refined manifestation of logical systems than are
`q.formal` logics which get entangled in model-thoretic
problems like the L\"owenheim-Skolem thorem.  In other words,
systems of formal logic are codifications of a mental order
and therefore natural cadidates for a technical representation
of the mental realm in its structure and specificity.
`p`


`p.
However, we can also observe that Husserl was writing at a time
when abstract logic was still the preeminent phenotype for
systematic exposition of formal structures in general
%-- this was before computer programming and even before
mathematical developments like Category Theory.  In the
first half of the last century, someone hoping to
create a formal and systematic representation
of cognitive processes would gravitate toward symbolic
logic simply because mathematicians and philosophers
at the time followed the general intuition that
`i.any` formal structure was essentially characterized by
its logical/axiomatic foundations.  A century later,
formal logic has been displaced from the germinal origins it
was once assigned.  For mathematicians, logic itself is
revealed to be a kind of emergent system that depends on
Categorial definitions like limits and colimits, and can
vary across Categories %-- there are different logics
for different kinds of Categories.  As such it is not
logic itself but the properties and contrasts between
Categories which is the truly primordial foundation
of logico-mathematical thought.  A contemporary
logico-mathematical formulation of phenomenology
may therefore try to establish a Category-Theoretic
grounding rather than a logical one as ordinarily understood.
A case in point would be how Jean Petitot situates
phenomnological analysis in certain soecific genres of Categories,
like sheaves and presheaves, in his `q.Morphological Eidetics`
chapter in `i.Naturalizing Phenomenology`
`cite<NaturalizingPhenomenology>; and elsewhere.
`p`

`p.
Petitot's and Barry Smith's formalizing projects were parallel
and collaborative to some extent.  Maxwell James Ramstead
in a 2015 master's thesis reviews the history elegantly:

`quote,
Now, the `q.science of salience`
proposed by Petitot and Smith (1997) illustrates the
kind of formalized analysis made possible through the direct
mathematization of phenomenological descriptions.
Its aim is to account for the invariant descriptive
structures of lived experience (what Husserl called `q.essences`/)
through formalization, providing a descriptive geometry of
macroscopic phenomena, a `q.morphological eidetics` of the
disclosure of objects in conscious experience (in Husserl's
words, the `q.constitution` of objects).
Petitot employs differential geometry and morphodynamics
to model phenomenal experience, and Smith uses formal structures from
mereotopology (the theory of parts, wholes, and their boundaries)
to a similar effect.
`quote`  Except, there are interesting
contrasts between the Cognitive-Phenomenological
adoption of mereotopology
(by Barry Smith and also Roberto Casati, Achille C. Varzi,
Kit Fine, Maureen Donnelly, etc.) %-- which
stays within a more classical logical paradigm
%-- and Petitot's Morphological Eidetics, which
is more Category-Theoretic.
`p`

`p.
Meanwhile, contemporary formalizations of phenomenological
analyses can also gravitate toward a more concrete and
computational framework %-- simulating cognitive
processes via software or comparing artificial
constructions of perceptual objects (via Virtual Reality,
3D graphics, 3D printing, robotics, etc.) to lived
experience `cite<GiuseppeRiva>;.  In particular,
graphics engineers have a rich theory
about how to create realistic (albeit not truly life-like) 3D
models and scenes.  The mathematical and computational
elements in this theory %-- triangular, quadrilateral, and
polygonal meshes; shader algorithms; `q.NURBS`
(Non-Uniform Rational Basis Spline) surfaces; textures and
`i.uv`/-mapping; camera matrices; diffusion and stochastic
processes %--  create a formal model of perceptual phenomena
insofar as these can simulated `i.to a close approximation`/:
visual phnomena artificially built with these techniques
can be `i.almost` realistic.
`p`

`p.
The gaps between such
`q.virtual` scenes and real life may suggest that there are
additional facets suffusing `q.real life` perception, or
even that despite their realism the mathematical
building-blocks of CGI-like scenes
are fundamentally different from the formal
structures governing nurophenomenological perception.
But nevertheless the almost lifelike realism that
`i.can` be achieved via Computer Graphics is a data-point
that phenomenology should acknowledge: that a
perceptual world built out of certain rigorously
mathematical constituents can feel almost
lifelike when apprehended as if it were
a real visual-perceptual surrounding.  In
particular, Computer-Generated Imagery can evoke the
same embodied engagement and intentional patterns as
non-artificial, ambient perception so long as
we accept a certain `q.suspension of disbelief`/, or
mentally adjust to the phenomenological limitations
of seeing visual tableau on a two-dimensional screen
%-- analogous to watching movies or television.
Despite the phenomenological chiasma
of directing visual attention to a 2D screen %-- it feels
`q.not quite right` %-- we can still become engaged and
largely immersed in the visual scenes before us; which means
that full phenomenological realism is not prerequisite for
our intentional comportment toward visual (and auditory)
phenomena.
`p`

`p.
We can then observe that constructed scenes
built entirely from mathematical structures carry comparable
potential for intentional engagement.  Moreover, Panoramic
Photography and Immersive Visual Reality present another
genre of phenomenological immersion that transcends the
limitations of the 2D screen %-- though still without
full realism, because however lifelike the visual
content we may perceive with, say, 3D
goggles, we still are not engaging tactilely and
kinaesthetically with the world in the usual ways.
`p`

`p.
In short, one route toward a formal framework for
elucidating perceptual-phenomenological content is to
examine realistic simulations of visual contents as
computational artifacts %-- not in terms of abstract
formulations (logical or otherwise) but in
terms of concrete computer code and software.  With
reference to Mereotopology, for example, we can
contrast the logical groundwork set out by
Barry Smith (for example) with the differential-geometric
and category-theoretic landscape considered by
Jean Petitot `i.and also` a more `q.experimental`/,
software-driven intuition associated with,
for example, Virtual Reality research.  Looking at
mereotopology in particular, this more computational
approach can examine how part-whole relations
are created within CGI and Computer Aided Design
by mesh alignment or texture mapping, or how
texture and diffusion algorthms create effects of material
continuity and locality.  In this case mereotopological
notions are not embedded in logical systems %-- or
even, from a computational perspective, in formal
Ontologies %-- but rather latent in graphics code.
`p`

`p.
For scholars pursuing a `q.Naturalized` phenomenology, then, we
(in the 2010s and 2020s) have several avenues to choose from,
including logical formalism (including as practically leveraged
in the Semantic Web) but also mathematical
formalization (as with Category Theory and Differential
Geomtry) and, also, computationally.  Computer
Aided Design and Computer Generatd Design point to a
theory of formal structures producing life-like
(if not perfectly realistic) perceptual content.  Analogously,
computational models of linguistic structure can help
represent the organizing principles of our reasoning
toward language understanding %-- even if these formal models
are not proposed as direct simulations of natural
linguistic cognition.
`p`

`p.
Simultaneously, real-world applied projects
%-- like the Semantic Web, Virtual Reality,
or CGI/CAD technology %-- can be seen as practical
test-beds for the realism and analytic potential
of formal-phenomenological frameworks.  In some
cases, like the OBO Foundary, the link between
applied technology and phenomnology is explicit
(at least at the level of institutional and
intellectual history); in
others, as with VR and CGI, this link
is more implicit and thematic (but still
addressed by interdisciplinary research grounded in
phenomenology and Husserl scholarship).  But in any
case technological experience retroactively seems to
shape the direction of phenomenological research,
while at the same time phenomenology has, for some
researchers, provided a metatheortic and
metaphysical guideline.
`p`

`p.
Here the Semantic Web presents an interesting case-study,
because the manifestation of phenomenological themes
(e.g., Mereotopology) in a practically
useful resource like Biomedical Ontologies suggests an
`i.ex-post-facto` vindication of `i.logical` formalization
as a `q.Naturalizing phenomenology` project.
On the other hand, critiques of the Semantic Web
%-- which have emerged, among elsewhere, from Cognitive
Linguistics %-- can accordingly be studied as potential
indications of how classical logical formalism is
limited in the phenomenological context.  Peter `Gardenfors;,
for example, has critiqued the Semantic Web on theoretical
grounds while also developing a model of Natural
Language semantics (via Conceptual Space Theory)
that we may find more phenomenologically realistic than
the Semantic Web's (as `Gardenfors; puts it)
`q.syllogistic` paradigms.  Here, I propose a
critique from a more `q.procedural` angle.
From my perspective, the foundational characteristic of
`q.information systems` is the existence of `i.procedures`
(say, cognitive and/or computational procedures) which `q.act on`
(aggregate, interpret, reshape) the data at hand.  I believe
it is a fair critique to say that Semantic Web technology
has unduly discounted the procedural dimension of
information management %-- not only in a thoretical
sense, but also quite practically.  For example, the
OBO Foundary does not include a mechanism for code sharing
with the goal of curating software libraries that
implement datatypes conformant to the various
published Ontologies.  In the Semantic Web paradigm,
defining logical formalizations of standardized concept-systems
is considered orthogonal to implementing software
components where these formal criteria are realized in practic.  In
short, `i.logical specification` is trated as distinct
from `i.implementation`/.  This is not a universal
approach: many technological standards are published
at least in part via `q.reference implementations` which demonstrate
standardized concepts and guide other implementations,
helping to enforce compatibility between different
software components.  Moreover, code sharing
and code reuse can promote interoperability no less effectively
than alignment relative to logically defined standards.
So there `i.are` technological trends that emphasize
`q.procedural alignment` and code-sharing as important
contributors to data integration.  However, these branches
of technology do not appear to have exerted a strong intuitive
influence on the Semantic Web.
`p`

`p.
I argued above that technology has a retroactive influence
on phenomenology, or at least on the threads of research
that follow the `q.Naturalizing` project and the
reconciliation of phenomenology with Analytic Philosophy.
Even if this effect is rather modest, it still bears on
the topic that is my primary emphasis at this point, namely
the integration of phenomenology with Cognitive Grammar.
One of the most prominent practical domains that has
influenced phenomenology has been the Semantic Web, insofar
as Semantic Web technology (via Ontologies) show
some evidence that formal models influenced by phenomenology
can be practically useful, which is one criteria to
suggest that the models have philosophical or epistemological
merit.  On the other hand, Cognitive Linguists have tended
(if anything) to be critical of the Semantic Web (in
contrast to other linguistics branches, which are generally
sympathetic to Semantic Web paradigms and incorporate
Ontologies into Computational Linguistics software).
So a potentially fertile ground for collaboration between
phenomenology and Cognitive Linguists has arguably been
overlooked insofar as the respective communities have
taken competing lessons from the successes and
limitations of the Semantic Web, particularly how
the Semantic Web leverages classical logical formalisms
(particularly Description Logics) rather than
`q.procedural` and/or Category-Theortic foundations.
`p`

`p.
This problem is not intrinsic to the Semantic Web as a
data-sharing platform, however, only to the
paradigms through which the current Semantic Web
has been conceptualized and implemented.  There are competing
intellectual frameworks that embrace parallel goals but
present alternative technical foundations, and phenomenology
would benefit from thematizing these frameworks in a role
analogous to the Semantic Web, both a practical application
and a retroactive intuition-guide.
`p`

`p.
A case in point would be
the OpenCog project, that presents a Hypergraph-based modeling
paradigm related to but technically distinct from Semantic Web
labeled graphs (one which is also consistent with
procedural-network models) and which also embraces a specific
linguistic model (based on Link Grammar).  Philosophically,
OpenCog appears to celebrate a vision of Artificial
General Intelligence which I have already flagged as problemmatic;
underestimating the context-sensitivity
and empathic intersubjectivity intrinsic to human cognition
and intelligence (and difficult to simulate with machines).
Nevertheless, formal models designed to `i.replicate` intelligent
behavior can still be useful as structural `i.models` of cognitive
phenomena, even if we believe in a metaphysical gap between the
model and the reality.  Implementing software on the basis of
explanatorarily useful cognitive models does not
guarantee that the software will realistically approximate human
cognition, but articulating and fine-tuning the models
themselves can notwithstanding be a valuable exercise.
`p`

`p.
In the case of OpenCog, the impetus toward `q.Artificial
General Intelligence` has motivated that project to explore
cognitive models that coalesced around several key structures,
including Directed Hypergraphs, Link Grammar, and
(in my terms) Procedural Networks (as underlying models
for Information Spaces).  This aggregate of theories
can be juxtaposed to Description Logic, Directed Labeled
Graphs, and formal Ontologies as the groundwork for
the Semantic Web.  I'd also argue that the OpenCog model
can potentially be a technological improvement over the existing
Semantic Web, in the sense that semantic networks built around
OpenCog-like structures can be more effective `visavis; several
important practical concerns, like application design,
data integration, and Human-Computer Interaction.
These comments do not necesarily apply to the actual OpenCog
software %-- the major OpenCog component, `q.AtomSpace`/,
is in a practical sense harder to compile and use than most
Semantic Web components %-- but instead to a
potential standardization of the core OpenCog data structures
as modeling paradigms that can be adopted by heterogeneous
information sources and data-sharing initiatives.  In
this eventuality, the OpenCog model can provide a
test-bed for applied phenomenology that stands on a
different formal foundation %-- one less inured to
symbolic logic.
`p`

`p.
Along these lines, then, I contend that a circle of
data models analogous to the OpenCog architcture can provide
a formal structuration for phenomenological accounts
of linguistic processing and information spaces comparable
in analytic roles to 3D modeling primitives in a phenomenology
of Perception.  On one side, mathematical elements like
mesh geomtry, NURBS surfaces, and texture mapping/generation
point toward formal theories of perceptual `i.cognition`
by allowing for the construction of perceptually
realistic `i.scenes`/.  These mathematical elements are
building-blocks for an (artificial) perceptual `i.content`
rather than (as far as we know) actual neurocognitive
subvenants of perceptual `i.experience`/, but their
formal specificity still gives us material to
work with when trying to consolidate a `q.scientific`
phenomenological research programme.  Analogously,
I suggest that OpenCog-like structures such as
Directed Hypergraphs, Procedural Network models, and
Link Grammar are potential building-blocks for formal
models of Cognitive Linguistics and `q.information
management` %-- for our procssing of both linguistic
content and the contxts and situations wherein language
artifacts are grounded.  In other words, these
procedural/hypergraph/link-grammar structures can model
linguistic `q.deep structure` and linguistic environments
by analogy to how mesh geometry, texture mapping,
and so forth model 3D spatial primitives and
visual-perceptual environments.
`p`

`p.
This idea is still somewhat hypothetical because the
`q.procedural/hypergraph/link-grammar` nexus has not
been consolidated into a general to the same degree
as a common vocabulary of 3D modeling primitives has
been incoporated into disparate software and research
projects.  Having said that, Link Grammar itself does
have standing as a distinct and institutionally
circumscribed body of research, so it is a reasonable
starting point for integration with phenomenological
and cognitive-linguistic approaches, an integration
which can then be extended to related structures like
Procedural Networks and Directed Hypergraphs.
`p`

`p.

`p`
