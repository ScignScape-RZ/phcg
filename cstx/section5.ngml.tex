\section{Phenomenology, AI, and Holism}
\p{As I suggested last section, we need to distinguish 
computational models of \i{aspects} of cognitive 
systems from AI-driven paradigms about intelligence 
as a holistic reality.  I 
called for \q{non-AI computational platforms}: 
theories or concrete tools to study, in a 
computational spirit, \i{parts} of 
intelligent (e.g., linguistic) behavior 
while rejecting formal or computational 
metaphors for \i{holistic} cognition.  
The proper role for cognitive-computational theories, I would argue,   
is as \i{local} analyses, targeting subsystems 
or parts thereof, remaining agnostic to overarching 
claims about the AI tractability or realizability 
of \q{mind} in total.
}  
\p{This is indeed how a lot of 
practical AI works: developers do not try to emulate 
all of human intelligence, but to implement 
practical tools with intelligent behavior in 
some narrow domain, like facial recognition 
or medical diagnosis.  
}
\p{However, philosophers and many linguists and computer scientists 
appear to move beyond this \q{limited} AI.  This may be 
for reasons even phenomenologists can sympathize with: 
pursuing AI research solely in predictable, circumscribed 
fields like medicine and transportation may seem to reduce 
human intelligence to a suite of problem-solving 
rituals: here's the system we use to drive a 
car; here's what we use to identify a familiar face.  
To counter that reductive instinct, researchers embrace 
something like an \q{Artificial General Intelligence} 
which recognizes how distinct rational abilities have 
to be holistically woven together to create truly humanlike 
intelligence.  As I have intimated, I think this 
leads any cognitive-computational research into something 
like an Interface Theory, where the \q{meaning} of 
cognitive systems lies in their interaction with 
other systems, not just their local structural properties.
}
\p{However, projecting AI paradigms up to the \q{holistic} reaches 
of consciousness threatens to be equally reductive, I believe, 
as a platform of \q{limited} or domain-specific AI achievements.  
The proper paradigm is something like 
cognitive-computational models theorized as local, subsystem-level 
structures.  But there are a couple of reasons why 
AI sympathizers may struggle against the 
limitations of that notion of computationality.   
Both of these issues I have addressed already in this 
paper to some extent.
}
\p{The first issue is that an Interface Theory is a theory of 
\i{local structures}, and it can be hard to reconcile 
such \q{localness} with logico-mathematical or truth-theoretic 
intuitions.  Recall Vakarelov's critique of both 
externalist and internalist analyses of a 
concept like \q{chair}: as he argues, it is problemmatic 
\i{either} to assume that this concept is an intramental
entity that encapsulates in some useful ways ideas and 
beliefs about chairs; \i{or} to assume it is some extramental 
correlation between our conceptual attitudes (e.g., 
our inclination to classify things as chairs) 
and external reality.  Both these accounts are flawed because 
they seem to start from some logically ordered picture of 
\q{chairness} \mdash{} in Vakarelov's words quoted above, 
\q{some fact in the world that the object depicted
by the indexical has the property of chairhood}.  Whether 
the logical \q{picture} is sited in the mind, or as a Platonic
given latent in the world-order \mdash{} chair qua 
\i{universal} \mdash{} which we mentally reach out to, 
the \i{meaning} of the concept is still figured as a kind 
of logical frame or combination, something like a 
logical \q{theory} of what chairs are and how they act.  
But this picture seems to discredit the 
polymorphic and partial nature of cognitive \i{subsystems} on 
a subsystem-oriented model. 
}
\p{The concept \i{chair} \mdash{} or, reprising one of my earlier 
discussions, \i{glass of water} \mdash{} and their concrete 
instances, lie at the nexus between multiple 
systems (perceptual, kinaesthetic/enactive, 
linguistic, intersubjective).  The concept itself is like 
a quantum of conceptualization that gets passed from 
system to system, but at no one point in its trajectory 
do we necessarily have a neat logical model of 
\q{chairhood} or glass-of-water-ness set forth 
in a propositional or semantic-frame manner.  In other 
words, \i{chair} and \i{glass of water} do not 
directly map to a \i{logical frame} internally 
in any one subsystem \mdash{} not perceptual, where we would 
attend to visual and sensory qualities rather than pragmatic 
considerations that would be intrinsic to a useful 
\q{frame} for the concept; not kinaesthetic-enactive, 
where we would be focused on motor-operational gestures 
to do things like drink from a glass; not intersubjective, 
where we would attend to how functional affordances of 
chairs and glasses intersect with others' wishes.  Nor 
linguistic, because conversation about chairs and glasses
can touch on any of these registers (even raw perceptual; 
consider discussions about a glass's artistic coloring).
In short, at no one \i{local} (sub)system as the glass- or chair-concept 
traverses different cognitive registers does the \q{logical}, 
or \i{semantic frame} model of the concepts actually 
operate as a local structure.  This, I have argued, is analogous 
to the partiality of information during the implementation 
of software systms, especially at the intermediate level; 
for example, the ambiguity as to whether a call to 
open a file has suceeded or failed (see \ref{itm:file}). 
}
\p{At most, a logical, semantic-frame-like account of everyday  
concepts emerges from the interplay of different subsystems: 
I don't dispute that at some holistic level we comport 
to a chair or a glass of water as if we have some 
structured, and mostly logically sound, 
background beliefs and dispositions which 
act in effect like a \i{logical frame}, 
encapsulating the logical coherence of the 
respective concepts.  However, to the degree that this
frame has explanatory place in analyses of cognition, 
it is not directly thematized in any concrete, operational 
register where cognitive processes actually 
occur.  It is more like a phantom hovering behind local 
structures, like drunkeness is a phenomenal gestalt 
hovering behind biochemical events whose macro-scale 
existence is what we would call someone being drunk.
}
\p{There is nothing wrong with pursuing 
logical semantic-frame models in this sense, or 
appreciating them as emrgent rationalities binding 
the fabric of consciousness.  But, at that holistic 
level, we are treating these rationalities as 
philosophical doxa, not as computable or 
logico-mathematical systems.  An Interface Theory would 
instead compel us to analyze the \i{formal} and 
\i{computational} structures in cognition 
as \q{local} subsystem structures, where emergent 
ratioanlity is only on the philosophical 
horizon.  These may be on reason why 
ITM-like paradigms may conflict with 
AI-influenced \mdash{} or even 
Analytic Philosophy influenced \mdash{} 
linguistic or cognitive-scientific intuitions.
}
\p{The second possible intuition conflict is the 
\q{Artificial General Intelligence} paradigm 
where AI formalisms are projected from local subsystem 
structures to the holistic, integrative terrain 
of consciousness or intelligence on the 
highest, most lived scales.  On this intuition, 
as I claimed at the end of last section, \q{local} 
subsystem models become valued primarily in terms of 
how they enable overall platforms \mdash{} like language-processing 
engines designed to mimic human behavior.  Taking 
such a stance toward localized models can potentially 
both overestimate the prospects of holistic AI and 
underestimate the value of localized 
models which have emerged in AI contexts 
(some of which, like Link Grammar and hypergraph 
data representations, I will mention 
later in this section).
}
\p{This situation is not irrelevant to either linguistics or to Cognitive
Phenomenology.  In Computational Linguistics,
for example, linguistic IR models seem
to be valued based on their utility in AI-driven
Natural Language Processing.  This presents a disciplinary
bifurcation, where potential computational models
are \i{either} connoted rather informally as part of
a thoretical investigation among linguists
(or philosophers of language, etc.), \i{or}
concretely implemented, in some kind of software package,
but then measured as components in a Natural Language Processing
system: assessed on the basis of how the system overall
approximates human language understanding via artifical means.
Another genre of formal models,
such as the type- or monadic theories I have alluded to here,
may also have potential software incarnations but
tend to be developed instead in a mathematical
style, effectively \q{programming} in the abstract
space of theorems and syllogisms rather than actual
compuers.  Each of these methodologies skirt
around the potential intermediary tie:
concrete computational systems that are
designed as exemplifications of semantic, grammmatical,
or pragmatic theories, presented as hands-on
software to anchor theoretical discussion but also intended
as tools to advance the human study of language, rather
than as steps toward synthetic avatars.
}
\p{At the same time, there is another side to the story: software
implementations offer a focal center for research, something
tangible that scholars can experiment and collaborate on.
The AI story provides a target goal; it helps developers understand
the local, technical code they are working on by
connecting it to a larger system.
Whatever philosophical objection one may have to AI
initiatives, we should recognize the value of
expanding academic and institutional practice beyond
just writing and reading research papers.  Insofar as
part of one's scholarly modus operandi can include
writing computer code, and studying code
repositories developed by others, we can benefit from a
hands-on, even trial-and-error kind of experimentation.
}
\p{In effect: software which can be given concrete tasks \mdash{} if it
does \i{this} properly (whatever \i{this} is),
then there is some larger thoretical point that
is demonstrated \mdash{} and then, incrementally, evolves
to realize those tasks, provides a distinct form
of intellectual engagement.  We get \i{that} to work, then
\i{that}, then fix \i{that}.  This kind of
\q{code-and-fix} cycle is quicker than conventional
research, especially in the humanities, where the
routines of authorship and publication and conferences can
feel like they are unfolding in slow motion.
}
\p{Perhaps for this reason, some of the most interesting
cognitive models have come from
computational and academic environemnts informed by
ambitious \q{Artificial General Intelligence}
programmes, like Carnegie Melon University's
OpenCog, and the \q{LMNtal} project at Waseda University
in Tokyo.  These projects both employ
formal-semantically expressive, hypergraph-oriented
data systems that embody both the structural
and procedural dimensions of computer systems \mdash{}
manifesting theories of both the execution
of computational processes and the representation of
formalized information.  These are important
models even outside the Artificial General Intelligence
ideology.
}
\p{In fact, these are models which in linguistics
and phenomenology may deserve more attention
than Artificial General Intelligence \i{qua}
philosophy.  But we should not discredit the
role that Artificial General Intelligence may
provide as a kind of intellectual compass helping
scholars and engineers reason through the
interstitial machinery which may in fact be
more real than the philosophical vision, but
also less effective as theoretical \i{vie ferrate}.
Metaphors can triangulate research
whereas analogies guide transfers of theories
or methods between fields \mdash{} i.e., analogies are
more trustworthy landmarks than metaphors
for surveying the envisioned future of a science
\mdash{} but metaphors can still be intuitive guides;
maybe AI and Artificial General Intelligence can
stabilize into our overall science and
philosophy as a modest but suggestive metaphor.
}
\p{So I have no desire to lob critiques at 
Artificial General Intelligence inspired projects in 
their role as unifying research communities, and I want 
to list certain computational models that emanate from 
AI research as valuable tools applicable to many 
(including non-AI) areas.  However, I 
\i{do} believe that AI's integrative and holistic
vision would be more persuasive if it seemed
less reductionistic as a vision of 
\i{total} human intelligence, and was 
historically grounded in the phenomenological
rather than Analytic track of 20th-century philosophy.  
Insofar as Analytic Philosophy exerted a greater influence 
on mathematics, computer science, and 
formal linguistics than \q{Continental} thought 
\mdash{} including phenomenology \mdash{} the legacy of 
this influence can clearly be traced in 
AI research.  Now that some scholars 
in the phenomenological tradition are considering a 
reconciliation or methodological hybrid with 
Analytic Philosophy, what does this imply for, say, 
AI, computational linguistics, and other 
cognitive-computational sciences?
}
\p{}
\p{}
