
`section.Channel-Algebraic Grammar`
`p.
This section will briefly introduce what I call 
`q.Channel Algebra` and how it can lead to a 
theory (and practice, in a sense) of formal 
and natural-language grammar.  Channel Algebra 
is discussed in greater detail in `cite<Neustein>;.  
It's fairly divergent from other formalizations 
in computer science, though loosely decended from 
Process Algebras and from the `q.sigma calculus`/, 
which is a formal model of Object-Oriented 
programming `cite<AbadiCardelli>;.  Channel Algebra 
may also be seen as distantly related to Santanu 
Paul's `q.Source Code Algebra` `cite<SantanuPaul>; 
and to a network of discussions %-- not necessarily 
coalesced into technical publications 
%-- about how to unify Object Oriented 
and Functional Programming.  There are many 
interesting analyses presented by scientists like 
Bartosz Milewski, on web forums such as 
Milewski's blog (the address is his full name as a 
dot-com domain).  In general, 
though, I am developing Channel Algebra 
in an `q.experimental` manner, using a concrete software 
implementation in lieu of a technical or 
mathematical axiomatic description.
`p`

`p.
In the present context I want to focus on Channel Algebra as a 
potential theory in linguistic %-- particularly 
Cognitive Grammar %-- but initially I'll describe 
the underlying theory in a more computational manner.  
A lot of the Channel Algebra formalization carries 
between formal and natural languages.
`p`

`p.
A key notion in Channel Algebra is `i.procedures`/.  As in 
Part 1, we can think of procedures as either cognitive 
processes or as functions implemented in a software 
system, although for exposition the latter interpretation 
is simpler.  So, assume we have a computing environment 
where many functions are available to be 
called %-- in effect, a bundle of software libraries 
each exposing some collection of function-implementations.  
For reasons I'll cover momentarily, I'll call thse 
`i.ambient procedures`/.
`p`

`p.
At one level, Channel Algebra is conceived as an alternative 
to data-sharing paradigms like the Semantic Web; so, one 
kind of analysis is concerned with cases where some body 
of information (which can be called a `i.data set`/) 
needs to be transferred between two different 
computing environments.  Channel Algebra takes the 
view that data does not have intrinsic semantics outside 
the computational environments where it is used.  
As I argued in the context of Searle's `q.Chinese Room`/, 
our identification that a software system represents facts 
%-- like someone's full name (the example I used over several 
paragraphs in the earlier discussion) %-- depends on the 
software possessing capabilities to display the 
information (usually visually).  In other words, 
among the total of all procedures that can be performed by 
the system, only a small set of procdures are involved in 
user-interactions where semantic intentions like 
`q.this piece of data represents someone's full name` are 
relevant.
`p`

`p.
As a consequence, when sharing data that includes 
information like `i.full-name`/, we should not assume 
that the raw data, in its semantic interpretation, 
actually `q.means` `i.full-name`/, or some kind  
of propositional assertion about full names.  For 
example, a graph-edge in a Semantic Web resource 
intended to model the proposition `q.This person has full 
name Jane Doe` should not be seen as `q.meaning` anything about 
full names.  Instead, it represents some computational 
artifact which `i.becomes` an asserion of that fact 
when a procedure is eventually calld which converts 
the full name to a (usually visual) representation 
which a human user would recognize as a view 
on a full name (and hence on the proposition).
`p`

`p.
In sum, the Semantic Web (or any data sharing 
network) only `i.has` a semantics because software 
connected by the network has requisite procedures to 
make data-views that people can understand.  Data does 
not have semantics (or at least not 
human-conceptual semantics); `i.views` do.  
This is consistent with an Interface Theory: most procedures 
manipulating Semantic Web data are part of an 
interface connecting networked data sources to the 
handful of procedures which create views for human users.  
Within the local structure of this interface, data does not 
have a `q.human` semantics; instead, it must be passed around 
between proceurs before eventually 
reaching human-interaction procedures where 
(what we would call) the `q.real` 
procedures come into play. 
`p`

`p.
When data is shared btween localities, then, the procedures 
that will receive and manipulate this data are logicallly 
prior to the data itself and constrain when data-sharing is 
possible.  Without the proper network of procedures, 
the data can never be transformed into the views 
where non-local semantics are relevant.  
This motivates my choosing the term `q.`i.ambient` procedures`/, 
because a certain collection of procedures must be in place 
on the receiver end of a data-sharing event.  This also implies 
that one important role of data modeling is to indicate 
which procures a potential receiver needs to have 
available %-- i.e., needs to implement %-- to qualify as a 
capable recipient of data conforming to the model.  
Data models should describe what procedural 
capabilities must be afforded by software libraries in 
order for the human-level, conceptual semantics of 
the data can actually emerge from humans' 
interactions with the system.  
`p`

`p.
Analogous to Ontologies as data model specifications 
for the Semantic Web, I'll use the term `q.Ambient-Procedural 
Ontologies` to express the paradigm that implememting data-sharing 
protocols involves crafting software libraries around 
procedural requirements.  This has two implications for 
how we theorize software systems.  First, we need to characterize 
procedures in a manner that expresses the proceural 
capabilities that a system offers, or must have to satisfy a 
data-sharing protocol.  Second, we can assume that 
whenever data is sent, received, manipulated, or visualized, 
there is a collection of procedures available in the system 
which enact these computational processes.  
`p`

`p.
On this basis, then, I will develop a Type Theory that operationalizes 
this intuition about `q.Ambient Procedural Ontologies`/.  
The main outlines of this type theory are first that procedures 
have types; and second that procedures are `q.ambient` or 
logically prior to (or at least equiprimordial with) the 
type system itself.  This is not a mathematical type system 
where every underlying type (like Natural Numbers) and 
every operation (like arithmetic operators) have to be 
mathematically described in the theory.  
Instead, we can always take certain types and procedures as 
`q.primitive` or (at least in their inner workings) 
external to the type theory.  For any type system `tSys;, 
whose structure is regulated by a type theory we intend 
to prsent or assess, we can say that `tSys; is built 
around a `i.kernal` `kErn; of `q.primitive` 
types and functions.
`p`

`p.
In general, an assumption of Channel Algebra is that a significant 
portion of information, present in some structured system,  
can be extracted by identifying elements in 
the system with types in a suitably developd 
type system `tSys;.  In the case of Natural Language 
%-- specifically Cognitiv Grammar %-- this means that 
many syntactic and semantic details for each word 
in a sentence can be provided by mapping words to types.  
As I have mentioned, linguists like Luo and Pustejovsky 
have given persuasive analyses of certain type systems 
for `i.semantics`/, but I intend to apply 
type theory also to `i.syntax`/.  
Later in this section I will demonstrate 
this in practice, but for now 
I will hust note that such analysis requires 
a sufficiently complex type system.  For example, 
semantic notions like dot-typs and dependent-product 
typs, which have proven to be effective in shedding 
light on common lexico-semantic phenomena, may need to be 
expressible in a `tSys;, `i.along with` link- or cognitive-grammatic 
notions like `i.connectors` and `i.expectations`/.  
`p`

`p.
Similar comments apply to modeling and sharing 
scientific data.  Here, I have in mind projects 
like Conceptual Space Markup Language, and 
applications of Conceptual Spaces to study the nature 
and evolution of scientific theories, as reflected in 
research by (notably) Frank Zenker and Gregor Strle 
`cite<Zenker>;, `cite<Strle>;.  
Implicit in this research is the philosophy 
that scientific data models are not just electronic 
specifications for transmitting raw data, but embody 
scientific theories in terms of how data is structured and 
constrained.  CSML, for example, 
defines criteria on data parameters such as ranges, 
dimensions (called `q.units` in CSML), and structural 
protocols (including CSML `q.scales`/) \cite[p. 6]{RaubalAdamsCSML}.  
As a concrete example, the biomedical concept `q.blood pressure` 
is usually understood as a pair of numbers whose dimnsions ar 
each in kilopascals (kPa) and whose first number 
(systolic pressure) is necessarily greater than 
the second (diastolic) %-- technically, the pair 
is `i.monotone decreasing`/.  In conventional Biomedical 
Ontology, cf. SNOMED-CT or the Vital 
Signs Ontology, these conditions might be stipulated by 
defining a `q.blood pressure measurement` concept subject 
to the relevant dimensional and range criteria (e.g. 
diastolic pressure must be greater than zero but 
less than systolic pressure), and/or by classifying 
systolic and diastolic pressure as subconcepts of 
blood pressure (see `cite<BarrySmithBlood>; and 
`cite<AlbertGoldfain>;).  In a kind of Ontology 
paradigm incoporating type theory, the same conceptual 
structure can be represented concisely by defining 
a type whose structure conforms to the 
semantic requirements on the `i.blood pressure` concept 
as it is scientifically understood: i.e., a 
monotone-decreasing integer pair whose 
dimensional units are labeled `q.kPa` or `q.kilopascals`/.  
Note also that the `q.monotone decreasing` 
criterion is an example of the structure of 
dependent-product types (in this case because the valid range for 
the second number depends on the value of the first), 
a construction which elsewhere is used for Natural 
Language semanticcs, e.g. \cite[p. ?]{Luo}.
`p`

`p.
Th point of this example is that many scientific concepts 
%-- the semantic norms embedded in how scientific terms are defined 
and understood and how the concepts ar used in scintific theories 
and research %-- can be rigorously specified with a suitably 
expressive type system.  Therefore, a sophisticated 
system `tSys; is both a practical tool for scientific data 
sharing and scientific computing, but also an expository 
vehicle capturing theories' conceptual underpinnings.  
Developing scientifically useful type theories can 
then serve as a kind of formalized philosophy 
of science %-- type theory as metascientific analysis.
`p`

`p.
So in the past several paragraphs I have discussed the 
idea of using type attributions to represent semantic and 
syntactic details in natural language, and also 
metascientific concepts in scientific theories and data 
sharing.  A suitably developed type-system can, in light 
of these possibilities, act as a kind of multi-purpose 
tool capturing semantic principles in a broad array 
of formal and informal cotexts.  This is possible insofar as 
type theory is developed on a flexible basis so that 
type systems can expand in different directions for 
different intellectual environments %-- dependent 
sum and product types for Natural Language semantics; 
`q.connectors` for Link Grammar; CSML-style 
units, ranges, and scales for scientific data; etc.  
The overarching goal of Channel Algebra is to 
enable these flexible, multi-purpose type systems.
`p`

`p.

`p`

